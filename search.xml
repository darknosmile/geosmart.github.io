<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark学习笔记]]></title>
    <url>%2F2017%2F07%2F23%2FSpark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[目前的大数据处理可以分为如以下三个类型。 复杂的批量数据处理（batch data processing），通常的时间跨度在数十分钟到数小时之间。 基于历史数据的交互式查询（interactive query），通常的时间跨度在数十秒到数分钟之间。 基于实时数据流的数据处理（streaming data processing），通常的时间跨度在数百毫秒到数秒之间。 目前已有很多相对成熟的开源软件来处理以上三种情景， 我们可以利用MapReduce来进行批量数据处理， 可以用Impala来进行交互式查询， 对于流式数据处理，我们可以采用Storm。 Apache Spark 的设计与实现 spark系列博客 spark java api doc 问题记录csv 转 parquetparquet 转csvparquet join操作]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深层神经网络]]></title>
    <url>%2F2017%2F07%2F20%2F%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[深度学习与深层神经网络损失函数经典损失函数交叉熵 如何判断输出向量和期望向量之间的接近程度？交叉熵是常用的评判方法，其刻画了两个概率分布之间的距离，它是分类问题中使用较广的一种损失函数； Softmax回归如何将神经网络前向传播得到的结果也变成概率分布？Softmax回归是一个非常常用的方法公式： tensorflow 实现交叉熵123456cross_entropy=-tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-10,1.0)))# y_：正确结果# y:预测结果# tf.clip_by_value：将一个张量的数值限制在一个范围内，避免一些运算错误，如log0# tf.log：对张量中的数据依次就对数# * ：乘法，每个位置上对应元素的乘积，不是矩阵乘法 因为交叉熵一般会与softmax回归一起使用，所以tensorflow对这两个功能进行了统一封装，并提供了tf.nn.softmax_cross_entropy_with_logists(y,y_) 对于回归问题，最常用的损失函数是均方差（MSE,mean squared error）；]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习基础概念]]></title>
    <url>%2F2017%2F07%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[监督学习分类问题和回归问题分类问题希望解决的是将不同的样本分到事先定义好的类别中； 分类（Classification）和回归（Regression）的区别在于输出变量的类型。 定量输出称为回归，或者说是连续变量（continous）预测； 定性输出称为分类，或者说是离散变量（discrete）预测。 对Regression回归一词的理解出自高尔顿种豆子的实验，通过大量数据统计，他发现个体小的豆子往往倾向于产生比其更大的子代，而个体大的豆子则倾向于产生比其小的子代，然后高尔顿认为这是由于新个体在向这种豆子的平均尺寸“回归”，大概的意思就是事物总是倾向于朝着某种“平均”发展，也可以说是回归于事物本来的面目。C.R.Rao等在Linear Models and Generalizations: Least Squares and Alternatives中解释道：the literature meaning of REGRESSION is “ to move in the backward direction”，看以下两个陈述：S1: model generates data orS2: data generates model.Rao认为很明显陈述S1才是对的，因为模型实际上本来就是存在的，只不过我们不知道(model exists in nature but is unknown to the experimenter)，先有模型所以我们知道X就能得到Y：先有模型 –&gt; 有了X就有Y（S1），而“回归”的意思就是我们通过收集X与Y来确定实际上存在的关系模型：收集X与Y –&gt; 确定模型（S2），与S1相比，S2就是一个“回到”模型的过程，所以就叫做“regression”。 非监督学习We can derive this structure by clustering the data based on relationships among the variables in the data.Clustering：基因分组Non-clustering: The “Cocktail Party Algorithm”,音频分离 CNNRNNRNN梯度消散问题LSTMMLP（Multi Layer Perceptron ）多层感知器，是一种前向结构的人工神经网络，映射一组输入向量到一组输出向量。MLP可以被看做是一个有向图，由多个节点层组成，每一层全连接到下一层。除了输入节点，每个节点都是一个带有非线性激活函数的神经元（或称处理单元）。一种被称为反向传播算法的监督学习方法常被用来训练MLP。MLP是感知器的推广，克服了感知器不能对线性不可分数据进行识别的弱点。 CUDA(Compute Unified Device Architecture)CUDA是显卡厂商NVIDIA推出的运算平台。 CUDA™是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。 它包含了CUDA指令集架构（ISA）以及GPU内部的并行计算引擎。 开发人员现在可以使用C语言来为CUDA™架构编写程序，C语言是应用最广泛的一种高级编程语言。所编写出的程序于是就可以在支持CUDA™的处理器上以超高性能运行。CUDA3.0已经开始支持C++和FORTRAN。 State of the art对应国内文献里的“研究现状”,当前的最高研究水平。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning课程笔记Week1-损失函数]]></title>
    <url>%2F2017%2F07%2F08%2FMachine%20Learning%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0Week1-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Coursera斯坦福大学机器学习（Machine Leaning）课程第一周课程笔记 given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis. 监督学习模型 When the target variable that we’re trying to predict is continuous, such as in our housing example, we call the learning problem a regression problem. When y can take on only a small number of discrete values (such as if, given the living area, we wanted to predict if a dwelling is a house or an apartment, say), we call it a classification problem. 损失函数（Cost Function）$$ J(\theta|_0,\theta|_1) $$ 示例：线性回归-单变量 梯度下降法（Gradiant Discent）梯度下降法求解线性回归问题，即求解最小化损失函数J θ0 on the x axis and θ1 on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. learning rate：The size of each step is determined by the parameter α, j=0,1 represents the feature index number. At each iteration j, one should simultaneously update the parameters θ1,θ2,…,θn. Updating a specific parameter prior to calculating another one on the j(th) iteration would yield to a wrong implementation. Learning Rate Debugging gradient descent. Make a plot with number of iterations on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α. Automatic convergence test. Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as 10−3. However in practice it’s difficult to choose this threshold value. To summarize: If α is too small: slow convergence. If α is too large: ￼may not decrease on every iteration and thus may not converge. 批量梯度下降（batch gradient descent）https://www.coursera.org/learn/machine-learning/supplement/U90DX/gradient-descent-for-linear-regression 凸二次函数（convex quadratic function） 特征选择和多项式回归Features and Polynomial Regression 多个特征可合并为一个新特征 线性假设函数效果不好时，可以用平方，立方，平方根或其他形式来改变函数曲线 One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important.eg. if x1 has range 1 - 1000 then range of x21 becomes 1 - 1000000 and that of x31 becomes 1 - 1000000000]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>Machine Leaning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习数学基础]]></title>
    <url>%2F2017%2F07%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[Variance(方差) 方差（Variance），应用数学里的专有名词。在概率论和统计学中，一个随机变量的方差描述的是它的离散程度，也就是该变量离其期望值的距离。方差越大，数据的分布越分散。一个实随机变量的方差也称为它的二阶矩或二阶中心动差，恰巧也是它的二阶累积量。说白了，就是将各个误差将之平方（而非取绝对值），使之肯定为正数，相加之后再除以总数，透过这样的方式来算出各个数据分布、零散（相对中心点）的程度。继续延伸的话，方差的算术平方根称为该随机变量的标准差（此为相对各个数据点间）。 总体方差计算公式： Bias(偏差)偏差：描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据，方差，是形容数据分散程度的，算是“无监督的”，客观的指标，偏差，形容数据跟我们期望的中心差得有多远，算是“有监督的”，有人的知识参与的指标。 Standard Deviation(标准差) 标准差（Standard Deviation，SD）又常称均方差，数学符号 σ（sigma），在概率统计中最常使用作为测量一组数值的离散程度之用。标准差定义：标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的两组数据，标准差未必相同。 标准差也被称为标准偏差，或者实验标准差，公式为 导数微分正态分布矩阵矩阵的性质 不满足交换律 方阵：行列相等 单位矩阵：xx对角线都为1 逆矩阵：IA=AI=A 奇异矩阵/退化矩阵（singular/degenerate）：没有逆矩阵，如零矩阵（矩阵元素都为0）矩阵的乘法矩阵的转置（transpose）]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>Machine Leaning</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R学习笔记]]></title>
    <url>%2F2017%2F07%2F04%2FR%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[R is a tool for statistics and data modeling. The R programming language is elegant, versatile, and has a highly expressive syntax designed around working with data. R is more than that, though — it also includes extremely powerful graphics capabilities. Try R 变量定义： x&lt;-42，x赋值为42 向量（Vectors）定义： c(1,T,”three”) Sequence Vectors输入：seq(5,9)或5:9输出：[1] 5 6 7 8 9 设置步长-increments12&gt;seq(5,9,0.5)[1] 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 向量访问1&gt; sentence &lt;- c('walk', 'the', 'plank') 索引访问:R索引从1开始 12&gt; sentence[3][1] "plank" 定义向量访问： 12&gt; sentence[c(1,3)][1] "walk" "dog" 范围访问： 12&gt; sentence[2:4] [1] "the" "dog" "to" 修改向量值 单个修改 1&gt; sentence[3]&lt;-"dog" 批量修改 1&gt; sentence[5:7]&lt;-c('the','poop','deck') 新增向量 1sentence[4]&lt;-"to" 绘图barplot直方图12&gt; vesselsSunk &lt;- c(4, 5, 1)&gt; barplot(vesselsSunk) scatter plots散点图123&gt; values &lt;- -10:10&gt; absolutes &lt;- abs(values)&gt; plot(values, absolutes) NA12345&gt; a &lt;- c(1, 3, NA, 7, 9)&gt; sum(a)[1] NA&gt; sum(a,na.rm=T)[1] 20 matrix定义：3行4列，值都为0matrix(0,3,4) 升维：dim设置matrix维度123456&gt; plank &lt;- 1:8&gt; dim(plank) &lt;- c(2,4)&gt; print(plank) [,1] [,2] [,3] [,4][1,] 1 3 5 7[2,] 2 4 6 8 赋值：&gt; plank[1,4] &lt;- 0 matrix访问1234&gt; print(plank) [,1] [,2] [,3] [,4][1,] 1 3 5 7[2,] 2 4 6 8 单个元素 12&gt; plank[2,3][1] 6 列 12&gt; plank[,4][1] 7 8 指定范围 1234&gt; plank[,2:4] [,1] [,2] [,3][1,] 3 5 7[2,] 4 6 8 绘图12&gt; elevation &lt;- matrix(1, 10, 10)&gt; elevation[4, 6] &lt;- 0 轮廓图 1&gt; contour(elevation) 三维轮廓图 12&gt; persp(elevation)&gt; persp(elevation, expand=0.2) 统计函数Mean 平均值1234567limbs &lt;- c(4, 3, 4, 3, 2, 4, 4, 4)names(limbs) &lt;- c('One-Eye', 'Peg-Leg', 'Smitty', 'Hook', 'Scooter', 'Dan', 'Mikey', 'Blackbeard')&gt; mean(limbs)# 平均值柱状图&gt; barplot(limbs)# 平均值柱状图 + 平均值水平线&gt; abline(h=mean(limbs) Median 中位数123456&gt; limbs &lt;- c(4, 3, 4, 3, 2, 4, 4, 14)&gt; names(limbs) &lt;- c('One-Eye', 'Peg-Leg', 'Smitty', 'Hook', 'Scooter', 'Dan', 'Mikey', 'Davy Jones')&gt; mean(limbs)[1] 4.75&gt; barplot(limbs)&gt; abline(h = mean(limbs)) It may be factually accurate to say that our crew has an average of 4.75 limbs, but it’s probably also misleading.For situations like this, it’s probably more useful to talk about the “median” value.The median is calculated by sorting the values and choosing the middle one (for sets with an even number of values, the middle two values are averaged). 12&gt; median(limbs)[1] 4 Standard Deviation 标准偏差 Statisticians use the concept of “standard deviation” from the mean to describe the range of typical values for a data set. For a group of numbers, it shows how much they typically vary from the average value. To calculate the standard deviation, you calculate the mean of the values, then subtract the mean from each number and square the result, then average those squares, and take the square root of that average. Data Frames R has a structure for just this purpose: the data frame. You can think of a data frame as something akin to a database table or an Excel spreadsheet. It has a specific number of columns, each of which is expected to contain values of a particular type. It also has an indeterminate number of rows - sets of related values for each column. 12345678&gt; treasure &lt;- data.frame(weights, prices, types)&gt; print(treasure) weights prices types1 300 9000 gold2 200 5000 silver3 100 12000 gems4 250 7500 gold5 150 18000 gems 数据访问12345678&gt; treasure[[2]][1] 9000 5000 12000 7500 18000&gt; treasure[["weights"]][1] 300 200 100 250 150&gt; treasure$prices[1] 9000 5000 12000 7500 18000 文件IO12&gt; piracy &lt;- read.csv("piracy.csv")&gt; gdp &lt;- read.table("gdp.txt", sep=" ", header=TRUE) 数据分析cor.testR can test for correlation between two vectors with the cor.test function.1&gt; cor.test(countries$GDP, countries$Piracy) lmif we calculate the linear model that best represents all our data points (with a certain degree of error).The lm function takes a model formula, which is represented by a response variable (piracy rate), a tilde character (~), and a predictor variable (GDP). (Note that the response variable comes first.) 12&gt; line &lt;- lm(countries$Piracy ~ countries$GDP)&gt; abline(line) 扩展安装install.packages(“ggplot2”)]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning课程笔记Week1-基础概念]]></title>
    <url>%2F2017%2F07%2F03%2FMachine%20Leaning%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0Week1-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[Coursera斯坦福大学机器学习（Machine Leaning）课程第一周课程笔记 A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. “对于某类任务T和性能度量P，如果一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么我们称这个计算机程序在从经验E学习。” 机器学习定义机器学习有下面几种定义： 机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。 机器学习是对能通过经验自动改进的计算机算法的研究。 机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。 What is Machine Learning?Two definitions of Machine Learning are offered. Arthur Samuel described it as: “the field of study that gives computers the ability to learn without being explicitly programmed.” This is an older, informal definition.Tom Mitchell provides a more modern definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”Example: playing checkers.E = the experience of playing many games of checkersT = the task of playing checkers.P = the probability that the program will win the next game.In general, any machine learning problem can be assigned to one of two broad classifications: Supervised learning and Unsupervised learning. 监督学习监督学习从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。 常见的监督学习算法包括回归分析和统计分类。 回归分析（连续）房价预测 统计分类（离散）癌症良恶性判断 分类（Classification）和回归（Regression）的区别在于输出变量的类型。定量输出称为回归，或者说是连续变量（continous）预测；定性输出称为分类，或者说是离散变量（discrete）预测。 非监督学习We can derive this structure by clustering the data based on relationships amongthe variables in the data. 应用场景： 市场细分 组织计算集群 社交网络分析 天文数据分析 无监督学习与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有聚类。 聚类问题基因数据分组 非聚类问题鸡尾酒会问题（音频分离） 原型工具matlaboctave]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>Machine Leaning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow实现神经网络]]></title>
    <url>%2F2017%2F07%2F03%2FTensorFlow%E5%AE%9E%E7%8E%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[神经网络可视化示例-TensorFlow游乐场一个最简单的神经元的结构的输出就是所有输入的加权和，而不同输入的权重就是神经元的参数。神经网络的优化过程就是优化神经元中参数的取值过程。 神经网络数据结构第一层是输入层，代表特征向量中每一个特征的取值；在输入层和输出层之间的神经网络叫隐藏层；一般一个神经网络的隐藏层越多，这个神经网络就越’深’； 神经网络解决分类问题的步骤：神经网络解决分类问题主要分为以下4个步骤： 提取问题中实体的特征向量作为神经网络的输入。 定义神经网络的结构，并定义如何从神经网络的输入得到输出； 通过训练数据来调整神经网络中参数的取值，这就是训练神经网络的过程。 使用训练好的神经网络来预测未知数据； 前向传播算法神经元神经元是构成一个神经网络的最小单元。 一个神经元有多个输入和一个输出； 每个神经元的输入既可以是其他神经元的输出，也可以是整个个神经网络的输入； 神经网络的结构即不同神经元之间的连接结构；一个最简单的神经元的结构的输出就是所有输入的加权和，而不同输入的权重就是神经元的参数。神经网络的优化过程就是优化神经元中参数的取值过程。 前向传播算法（forward-propagation）计算神经网络的前向传播算法需3部分信息： 神经网络的输入，即从实体中提取的特征向量； 神经网络的连接结构 神经元中的参数； 给定神经网络的输入，神经网络的结构以及边上权重，就可以通过前向传播算法来计算神经网络的输出。在TensorFlow中实现123# matmul为矩阵乘法函数a= tf.matmul(x1 , w1)y= tf.matmul(a , w2) 前向传播算法可以表示为矩阵乘法，将输入x1,x2组织成一个1x2的矩阵x=[x1,x2]，而W(1)组织成一个2x3的矩阵： 神经网络参数与TensorFlow变量通过TensorFlow训练神经网络模型完整神经网络示例程序]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow基础概念]]></title>
    <url>%2F2017%2F07%2F03%2FTensorFlow%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[TensorFlow最重要的两个概念-Tensor和Flow；Tensor就是张量，可简单理解为多维数组；Flow直观表达Tensor之间通过计算相互转换的过程；TensorFlow是一个通过计算图的形式来表述计算的编程系统。TensorFlow中的每个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。 TensorFlow的计算模型-计算图常用函数 tf.get_default_grapph()：获取当前默认计算图； tf.Graph()：生成新的计算图； tf.Graph.device(‘device’)：指定运行计算的设备； TensorFlow的数据模型-张量TensorFlow中所有运算的输入、输出都是张量（Tensor）。张量本身并不存储任何数据，它是对运算结果的引用。Tensor可简单理解为多维数组； 零阶张量-标量（scalar） 第一阶张量-向量（vector） 第n阶张量（n维数组） TensorFlow中的每一个计算是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系；不同计算图中的张量和运算都不会共享； 张量的数据结构示例：Tensor( “add:0” , shape=(2,) , dtype=float32 ) 名字（name）node:src_output 维度（shape）类型（type） 实数（tf.float32、tf.float64） 整数（tf.int8、tf.int16、tf.int32、tf.int64、tf.uint8） 布尔型（tf.bool） 复数（tf.complex64、tf.complex128） 张量的使用通过Tensor可更好的组织TensorFlow程序 对中间结果的引用，提供代码可读性 当计算图构造完成后，通过张量获取计算结果 TensorFlow的运算模型-会话（session）session的使用模式明确调用会话生成函数和关闭会话函数123session=tf.Session()sess.run(...)sess.close() Python上下文管理器管理会话将所有计算放在with内部12with tf.Session() as sess sess.run(...) 优点： 解决异常退出时资源释放问题， 解决忘记调用Session.close函数而产生的资源泄露问题； 默认会话 手动指定session为默认会话 交互式环境以tf.InteractiveSession配置会话 ConfigProto配置会话常用参数 allow_soft_placement（bool）：当运算无法被当前GPU支持时，可以自动切换到CPU上； log_device_placemnet（bool）：日志中将会记录每个节点被安排在那个设备上以方便调试。 ConfigProto可配参数 并行线程数 GPU分配策略 运算超时时间]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow环境搭建]]></title>
    <url>%2F2017%2F07%2F01%2FTensorFlow%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[将近一年未更新博客，期间专注做互联网金融领域的身份识别类产品（身份证OCR、活体检测、人脸比对），现在产品线趋于成熟，可以静下心来研究神往已久的深度学习了。 Anaconda安装Anaconda是一个和Canopy类似的Python科学计算环境，但用起来更加方便。 下载：wget https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86_64.sh 安装：bash Anaconda3-4.4.0-Linux-x86_64.sh 配置：123456789# 将anaconda的bin目录加入PATHecho 'export PATH="~/anaconda3/bin:$PATH"' &gt;&gt; ~/.bashrc# 更新bashrc以立即生效source ~/.bashrc配置好PATH后，可以通过 which conda 或 conda --version 命令检查是否正确；# 配置镜像安装完以后，打开Anaconda Prompt，输入清华的仓库镜像，更新包更快；conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yes TensorFlow安装 打开Anaconda Prompt，输入：conda create -n tensorflow python=3.6 查看已安装环境列表: conda env list； 激活环境： linux：source activate tensorflow； windows：activate tensorflow； 安装tensorflow的CPU版本， linux安装：pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp36-cp36m-linux_x86_64.whl windows安装：pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.2.1-cp36-cp36m-win_amd64.whl HelloWorld示例一个TensorFlow示例实现两个向量求和123456import tensorflow as tfa=tf.constant([1.0,2.0],name="a")b=tf.constant([2.0,3.0],name="b")result=a+bsess=tf.Session() sess.run(result) 开发环境IDE：pycharm 2017注册服务器：http://idea.iteblog.com/key.php pycharm远程调试以pycharm配置remote interpreter远程开发调试，并以ssh自动上传本地程序到测试/生产环境；配置参考feature-spotlight-python-remote-development-with-pycharm 归档环境Jupyter NotebookJupyter Notebook-machine_learning 一次运行, 多次阅读,保存运行结果 交互式编程, 边看边写 可以添加各种元素,比如图片,视频, 链接, 文档(比代码注释要好看), 相当于PPTref问题记录]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Python</tag>
        <tag>Anaconda</tag>
        <tag>Jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实时计算系统Storm学习笔记]]></title>
    <url>%2F2016%2F09%2F13%2F%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9FStorm%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Storm简介Storm是一个开源的分布式实时计算系统，可以简单、可靠的处理大量的数据流。而且支持水平扩展，具有高容错性，保证每个消息都会得到处理。Storm处理速度很快（在一个小集群中，每个结点每秒可以处理数以百万计的消息）。Storm的部署和运维都很便捷，更为重要的是可以使用任意编程语言来开发应用。 目前国内一般采用阿里巴巴开源的JStorm版本； 参考教程getting-started-with-stom-index Storm典型应用案例？ 数据处理流；不像其它的流处理系统，Storm不需要中间队列。 连续计算：连续发送数据到客户端，使它们能够实时更新并显示结果，如网站指标。 分布式远程过程调用：频繁的CPU密集型操作并行化。 Storm组件在Storm集群中，有两类节点：主节点master node和工作节点worker nodes。主节点运行着一个叫做Nimbus的守护进程。这个守护进程负责在集群中分发代码，为工作节点分配任务，并监控故障。Supervisor守护进程作为拓扑的一部分运行在工作节点上。一个Storm拓扑结构在不同的机器上运行着众多的工作节点。因为Storm在Zookeeper或本地磁盘上维持所有的集群状态，守护进程可以是无状态的而且失效或重启时不会影响整个系统的健康； NimBus: 责资源分配和任务调度 Supervisor:负责接受nimbus分配的任务，启动和停止属于自己管理的worker进程 Work:运行具体处理组件逻辑的进程 Task: worker中每一个spout/bolt的线程称为一个task 0mq在系统底层，Storm使用了zeromq(0mq)。这是一种先进的，可嵌入的网络通讯库，它提供的绝妙功能使Storm成为可能。zeromq的特性： 一个并发架构的Socket库 对于集群产品和超级计算，比TCP要快 可通过inproc（进程内）, IPC（进程间）, TCP和multicast(多播协议)通信 异步I / O的可扩展的多核消息传递应用程序 利用扇出(fanout), 发布订阅（PUB-SUB）,管道（pipeline）, 请求应答（REQ-REP），等方式实现N-N连接 客户端提交拓扑到nimbus。 Nimbus针对该拓扑建立本地的目录根据topology的配置计算task，分配task，在zookeeper上建立assignments节点存储task和supervisor机器节点中woker的对应关系； 在zookeeper上创建taskbeats节点来监控task的心跳；启动topology。 Supervisor去zookeeper上获取分配的tasks，启动多个woker进行，每个woker生成task，一个task一个线程；根据topology信息初始化建立task之间的连接;Task和Task之间是通过zeroMQ管理的；后整个拓扑运行起来。 Storm特点storm特性使用场景：如实时分析，在线机器学习，持续计算，分布式RPC，ETL等等。 Storm有如下特点： 编程模型简单在大数据处理方面相信大家对hadoop已经耳熟能详，基于Google Map/Reduce来实现的Hadoop为开发者提供了map、reduce原语，使并行批处理程序变得非常地简单和优美。同样，Storm也为大数据的实时计算提供了一些简单优美的原语，这大大降低了开发并行实时处理的任务的复杂性，帮助你快速、高效的开发应用。 ##可扩展在Storm集群中真正运行topology的主要有三个实体：工作进程、线程和任务。Storm集群中的每台机器上都可以运行多个工作进程，每个工作进程又可创建多个线程，每个线程可以执行多个任务，任务是真正进行数据处理的实体，我们开发的spout、bolt就是作为一个或者多个任务的方式执行的。因此，计算任务在多个线程、进程和服务器之间并行进行，支持灵活的水平扩展。 高可靠性Storm可以保证spout发出的每条消息都能被“完全处理”，这也是直接区别于其他实时系统的地方，如S4。请注意，spout发出的消息后续可能会触发产生成千上万条消息，可以形象的理解为一棵消息树，其中spout发出的消息为树根，Storm会跟踪这棵消息树的处理情况，只有当这棵消息树中的所有消息都被处理了，Storm才会认为spout发出的这个消息已经被“完全处理”。如果这棵消息树中的任何一个消息处理失败了，或者整棵消息树在限定的时间内没有“完全处理”，那么spout发出的消息就会重发。 考虑到尽可能减少对内存的消耗，Storm并不会跟踪消息树中的每个消息，而是采用了一些特殊的策略，它把消息树当作一个整体来跟踪，对消息树中所有消息的唯一id进行异或计算，通过是否为零来判定spout发出的消息是否被“完全处理”，这极大的节约了内存和简化了判定逻辑，后面会对这种机制进行详细介绍。 这种模式，每发送一个消息，都会同步发送一个ack/fail，对于网络的带宽会有一定的消耗，如果对于可靠性要求不高，可通过使用不同的emit接口关闭该模式。 上面所说的，Storm保证了每个消息至少被处理一次，但是对于有些计算场合，会严格要求每个消息只被处理一次，幸而Storm的0.7.0引入了事务性拓扑，解决了这个问题。 高容错性如果在消息处理过程中出了一些异常，Storm会重新安排这个出问题的处理单元。Storm保证一个处理单元永远运行（除非你显式杀掉这个处理单元）。 当然，如果处理单元中存储了中间状态，那么当处理单元重新被Storm启动的时候，需要应用自己处理中间状态的恢复。 支持多种编程语言除了用java实现spout和bolt，你还可以使用任何你熟悉的编程语言来完成这项工作，这一切得益于Storm所谓的多语言协议。多语言协议是Storm内部的一种特殊协议，允许spout或者bolt使用标准输入和标准输出来进行消息传递，传递的消息为单行文本或者是json编码的多行。 Storm支持多语言编程主要是通过ShellBolt, ShellSpout和ShellProcess这些类来实现的，这些类都实现了IBolt 和 ISpout接口，以及让shell通过java的ProcessBuilder类来执行脚本或者程序的协议。 可以看到，采用这种方式，每个tuple在处理的时候都需要进行json的编解码，因此在吞吐量上会有较大影响。 ##支持本地模式Storm有一种“本地模式”，也就是在进程中模拟一个Storm集群的所有功能，以本地模式运行topology跟在集群上运行topology类似，这对于我们开发和测试来说非常有用。 高效用ZeroMQ作为底层消息队列, 保证消息能快速被处理。 Storm基本慨念Storm集群和Hadoop集群表面上看很类似。但是Hadoop上运行的是MapReduce jobs，而在Storm上运行的是拓扑（topology），这两者之间是非常不一样的。Topology的定义是一个Thrift结构，并且Nimbus就是一个Thrift服务， 你可以提交由任何语言创建的topology。 Topologies一个topology是spouts和bolts组成的图， 通过stream groupings将图中的spouts和bolts连接起来;一个topology会一直运行直到你手动kill掉，Storm自动重新分配执行失败的任务， 并且Storm可以保证你不会有数据丢失（如果开启了高可靠性的话）。如果一些机器意外停机它上面的所有任务会被转移到其他机器上。运行一个topology很简单。首先，把你所有的代码以及所依赖的jar打进一个jar包。然后运行类似下面的这个命令：storm jar all-my-code.jar backtype.storm.MyTopology arg1 arg2这个命令会运行主类: backtype.strom.MyTopology, 参数是arg1, arg2。这个类的main函数定义这个topology并且把它提交给Nimbus。storm jar负责连接到Nimbus并且上传jar包。 Topology的定义是一个Thrift结构，并且Nimbus就是一个Thrift服务， 你可以提交由任何语言创建的topology。上面的方面是用JVM-based语言提交的最简单的方法。 Streams消息流stream是storm里的关键抽象。一个消息流是一个没有边界的tuple序列， 而这些tuple序列会以一种分布式的方式并行地创建和处理。通过对stream中tuple序列中每个字段命名来定义stream。在默认的情况下，tuple的字段类型可以是：integer，long，short， byte，string，double，float，boolean和byte array。你也可以自定义类型（只要实现相应的序列化器）。 每个消息流在定义的时候会被分配给一个id，因为单向消息流使用的相当普遍， OutputFieldsDeclarer定义了一些方法让你可以定义一个stream而不用指定这个id。在这种情况下这个stream会分配个值为‘default’默认的id 。 Storm提供的最基本的处理stream的原语是spout和bolt。你可以实现spout和bolt提供的接口来处理你的业务逻辑。 Nimbusnimbus 雨云，主节点的守护进程，负责为工作节点分发任务。 Spouts（消息源）spout 龙卷，读取原始数据为bolt提供数据；消息源spout是Storm里面一个topology里面的消息生产者。一般来说消息源会从一个外部源读取数据（如MQ）并且向topology里面发出消息：tuple。Spout可以是可靠的也可以是不可靠的。如果这个tuple没有被storm成功处理，可靠的消息源spouts可以重新发射一个tuple， 但是不可靠的消息源spouts一旦发出一个tuple就不能重发了。 消息源可以发射多条消息流stream。使用OutputFieldsDeclarer.declareStream来定义多个stream，然后使用SpoutOutputCollector来发射指定的stream。 Spout类里面最重要的方法是nextTuple。要么发射一个新的tuple到topology里面或者简单的返回如果已经没有新的tuple。要注意的是nextTuple方法不能阻塞，因为storm在同一个线程上面调用所有消息源spout的方法。 另外两个比较重要的spout方法是ack和fail。storm在检测到一个tuple被整个topology成功处理的时候调用ack，否则调用fail。storm只对可靠的spout调用ack和fail。 消息流中的TupleTuple是一次消息传递的基本单元，tuple里的每个字段一个名字,并且不同tuple的对应字段的类型必须一样。tuple的字段类型可以是： integer, long, short, byte, string, double, float, boolean和byte array；还可以自定义类型 — 只要实现对应的序列化器。每个消息流中包括若干个tuple。 Bolts（消息处理者）bolt 雷电，从spout或其它bolt接收数据，并处理数据，处理结果可作为其它bolt的数据源或最终结果；所有的消息处理逻辑被封装在bolts里面。Bolts可以做很多事情：过滤，聚合，查询数据库等等。 Bolts可以简单的做消息流的传递。复杂的消息流处理往往需要很多步骤，从而也就需要经过很多bolts。比如算出一堆图片里面被转发最多的图片就至少需要两步：第一步算出每个图片的转发数量。第二步找出转发最多的前10个图片。（如果要把这个过程做得更具有扩展性那么可能需要更多的步骤）。Bolts可以发射多条消息流， 使用OutputFieldsDeclarer.declareStream定义stream，使用OutputCollector.emit来选择要发射的stream。 Bolts的主要方法是execute, 它以一个tuple作为输入，bolts使用OutputCollector来发射tuple，bolts必须要为它处理的每一个tuple调用OutputCollector的ack方法，以通知Storm这个tuple被处理完成了，从而通知这个tuple的发射者spouts。 一般的流程是： bolts处理一个输入tuple, 发射0个或者多个tuple, 然后调用ack通知storm自己已经处理过这个tuple了。storm提供了一个IBasicBolt会自动调用ack。 Stream groupings（消息分发策略）定义一个topology的其中一步是定义每个bolt接收什么样的流作为输入。stream grouping就是用来定义一个stream应该如何分配数据给bolts上面的多个tasks。 Storm里面有7种类型的stream grouping 1. Shuffle Grouping: 随机分组， 随机派发stream里面的tuple，保证每个bolt接收到的tuple数目大致相同。 2. Fields Grouping：按字段分组， 比如按userid来分组， 具有同样userid的tuple会被分到相同的Bolts里的一个task， 而不同的userid则会被分配到不同的bolts里的task。 3. All Grouping：广播发送，对于每一个tuple，所有的bolts都会收到。 4. Global Grouping：全局分组， 这个tuple被分配到storm中的一个bolt的其中一个task。再具体一点就是分配给id值最低的那个task。 5. Non Grouping：不分组，这个分组的意思是说stream不关心到底谁会收到它的tuple。目前这种分组和Shuffle grouping是一样的效果， 有一点不同的是storm会把这个bolt放到这个bolt的订阅者同一个线程里面去执行。 6. Direct Grouping： 直接分组， 这是一种比较特别的分组方法，用这种分组意味着消息的发送者指定由消息接收者的哪个task处理这个消息。 只有被声明为Direct Stream的消息流可以声明这种分组方法。而且这种消息tuple必须使用emitDirect方法来发射。消息处理者可以通过TopologyContext来获取处理它的消息的task的id （OutputCollector.emit方法也会返回task的id）。 7. Local or shuffle grouping：如果目标bolt有一个或者多个task在同一个工作进程中，tuple将会被随机发生给这些tasks。否则，和普通的Shuffle Grouping行为一致。 ReliabilityStorm保证每个tuple会被topology完整的执行。Storm会追踪由每个spout tuple所产生的tuple树（一个bolt处理一个tuple之后可能会发射别的tuple从而形成树状结构），并且跟踪这棵tuple树什么时候成功处理完。每个topology都有一个消息超时的设置，如果storm在这个超时的时间内检测不到某个tuple树到底有没有执行成功， 那么topology会把这个tuple标记为执行失败，并且过一会儿重新发射这个tuple。 为了利用Storm的可靠性特性，在你发出一个新的tuple以及你完成处理一个tuple的时候你必须要通知storm。这一切是由OutputCollector来完成的。通过emit方法来通知一个新的tuple产生了，通过ack方法通知一个tuple处理完成了。 Tasks每一个spout和bolt会被当作很多task在整个集群里执行。每一个executor对应到一个线程，在这个线程上运行多个task，而stream grouping则是定义怎么从一堆task发射tuple到另外一堆task。你可以调用TopologyBuilder类的setSpout和setBolt来设置并行度（也就是有多少个task）。 Workers一个topology可能会在一个或者多个worker（工作进程）里面执行，每个worker是一个物理JVM并且执行整个topology的一部分。比如，对于并行度是300的topology来说，如果我们使用50个工作进程来执行，那么每个工作进程会处理其中的6个tasks。Storm会尽量均匀的工作分配给所有的worker。 ConfigurationStorm里面有一堆参数可以配置来调整Nimbus, Supervisor以及正在运行的topology的行为，一些配置是系统级别的，一些配置是topology级别的。default.yaml里面有所有的默认配置。你可以通过定义个storm.yaml在你的classpath里来覆盖这些默认配置。并且你也可以在代码里面设置一些topology相关的配置信息（使用StormSubmitter）。]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7安装MySQL5.6]]></title>
    <url>%2F2016%2F09%2F10%2FCentos7%E5%AE%89%E8%A3%85MySQL5-6%2F</url>
    <content type="text"><![CDATA[Centos7系统，以rpm方式安装MySQL5.6 参考教程 卸载MySQLrpm -qa | grep MySQLrpm -e –nodeps mysql MySQL-server-5rpm -e –allmatches MySQL-client-5.6.33-1.el7.x86_64rpm -e –allmatches MySQL-devel-5.6.33-1.el7.x86_64rpm -e –allmatches MySQL-server-5.6.33-1.el7.x86_64chkconfig –del mysqlrm -rf /user/local/mysqlrm -rf /etc/my.cnfrm -rf /var/lib/mysql 准备mysql安装文件下载 mysql包：wget http://cdn.mysql.com//Downloads/MySQL-5.6/mysql-5.6.33-winx64.zip解压缩：tar -xvf mysql-5.6.33-winx64.zip 安装MySQL1234su mysqlrpm -ivh MySQL-server-5.6.33-1.el7.x86_64.rpmrpm -ivh MySQL-devel-5.6.33-1.el7.x86_64.rpmrpm -ivh MySQL-client-5.6.33-1.el7.x86_64.rpm 修改配置文件位置cp /usr/share/mysql/my-default.cnf /etc/my.cnf 初始化MySQL及设置密码安装server会自动执行数据库初始化（perl）：/usr/bin/mysql_install_db启动服务：service mysql start查看默认密码：cat /root/.mysql_secret修改root密码：123mysql -uroot -pFiXBgAhVDgythr6BSET PASSWORD = PASSWORD('root'); exit 允许远程登陆12345mysql -uroot -prootuse mysql;update user set host='%' where user='root' and host='localhost';flush privileges;exit 设置开机自启动12chkconfig mysql onchkconfig --list | grep mysql 配置/etc/my.cnf1234567891011121314151617181920212223242526# whereis my.ini[server]bind-address = 0.0.0.0character-set-server = utf8collation-server = utf8_unicode_ciinit_connect = 'SET NAMES utf8'max_connections = 5000max_allowed_packet = 20Mmax_connect_errors= 1000lower_case_table_names=2[mysqld]data=/usr/local/mysql/datasocket=/var/lib/mysql/mysql.sock innodb_file_per_table = 1innodb_flush_method=O_DIRECTinnodb_log_file_size=1Ginnodb_buffer_pool_size=4G[mysqld_safe]log-error=/var/log/mysqld.loglong_query_time =1log-slow-queries=slowqueris.loglog-queries-not-using-indexes = nouseindex.loglog=mylog.log mysql数据目录设置权限123su rootchown -R root:root /usr/local/mysql/datachown -R root:root /var/lib/mysql]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Drools学习笔记]]></title>
    <url>%2F2016%2F08%2F22%2FDrools%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[You can build a simple rules engine yourself. All you need is to create a bunch of objects with conditions and actions, store them in a collection, and run through them to evaluate the conditions and execute the actions. -Martin Fowler 为什么会有规则引擎 ？背景：复杂企业级项目的开发以及其中随外部条件不断变化的业务规则（business logic），迫切需要分离商业决策者的商业决策逻辑和应用开发者的技术决策，并把这些商业决策放在中心数据库或其他统一的地方，让它们能在运行时（即商务时间）可以动态地管理和修改从而提供软件系统的柔性和适应性。规则引擎正是应用于上述动态环境中的一种解决方法。 企业管理者对企业级IT系统的开发有着如下的要求： 为提高效率，管理流程必须自动化，即使现代商业规则异常复杂； 市场要求业务规则经常变化，IT系统必须依据业务规则的变化快速、低成本的更新； 为了快速、低成本的更新，业务人员应能直接管理IT系统中的规则，不需要程序开发人员参与； 什么是规则引擎 ？也许这又是一种“先有蛋还是先有鸡”哲学争论，在JSR-94种也几乎没有定义,规则引擎这个术语是非常不明确的，因为任何以任意形式使用能够应用于数据生成结果的规则的系统都可以称为规则引擎。包括像表单验证和动态表达式引擎这样的简单系统都可以称之为规则引擎。可以这样理解规则引擎由推理引擎发展而来，是一种嵌入在应用程序中的组件，实现了将业务决策从应用程序代码中分离出来，并使用预定义的语义模块编写业务决策。接受数据输入，解释业务规则，并根据规则做出业务决策。 Drools is Rule Engine or a Production Rule System（产生式规则系统） that uses the rule-based approach to implement and Expert System. Expert Systems（专家系统） are knowledge-based systems that use knowledge representation（知识表达） to process acquired knowledge into a knowledge base（知识库） that can be used for reasoning（推理）.A Production Rule System is Turing complete with a focus on knowledge representation to express propositional and first-order logic in a concise, non-ambiguous and declarative manner.The brain of a Production Rules System is an Inference Engine（推理机） that can scale to a large number of rules and facts. The Inference Engine matches facts and data against Production Rules（根据规则匹配数据和事实）– also called Productions or just Rules – to infer conclusions which result in actions.A Production Rule is a two-part structure that uses first-order logic（一阶逻辑） for reasoning over knowledge representation.A business rule engine（商业规则引擎） is a software system that executes one or more business rules in a runtime production environment.A Rule Engine（定义做什么而不是怎么做） allows you to define “What to Do”（声明式编程） and not “How to do it.”（命令式编程） 规则引擎的优点Declarative Programming（声明式编程）规则引擎允许你描述做什么而不是如何去做。这里的主要优点是使用规则更加容易对复杂的问题进行表述，并得到验证。 (规则比编码更容易阅读).规则系统能够解决非常非常困难的问题，并提供了方案怎样达到和在解决问题的方向上所作的每一个决定的原因（这对于类似神经网络这样的AI系统来说不容易达到）与code不同，DSL易于编写复杂业务逻辑，对复杂问题的描述也变得简单化，且更易于阅读，理解和核查。如SQL和D3.js也是声明式编程 Logic and Data Separation（逻辑与数据分离）The data resides in the Domain Objects and the business logic resides in the Rules. Depending upon the kind of project, this kind of separation can be very advantageous.数据保存在系统对象中，逻辑保存在规则中。这根本性的打破了面向对象系统中将数据和逻辑耦合起来的局面，这点是有利的也是不利的，在于你的观察角度。这样做的结果是，未来逻辑发生改变时更容易被维护，因为逻辑保存在规则中，这点在逻辑是跨领域或多领域中使用时尤其有用。通过将逻辑集中在一个或数个清晰的规则文件中，取代了之前分散在代码中的局面。 Speed and Scalability（速度和可测量性）The Rete OO algorithm on which Drools is written is already a proven algorithm. With the help of Drools, your application becomes very scalable. If there are frequent change requests, one can add new rules without having to modify the existing rules.Rete算法、Leaps算法,以及由此衍生出来的 Drools的 Rete、Leaps算法，提供了对系统数据对象非常有效率的匹配。这些都是高效率尤其当你的数据是不完全的改变（规则引擎能够记得之前的匹配）。这些算法经过了大量实际考验的证明。 Centralization of Knowledge（集中化知识管理）By using Rules, you create a repository of knowledge (a knowledge base) which is executable. It is a single point of truth for business policy. Ideally, Rules are so readable that they can also serve as documentation.通过使用规则，将建立一个可执行的规则库。这意味着规则库代表着现实中的业务策略的唯一对应，理想情况下可读性高的规则还可以被当作文档使用。 Tool Integration（工具集成）Tools such as Eclipse provide ways to edit and manage rules and get immediate feedback, validation, and content assistance. Auditing and debugging tools are also available.例如Eclipse（将来可能在基于Web的界面上）这样的工具为规则的修改与管理、即时获得反馈、内容验证与修补提供了办法。审查与调试工具同样也可用了。 什么情况下使用规则引擎 ？最简短的回答就是“当没有令人满意的传统的程序设计方法能够解决这个问题时”。下面对这个所谓的传统解决方法的一个描述： 对于传统代码来说，问题需要的精确度太高。 这种问题可能并不复杂，但是你找不到一种稳定的方法去建立它。 问题超越了任何有明显运算法则的方案。 它是一个难以解决的复杂问题，没有明显的传统解决方案或者问题没有一个准确的定论。 业务逻辑经常发生改变：逻辑本身是简单的（但不是指过于简单），但是规则经常发生变化。在许多软件组织中正式版本的间隔是较长并且较少的，规则可以在适当的安全前提下帮助提供一定的敏捷性。 领域专家（或者业务分析师）是非技术人员：领域专家通常对业务规则和流程具有很好的认知。他们通常是不了解软件技术的人员，但是具有很好的逻辑性。规则能够让他们用自己的术语来描述业务逻辑。当然他们仍然需要严密的思考和良好的逻辑思维能力（许多在非软件技术型岗位上的人没有进行过形式逻辑的训练，因此在和他们工作时要特别小心，在将业务知识编撰成规则时，要特别注意业务规则和流程应当是当前能够理解的）。 什么情况下不能使用规则引擎 ？ 因为规则引擎是动态的 (动态的在这里意味着规则可以象数据一样保存、管理和更新),它们通常被看作发布软件系统的一种解决方案(大多数IT部门似乎存在的目的是防止软件系统被遗弃)。如果这是你希望使用规则引擎的原因，应当意识到在可以写出公开发布的规则时，规则引擎能够以最佳方式工作。 另一个方面，你也可以考虑使用数据驱动的设计（查找表）或者脚本/流程引擎等有能够在数据库中管理并能够动态更新的脚本。对特定的工作要使用恰当的工具。当然，必要时老虎钳可以当作锤子用，但那并不是发明老虎钳的本意。” Drools一个应用程序一般可分为3部分：一个和用户交互的前台(UI), 一个和后台系统，例如数据库交互的服务层(DAO)，以及他们中间的业务逻辑(BLL)。使用框架构建前台和后台系统已经成为普遍共识，如 Spring , Struts，Hibernate，Mybatis等；而没有一个标准的方法来构建业务逻辑，为什么没有一个框架来替换冗繁，易错的if…then语句呢，这个框架应该和其它前台或后台框架一样，易于配置，具有可读性和重用性。Drools 规则引擎就是解决我们这个问题的框架。 Drools是一个基于java的规则引擎，开源的，可以将复杂多变的规则从硬编码中解放出来，以规则脚本的形式存放在文件中，使得规则的变更不需要修正代码重启机器就可以立即在线上环境生效。 Drools is a business rule management system (BRMS) with a forward and backward chaining inference based rules engine, more correctly known as a production rule system, using an enhanced implementation of the Rete algorithm. KIE (Knowledge Is Everything) is the new umbrella name to drools, optaPlanner, jBPM, Guvnor, uberFire and related technologies.Drools supports the JSR-94 standard for its business rule engine and enterprise framework for the construction, maintenance, and enforcement of business policies in an organization, application, or service. Drools is a Business Logic integration Platform (BLiP). It is written in Java. It is an open source project that is backed by JBoss and Red Hat, Inc. It extends and implements the Rete Pattern matching algorithm（实现并扩展了Rete模式匹配算法）.In layman’s terms, Drools is a collection of tools that allow us to separate and reason over logic and data found within business processes（分离数据与业务逻辑，推理）. The two important keywords we need to notice are Logic and Data. Drools 分为两个主要部分：构建（ Authoring ）和运行时（ Runtime ）。 Authoring（构建）构建的过程涉及到.drl或.xml规则文件的创建，它们被读入一个解析器，使用ANTLR3语法进行解析。解析器对语法进行正确性的检查，然后产生一种中间结构“ descr ”， descr 用 AST 来描述规则。AST 然后被传到PackageBuilder ，由 PackagBuilder 来产生 Packaged 对象。PackageBuilder 还承担着一些代码产生和编译的工作，这些对于产生 Package 对象都时必需的。Package 对象是一个可以配置的，可序列化的，由一个或多个规则组成的对象。 Runtime（运行时）It involves the creation of working memory(被推理机进行匹配的数据称为 WorkingMemory) and handling the activation. Rule（规则）Rules are pieces of knowledge often expressed as, “When some conditions occur, then do some tasks.”123456789rule "&lt;name&gt;" &lt;attribute&gt; &lt;value&gt; when &lt;conditions&gt; then &lt;actions&gt;end Pattern Matching（模式匹配）对新的数据和被修改的数据进行规则的匹配称为模式匹配（ Pattern Matching ）。进行匹配的引擎称为推理机（Inference Engine ）。被访问的规则称为 ProductionMemory ，被推理机进行匹配的数据称为 WorkingMemory。 Agenda 管理被匹配规则的执行。推理机所采用的模式匹配算法有下列几种： Linear，RETE，Treat，Leaps。DroDrools是为Java量身定制的基于Charles Forgy的RETE算法的规则引擎的实现。具有了OO接口的RETE,使得商业规则有了更自然的表达。 在规则引擎中，将知识表达为规则（rules），要分析的情况定义为事实（facts）。二者在内存中的存储分别称为Production Memory和Working Memory，如下图：rules和facts是规则引擎接受的输入参数，而规则引擎本身包括两个组成部分：Pattern Matcher和Agenda。Pattern Matcher根据facts找到匹配的rules，Agenda管理PatternMatcher挑选出来的规则的执行次序。在外围，还会有一个执行引擎（Execution Engine）负责根据Agenda输出的rules执行具体的操作。 其中Pattern Matcher是规则引擎负责推理的核心。和人类的思维相对应，规则引擎中也存在两种推理方式：正向推理（Forward-Chaining）和反向推理（Backward-Chaining）。 正向推理也叫演绎法，由事实驱动，从 一个初始的事实出发，不断地应用规则得出结论。首先在候选队列中选择一条规则作为启用规则进行推理，记录其结论作为下一步推理时的证据。如此重复这个过程，直到再无可用规则可被选用或者求得了所要求的解为止。 反向推理也叫归纳法，由目标驱动，首先提出某个假设，然后寻找支持该假设的证据，若所需的证据都能找到，说明原假设是正确的；若无论如何都找不到所需要的证据，则说明原假设不成立，此时需要另做新的假设。 RETE算法 Drools-常用术语Rules（规则）The heart of the Rules Engine where you specify conditions (if ‘a’ then ‘b’). 业务调研中很重要的内容就是了解业务规则。在企业流程中，可能还会接触到流程规则。在IT技术领域，很多地方也应用了规则，比如路由表，防火墙策略，乃至角色权限控制(RBAC)，或者Web框架中的URL匹配。不管是哪种规则，都规定了一组确定的条件和此条件所产生的结果。 每条规则都是一组条件决定的一系列结果； 一条规则可能与其他规则共同决定最终结果； 可能存在条件互相交叉的规则，此时有必要规定规则的优先级； Facts（事实）Facts are the data on which the rules will act upon.From Java perspective, Facts are the POJO (Plain Old Java Object). KnowledgeBuilderKnowledgeBuilder是用来在业务代码中收集已经编好的规则，找到这些规则并把这些规则文件进行编译，最终产生一批编译好的KnowledgePackage（规则包）给其它的应用程序使用。创建KnowledgeBuilder对象使用的是KnowledgeBuilderFactory的newKnowledgeBuilder方法。 Knowledge Base Knowledge Base是管理一系列rules, processes和 internal types的接口. 它在org.drools.KnowledgeBase包中； KnowledgeBase 是Drools提供的用来收集应用当中知识（Knowledge）定义的知识库对象，在一个KnowledgeBase当中可以包含普通的规则、规则流、函数定义、用户自定义对象等； KnowledgeBase本身不包含任何业务数据对象(fact 对象)，业务对象都是插入到由KnowledgeBase产生的两种类型的session对象； 一系列Knowledge definitions组成knowledge packages； Knowledge definitions 可以新增和移除； Knowledge Base的主要目的是为了重用，因为Knowledge definitions成本很高； Knowledge Base 提供新建knowledge sessions的方法； Sessionknowledge session是激发规则的核心组件，knowledge session容纳所有规则和资源， knowledge session由KnowledgeBase创建。规则引擎工作时会将facts插入session中，当满足特定的condition时，对应的rule就会被激发。knowledge session从knowledge base中获取，是与drools规则引擎交互的主要接口，knowledge session有两种类型: Stateful Knowledge SessionStatefulKnowledgeSession对象是一种最常用的与规则引擎进行交互的方式，它可以与规则引擎建立一个持续的交互通道。StatefulKnowledgeSession执行完之后一定要调用dispose()方法释放资源。StatefulKnowledgeSession可以接受外部插入(insert方法)的业务数据——也叫fact，一个对象通常可以对应一个普通的POJO, 而一个POJO有若干个属性来描述这个业务对象，比如一个PeopleEvent中包含了isComing(人是否进来属性), roomed(房间号)，每一个属性对应getter和setter方法，供规则定义来使用（注意：在规则定义中isComing默认的getter方法是getIsComing()）。如果规则当中需要有数据传出，那么可以通过在StatefulKnowledgeSession当中设置global对象来实现，一个global对象也是一个普通的Java对象，在向StatefulKnowledgeSession当中设置global对象时不用insert方法而用setGlobal方法实现。 一些stateless session常用的用例： Monitoring 监控Stock market monitoring and analysis for semi-automatic buying. Diagnostics 诊断Fault finding, medical diagnostics LogisticsParcel tracking and delivery provisioning Stateless Knowledge SessionStateless Knowledge Session是一个无状态的session，形成最简单的用例，不利用推理Stateless Knowledge Session可以像function一样被调用，可传入一些数据和接收一些结果。一般 stateless session包括: Validation 验证Is this person eligible for a mortgage? Calculation 计算Compute a mortgage premium. Routing and Filtering 路由和过滤Filter incoming messages, such as emails, into folders.Send incoming messages to a destination Stateless和Stateful两种session的区别 StatelessKnowledgeSession是在StatefulKnowledgeSession基础上进行进一步的封装， StatelessKnowledgeSession跟StatefulKnowledgeSession的区别就是它不需要调用dispose方法释放内存资源了， StatelessKnowledgeSession不能重复的执行插入fact的操作、也不能重复的调用fireAllRules方法来执行所有的规则，因为它不能保存状态，对应的这些要完成的工作在StatelessKnowledgeSession当中只有execute方法，通过这个方法可以实现插入所有的fact并且可以同时执行所有的规则或规则流。 MVEL（MVFLEX Expression Language ）MVEL is a hybrid dynamic/statically typed, embeddable Expression Language and runtime for the Java Platform.MVEL is particularly ideal for restrictive environments that can’t use bytecode generation due to memory restrictions or sand boxing. Instead of trying to re-invent Java, it instead aims to provide a familiar syntax for Java programmers while also adding syntactic sugar for short and concise expressions. Rule Attributesruleflow-groupagenda-groupactivation-groupsalienceno-looplock-on-actionAgendaThe Agenda is a Rete feature. It maintains set of rules that are able to execute, its job is to schedule that execution in a deterministic order. It’s a logical concept. The agenda is the logical place where activations are waiting to be fired.Agenda是一个逻辑概念，逻辑上存储待执行规则（activations）。 Activations（被匹配的规则）Activations are the ‘then’ part of the rule. Activations are placed in the agenda where the appropriate rule is fired.Activations是rule的’then’部分，存储在Agenda中的已匹配但未激发的rule，即Activations； DRL规则文件的语法PackageEvery Rule starts with a package name. The package acts as a namespace for Rules. Rule names within a package must be unique. Packages in Rules are similar to packages in Java.package是Rules的命名空间，且必须唯一 Import statementWhatever facts you want to apply the rule on, those facts needs to be imported.导入定义Fatcs的className Rule Definition Rule consists of the Rule Name, the condition, and the Consequence. Drools keywords： rule, when, then, end. The when part is the condition in both the rules and the then part is the consequence. In rule terminology, the when part is also called as LHS (left hand side) and the then part as the RHS (right hand side) of the rule. Drools API规则引擎中，将知识表达为规则（rules），要分析的情况定义为事实（facts）。二者在内存中的存储分别称为Production Memory和Working Memory。在外围，还会有一个执行引擎（Execution Engine）。与此对应，规则引擎API也分成三个部分。在Drools中，分别叫做Knowledge API，Fact API和Execution API。 Knowledge APIDrools将知识库(KnowledgeBase)作为JSR94中的规则执行集(RuleExecutionSet)。知识库中的知识以包(KnowledgePackage)为单位组合而成。每个包中聚合多个规则(Rule)。通常，一个包中的内容会在一个或多个资源(Resource)中保存。资源的类型可以有很多种,如.drl 文件、.dslr 文件或 xls 文件等。规则包还可以从规则流(rule flow) 文件中获取。 Fact API要操作Working Memory，首先要建立规则引擎的一个会话。Drools中的有状态会话和无状态会话分别为StatefulKnowledgeSession和StatelessKnowledgeSession，都可以由KnowledgeBase建立。通过会话可以进行操作Fact对象，执行规则等交互。 Execution API插入到WorkingMemory中的对象，并不是克隆，而是对原对象的引用。这就意味着引擎中可以改变外部的对象，这是引擎与外部数据交互的一个通道。此外，insert()方法还会返回一个FactHandler，作为引擎中该Fact对象的一个句柄。最后，session上可以注册AgendaEventListener、ProcessEventListener和WorkingMemoryEventListener，这也是常用的交互方式。 参考资料JAVA规则引擎总结心内求法-规则引擎系列博客martinfowler-RulesEngine 问题记录drl逻辑编译问题据说drl中写的逻辑明明是对的，但是编译判断还是会有问题，待验证]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>Drools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch常用API]]></title>
    <url>%2F2016%2F07%2F30%2FElasticsearch%E5%B8%B8%E7%94%A8API%2F</url>
    <content type="text"><![CDATA[/_nodes/ 下有一个监控接口：curl -XGET ‘http://127.0.0.1:9200/_nodes/_local/hot_threads?interval=60s‘该接口会返回在 interval 时长内，该节点消耗资源最多的前三个线程的堆栈情况。这对于性能调优初期，采集现状数据，极为有用。默认的，资源消耗是按照 CPU 来衡量，还可以用 ?type=wait 或者 ?type=block 来查看在等待和堵塞状态的当前线程排名。 curl -XGET ‘http://v3es1:9200/_nodes/v3es1/stats/jvm‘查看：JVM stats, memory pool information, garbage collection, buffer pools, number of loaded/unloaded classes你看看目前jvm状态如何 如果你的 ES 集群监控里发现经常有很耗时的 GC，说明集群负载很重，内存不足。严重情况下，这些 GC 导致节点无法正确响应集群之间的 ping ，可能就直接从集群里退出了。然后数据分片也随之在集群中重新迁移，引发更大的网络和磁盘 IO，正常的写入和搜索也会受到影响。 动态修改Replicacurl -XPUT ‘localhost:9200/my_index/_settings’ -d ‘{ “index” : { “number_of_replicas” : 1 }}’ 修改 queue_sizehttps://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.htmlcurl -XPUT _cluster/settings -d ‘{ “persistent” : { “threadpool.bulk.queue_size” : 1000 }}’ 参考：http://jfzhang.blog.51cto.com/1934093/1685530 ES高频写优化配置http://edgeofsanity.net/article/2012/12/26/elasticsearch-for-logging.html]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JMS学习笔记]]></title>
    <url>%2F2016%2F07%2F27%2FJMS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[JMS即Java消息服务（Java Message Service）应用程序接口是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。Java消息服务是一个与具体平台无关的API，绝大多数MOM提供商都对JMS提供支持。 JMS (Java Message Service) is an API that provides the facility to create， send and read messages。It provides loosely coupled（松耦合）， reliable（可靠） and asynchronous（异步） communication。 JMS简介JMS全称是Java Message Service。其是JavaEE技术规范中的一个重要组成部分，是一种企业消息处理的规范。它的作用就像一个智能交换机，负责路由分布式应用中各个组件所发出的消息； JMS提供了一组通用的Java API，开发者可以通过API来 创建，发送，接收，读取 、消息； JMS是一种和具体实现厂商无关的API。它的作用类似于JDBC。不管底层采用何种消息服务器的实现，应用程序总是面向通用的JMS API编程； 常用的有apache的ActiveMQ，Jboss的HornetQ JMS优势1。 异步 Asynchronous: To receive the message， client is not required to send request。 Message will arrive automatically to the client。 消息采用异步处理机制，避免客户机等待。2。 可靠 Reliable: It provides assurance that message is delivered。JMS可以持久的保存消息，因而提高系统的可靠性。3。 效率：JMS允许一条消息同时发给多个接受者，更具效率。 JMS总体架构JMS的架构总体架构分3部分：1。 JMS服务器，路由消息的服务系统，广义上说就是服务器，比如JBOSS，GLASSFISH，WAS8；2。 JMS生产者，负责创建并发送消息的程序组件；3。 JMS消费者，负责读取并处理消息的程序组件。 JMS的消息机制模型JMS的消息机制模型主要分2类: 点对点PTP模型 PTP消息处理模型为应用中的各个逻辑处理单元提供可靠的通信支持； 在PTP通信中，JMS把每一个消息传递给一个消息消费者； JMS系统保证消息传递给消费者，消息不会同时被多个消费者接受； 如果消息消费者不在连接范围内，JMS会自动保证消息不会丢失。直到消息消费者进入连接，消息将自动送达。因此JMS需要将消息保存到永久介质上如数据库； 发布/订阅Pub-Sub模型 在这种模型中，每个消息被发送到一个消息主题，该主题可以拥有多个订阅者。 JMS系统负责将消息的副本传给该主题的每个订阅者。 Point-to-Point (PTP) Messaging DomainPoint-to-Point (PTP) Messaging Domain(点对点通信模型)是基于队列(Queue)的，对于PTP消息模型而言，它的消息目的是一个消息队列(Queue)，消息生产者每次发送消息总是把消息送入消息队列中，消息消费者总是从消息队列中读取消息。先进队列的消息将先被消息消费者读取。 In PTP model， one message is delivered to one receiver only。 Here，Queue is used as a message oriented middleware (MOM) 面向消息的中间件。The Queue is responsible to hold the message until receiver is ready。（串行）In PTP model， there is no timing dependency between sender and receiver。 PTP模型的对象的主要概念和方法Queue（队列）Queue由JMS Provider 管理，队列由队列名识别，客户端可以通过JNDI 接口用队列名得到一个队列对象。 TemporaryQueue（临时队列）由QueueConnection 创建，而且只能由创建它的QueueConnection 使用。 QueueConnectionFactory客户端用QueueConnectionFactory 创建QueueConnection 对象。 QueueConnection一个到JMS PTP provider 的连接，客户端可以用QueueConnection 创建QueueSession 来发送和接收消息。 QueueSessionQueueSession提供一些方法创建QueueReceiver，QueueSender，QueueBrowser 和TemporaryQueue。如果在QueueSession 关闭时，有一些消息已经被收到，但还没有被签收(acknowledged)，那么，当接收者下次连接到相同的队列时，这些消息还会被再次接收。 QueueReceiver客户端用QueueReceiver 接收队列中的消息，如果用户在QueueReceiver中设定了消息选择条件，那么不符合条件的消息会留在队列中，不会被接收到。 QueueSender客户端用QueueSender 发送消息到队列 QueueBrowser客户端可以QueueBrowser 浏览队列中的消息，但不会收走消息。 QueueRequestorJMS 提供QueueRequestor 类简化消息的收发过程。QueueRequestor 的构造函数有两个参数:QueueSession 和queue，QueueRequestor 通过创建一个临时队列来完成最终的收发消息请求。 Reliability可靠性队列可以长久地保存消息直到接收者收到消息。接收者不需要因为担心消息会丢失而时刻和队列保持激活的连接状态，充分体现了异步传输模式的优势。 Publisher/Subscriber (Pub/Sub) Messaging DomainJMS Publisher/Subscriber (Pub/Sub) Messaging Domain(出版者/订阅者模型)模型定义了如何向一个内容节点发布和订阅消息，这些节点被称作主题(topic)。 主题可以被认为是消息的传输中介； 发布者(publisher)发布消息到主题； 订阅者(subscribe) 从主题订阅消息； 主题使得消息订阅者和消息发布者保持互相独立，不需要接触即可保证消息的传送。 In Pub/Sub model， one message is delivered to all the subscribers。 It is like broadcasting。 Here，Topic（主题） is used as a message oriented middleware that is responsible to hold and deliver messages。In PTP model， there is timing dependency between publisher and subscriber。 JMS Pub/Sub 模型中的主要概念和对象subscription（订阅）消息订阅分为非持久订阅(non-durable subscription)和持久订阅(durable subscrip-tion)： 非持久订阅只有当客户端处于激活状态，也就是和JMS Provider 保持连接状态才能收到发送到某个主题的消息，而当客户端处于离线状态，这个时间段发到主题的消息将会丢失，永远不会收到。 持久订阅时，客户端向JMS 注册一个识别自己身份的ID，当这个客户端处于离线时，JMS Provider 会为这个ID 保存所有发送到主题的消息，当客户再次连接到JMS Provider时，会根据自己的ID 得到所有当自己处于离线时发送到主题的消息。 Topic（主题） Topic主题由JMS Provider 管理， 主题由主题名识别 客户端可以通过JNDI 接口用主题名得到一个主题对象。 JMS没有给出主题的组织和层次结构的定义，由JMS Provider 自己定义 TemporaryTopic（临时主题）临时主题由TopicConnection创建，而且只能由创建它的TopicConnection使用。临时主题不能提供持久订阅功能。 TopicConnectionFactory客户端用TopicConnectionFactory创建TopicConnection对象。 TopicConnectionTopicConnection是一个到JMS Pub/Sub provider的连接，客户端可以用TopicConnection创建TopicSession 来发布和订阅消息。 TopicSessionTopicSession 提供一些方法创建TopicPublisher，TopicSubscriber，TemporaryTopic。它还提供unsubscribe方法取消消息的持久订阅。 TopicPublisher客户端用TopicPublisher 发布消息到主题。 TopicSubscriber客户端用TopicSubscriber 接收发布到主题上的消息。可以在TopicSubscriber 中设置消息过滤功能，这样，不符合要求的消息不会被接收。 Durable TopicSubscriber如果一个客户端需要持久订阅消息，可以使用Durable TopicSubscriber，TopSession 提供一个方法createDurableSubscriber创建Durable TopicSubscriber 对象。 Recovery and Redelivery（恢复和重新派送）恢复和重新派送非持久订阅状态下，不能恢复或重新派送一个未签收的消息。只有持久订阅才能恢复或重新派送一个未签收的消息 TopicRequestor JMS 提供TopicRequestor 类简化消息的收发过程。 TopicRequestor的构造函数有两个参数:TopicSession和topic。 TopicRequestor 通过创建一个临时主题来完成最终的发布和接收消息请求。 Reliability（可靠性） 当所有的消息必须被接收，则用持久订阅模式。 当丢失消息能够被容忍，则用非持久订阅模式。 JMS Programming Model（JMS编程模型） ConnectionFactory（连接工厂）它由服务器管理员创建，并绑定到JNDI树上，JMS客户端使用JNDI查找，定位连接工厂，然后利用连接工厂创建JMS连接。 Connection（JMS连接）连接表示客户机和服务器之间的活动连接。JMS通过连接工厂创建连接。JMS是一个相当重要的对象。通常，每个客户机使用单独的连接，而每个连接则可以连接多个JMS目的。 Session（JMS会话）会话表示客户机与JMS服务器之间的通信状态。JMS会话建立在连接之上，表示JMS客户机与服务器之间的通信线程。会话定义了消息的顺序。JMS使用会话进行事务性的消息处理。 Destination（JMS消息目的地）Destination即消息生产者发送消息的目的地，也就是消息消费者获取消息的消息源。 Message Producer （JMS消息生产者）消息生产者负责创建消息并将消息发送到消息目的。 Message Consumer （JMS消息消费者）消息消费者负责接收消息并读取消息内容。 JMS消息的确认方式消息的确认是指消息接受者接到消息，并做出了对应的处理之后，它将回送一个确认消息。对于 非事务性 会话，创建会话时应该指定确定方式，JMS定义了3种确认方式: Auto_ACKnowledge 自动通知对于同步消费者，Receive方法调用返回，且没有异常发生时，将自动对收到的消息予以确认。对于异步消息，当onMessage方法返回，且没有异常发生时，即对收到的消息自动确认。 Client_AcKnowledge 客户端自行决定通知时机这种方式要求客户端使用javax。jms。Message。acknowledge()方法完成确认。 Dups_OK_ACKnowledge 延时/批量通知这种确认方式允许JMS不必急于确认收到的消息，允许在收到多个消息之后一次完成确认，与Auto_AcKnowledge相比，这种确认方式在某些情况下可能更有效，因为没有确认，当系统崩溃或者网络出现故障的时候，消息可以被重新传递。 参考阅读Sun Java System Message Queuejms-tutorial]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>JMS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch批量插入测试]]></title>
    <url>%2F2016%2F07%2F25%2FElasticsearch%E6%89%B9%E9%87%8F%E6%8F%92%E5%85%A5%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[Elasticsearch批量插入测试数据：10万结构化document（json），每个1.3k；测试结果：bulkProcess性能最优(4s)，bulkRequest次之（1m）；indexAPI适用于单条插入。测试项目源码见me.demo.elasticsearch 测试环境两个Node(8G/4核) JMeter测试框架TODO：模拟http测试rest 单条插入测试以IndicesAdminClient新建Index,Type；以IndexAPI插入document(fields); IndicesAdminClient Index API 批量插入性能测试以Bulk API进行手动批量插入，或采用BulkProcessor 进行自动分段插入； bulk-api bulk-processor bulk-api-examples 问题记录EsRejectedExecutionExceptionbulkProcess执行成功，但设置.setConcurrentRequests(4)后，断开线程连接时会抛出如下错误1transport: [Orchid] failed to notify response handler on rejection 解决：todo ProcessClusterEventTimeoutException123456789101112131415161718192021222324252627282930313233343536373839################################################################### /etc/elasticsearch/elasticsearch.yml## Base configuration for a write heavy cluster# Force all memory to be locked, forcing the JVM to never swapbootstrap.mlockall: true## Threadpool Settings ### Search poolthreadpool.search.type: fixedthreadpool.search.size: 20threadpool.search.queue_size: 100# Bulk poolthreadpool.bulk.type: fixedthreadpool.bulk.size: 60threadpool.bulk.queue_size: 300# Index poolthreadpool.index.type: fixedthreadpool.index.size: 20threadpool.index.queue_size: 100# Indices settingsindices.memory.index_buffer_size: 30%indices.memory.min_shard_index_buffer_size: 12mbindices.memory.min_index_buffer_size: 96mb# Cache Sizesindices.fielddata.cache.size: 15%indices.fielddata.cache.expire: 6hindices.cache.filter.size: 15%indices.cache.filter.expire: 6h# Indexing Settings for Writesindex.refresh_interval: 30sindex.translog.flush_threshold_ops: 50000 请教个es插入速度优化的问题，刚开始很快，但是现在目前每秒只能平均处理2个doc，bulkProcess批量插入document，每个doc 25个字段，每个doc大小4k左右，3个Node(6个share,1个replica)；node配置（8G,4核）配置了ES_HEAP_SIZE=3G ElasticSearch索引优化http://m635674608.iteye.com/blog/2289439]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch集群部署]]></title>
    <url>%2F2016%2F07%2F23%2FElasticsearch%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Elasticsearch V2.3.4 集群部署记录：3个Node，6个Shard，2个Replica，能保证一台服务器宕机服务还能正常运行，且后期容易扩展到6个Node； 安装Javasudo yum install java-1.8.0-openjdk.x86_64Centos7自带openJdk8 swap配置swapping_is_the_death_of_performance,ElasticSearch建议将 /proc/sys/vm/swappiness 设置为 0。默认设置为30。查看当前swap分区设置cat /proc/sys/vm/swappiness临时修改值：sudo sysctl vm.swappiness=0 / sudo swapoff -a永久修改值：vim /etc/sysctl.conf，在最后加一行vm.swappiness = 0 修改hostname设置hostname为v3es1，并查看修改结果1234#es1hostnamectl set-hostname es1hostnamectl set-hostname es1 --statichostnamectl status 1234#es2hostnamectl set-hostname es2hostnamectl set-hostname es2 --statichostnamectl status 1234#es3hostnamectl set-hostname es3hostnamectl set-hostname es3 --statichostnamectl status vim /etc/hosts1234# es节点配置193.168.201.213 es1.es.com es1193.168.201.85 es2.es.com es2193.168.201.117 es3.es.com es3 局域网hosts配置1234567# 局域网hosts配置es193.168.201.223 es1.es.com193.168.201.223 www.es1.es.com193.168.201.252 es2.es.com193.168.201.252 www.es2.es.com193.168.201.117 es3.es.com193.168.201.117 www.es3.es.com 关闭防火墙1234#停止firewallsystemctl stop firewalld.service#禁止firewall开机启动 systemctl disable firewalld.service 新增用户12345678910mkdir -p /data/elasticsearch/&#123;data,work,plugins,scripts&#125; useradd elasticsearch -s /bin/bash # 1passwd elasticsearchvim /etc/sudoers # 新增elasticsearch ALL=(ALL) ALL# 授权mkdir /var/log/elasticsearchchown -R elasticsearch:elasticsearch /var/log/elasticsearch /data/elasticsearch 安装elasticsearchhow-to-install-and-configure-elasticsearch-on-centos-7 下载rpm包cd /mntwget https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/rpm/elasticsearch/2.3.4/elasticsearch-2.3.4.rpm yum本地安装cd /mnt &amp;&amp; sudo yum localinstall elasticsearch-2.3.4.rpm 设置自启动sudo systemctl daemon-reloadsudo systemctl enable elasticsearch.service 修改配置文件vim /etc/elasticsearch/elasticsearch.yml 1234567891011121314cluster.name: cloudx_webnode.name: $&#123;HOSTNAME&#125;node.master: truenode.data: trueindex.number_of_shards: 6index.number_of_replicas: 2network.host: $&#123;HOSTNAME&#125;index.max_result_window: 10000000discovery.zen.minimum_master_nodes: 2discovery.zen.ping.unicast.hosts: [es1,es2,es3]discovery.zen.ping.multicast.enabled: truepath.data: /data/elasticsearch http.cors.allow-origin: "*"http.cors.enabled: true 复制到集群：scp /etc/elasticsearch/elasticsearch.yml root@es1:/etc/elasticsearch/ 理想状态下，3个Node，6个Shard，2个Replica，能保证一台服务器宕机服务还能正常运行，且后期容易扩展到6个Node；但如果存储成本高，可改为1个Replica另配置discovery.zen.minimum_master_nodes=2可保证不会出现脑裂问题； 3个节点正常运行节点3挂了replica自动复制迁移节点3恢复正常 配置elasticsearch heap大小vim /etc/sysconfig/elasticsearch12ES_HEAP_SIZE=3GMAX_OPEN_FILES=65535 启动服务su elasticsearchsudo systemctl start elasticsearch.service 查看运行状态service elasticsearch status 12345678● elasticsearch.service - Elasticsearch Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; enabled; vendor preset: disabled) Active: active (running) since 日 2016-07-24 19:56:07 CST; 8min ago Docs: http://www.elastic.co Process: 9370 ExecStartPre=/usr/share/elasticsearch/bin/elasticsearch-systemd-pre-exec (code=exited, status=0/SUCCESS) Main PID: 9374 (java) CGroup: /system.slice/elasticsearch.service └─9374 /bin/java -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseC... 测试是否成功curl -X GET ‘http://localhost:9200‘curl -X GET ‘http://193.168.201.250:9200‘curl -X GET ‘http://193.168.201.252:9200‘curl -X GET ‘http://193.168.201.117:9200‘ 插入测试curl -X POST ‘http://es1:9200/tutorial/helloworld/1‘ -d ‘{ “message”: “Hello World!” }’curl -X GET ‘http://es1:9200/tutorial/helloworld/1?pretty‘ 查看日志tail -f 200 /var/log/elasticsearch/cloudx_web.log 安装信息 安装目录：/usr/share/elasticsearch/ 配置目录：/etc/elasticsearch 启动初始化脚本：/etc/init.d/elasticsearch 日志目录：/var/log/elasticsearch 配置参数(https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html) 删除elasticsearchyum remove elasticsearchfind / -name &quot;elasticsearch&quot; -exec rm -rf {} \; ElasticSearch插件Elastic-HQ Monitoring, Management, and Querying Web Interface for ElasticSearch instances and clusters.Benefits: Active real-time monitoring of ElasticSearch clusters and nodes. Manage Indices, Mappings, Shards, Aliases, and Nodes. Query UI for searching one or multiple Indices. REST UI, eliminates the need for cURL and cumbersome JSON formats. No software to install/download. 100% web browser-based. Optimized to work on mobile phones, tablets, and other small screen devices. Easy to use and attractive user interface. Free (as in Beer) 安装：cd /usr/share/elasticsearch/bin &amp;&amp; ./plugin install royrusso/elasticsearch-HQ 地址：http://es1.es.com9200/_plugin/hq/ ElasticSearch-Kopf Kopf是一个ElasticSearch的管理工具，它也提供了对ES集群操作的API。 安装：cd /usr/share/elasticsearch/bin &amp;&amp; ./plugin install lmenezes/elasticsearch-kopf 地址：http://es1.es.com:9200/_plugin/kopf/ Elasticsearch-head A web front end for an Elasticsearch cluster，查看集群状态 安装：cd /usr/share/elasticsearch/bin &amp;&amp; ./plugin install mobz/elasticsearch-head ElasticSearch-Paramedic Paramedic is a simple yet sexy tool to monitor and inspect ElasticSearch clusters.It displays real-time statistics and information about your nodes and indices, as well as shard allocation within the cluster.The application is written in JavaScript, using the Ember.js framework for sanity and the Cubism.js library for visuals. While the project is useful, the codebase, with most logic in controllers, lacking proper component separation and test suite, can’t be considered mature enough, yet. 安装：cd /usr/share/elasticsearch/bin &amp;&amp; ./plugin install karmi/elasticsearch-paramedic/ 地址：http://es1.es.com:9200/_plugin/paramedic ElasticSearch-Whatson Whatson is an elasticsearch plugin to visualize the state of a cluster. It’s inpired by other excellent plugins:xyu/elasticsearch-whatson 安装：cd /usr/share/elasticsearch/bin &amp;&amp; ./plugin install xyu/elasticsearch-whatson 地址：http://es1.es.com:9200/_plugin/whatson 安装问题服务启动问题-Failed to created node environment启动权限问题，必须以elasticsearch用户启动服务，不能以root启动 配置问题-discovery.zen.ping.unicast.hostsLikely root cause: java.net.UnknownHostException: v3es1: unknown errorhostname -f查看fqdn ElasticHQ安装问题关闭防火墙，设置好fqdn，关闭VPN discovery.zen.ping.unicast.hosts中的hostname ping不通日志 12345discovery.zen.ping.unicast] [es1] failed to send ping to [&#123;#zen_unicast_3#&#125;&#123;193.168.201.117&#125;&#123;es3/193.168.201.117:9300&#125;]SendRequestTransportException[internal:discovery/zen/unicast]]; nested: NodeNotConnectedException[[][es3/193.168.201.117:9300] Node not connected];at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:340)[action.admin.cluster.health] [es1] no known master node, scheduling a retry 解决：hostname忘记设置了，(╯□╰)]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch学习笔记]]></title>
    <url>%2F2016%2F07%2F22%2FElasticsearch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Elasticsearch是一个基于Apache Lucene(TM)的开源搜索引擎。Lucene非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。不过，Elasticsearch不仅仅是Lucene和全文搜索，我们还能这样去描述它： 分布式的实时文件存储，每个字段都被索引并可被搜索 分布式的实时分析搜索引擎 可以扩展到上百台服务器，处理PB级结构化或非结构化数据而且，所有的这些功能被集成到一个服务里面，你的应用可以通过简单的RESTful API、各种语言的客户端甚至命令行与之交互。 在Elasticsearch中，文档归属于一种类型(type),而这些类型存在于索引(index)中，类比传统关系型数据库： Relational DB -&gt; Databases -&gt; Tables -&gt; Rows -&gt; Columns Elasticsearch -&gt; Indices -&gt; Types -&gt; Documents -&gt; Fields Elasticsearch集群可以包含多个索引(indices)（数据库），每一个索引可以包含多个类型(types)（表），每一个类型包含多个文档(documents)（行），然后每个文档包含多个字段(Fields)（列）。 Document 文档程序中大多的实体或对象能够被序列化为包含键值对的JSON对象，键(key)是字段(field)或属性(property)的名字，值(value)，可以是字符串、数字、布尔类型、另一个对象、值数组或者其他特殊类型，比如表示日期的字符串或者表示地理位置的对象。 通常，我们可以认为对象(object)和文档(document)是等价相通的。但他们还是有所差别： 对象(Object)是一个JSON结构体——类似于哈希、hashmap、字典或者关联数组； 对象(Object)中还可能包含其他对象(Object)。 Elasticsearch中，文档(document)这个术语有着特殊含义，它特指最顶层结构或者根对象(root object)序列化成的JSON数据（以唯一ID标识并存储于Elasticsearch中）。 文档元数据一个文档不只有数据。它还包含了元数据(metadata)——关于文档的信息。三个必须的元数据节点是： 节点 说明 _index 文档存储的地方 _type 文档代表的对象的类 _id 文档的唯一标识 _index索引(index)类似于关系型数据库里的“数据库”——它是我们存储和索引关联数据的地方。 事实上，我们的数据被存储和索引在分片(shards)中，索引只是一个把一个或多个分片分组在一起的逻辑空间。然而，这只是一些内部细节——我们的程序完全不用关心分片。我们唯一需要做的仅仅是选择一个索引名。这个名字必须是全部小写，不能以下划线开头，不能包含逗号。 _type在应用中，我们使用对象表示一些“事物”，例如一个用户、一篇博客、一个评论，或者一封邮件。每个对象都属于一个类(class)，这个类定义了属性或与对象关联的数据。user类的对象可能包含姓名、性别、年龄和Email地址。 在Elasticsearch中，我们使用相同类型(type)的文档表示相同的“事物”，因为他们的数据结构也是相同的。 每个类型(type)都有自己的映射(mapping)或者结构定义，就像传统数据库表中的列一样。 所有类型下的文档被存储在同一个索引下，但是类型的映射(mapping)会告诉Elasticsearch不同的文档如何被索引。 _type的名字可以是大写或小写，不能包含下划线或逗号。我们将使用blog做为类型名。 _idid仅仅是一个字符串，它与_index和_type组合时，就可以在Elasticsearch中唯一标识一个文档。当创建一个文档，你可以自定义_id，也可以让Elasticsearch帮你自动生成。 Index 索引文档通过index API被索引——使数据可以被存储和搜索。但是首先需要决定文档所在。文档通过其_index、_type、_id唯一确定。 Elasticsearch CRUD检索request：GET /{_index}/{_type}/{_id}?pretty，如/website/blog/123?pretty；response：123456789101112&#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 1, "found" : true, "_source" : &#123; "title": "My first blog entry", "text": "Just trying this out...", "date": "2014/01/01" &#125;&#125; pretty 在任意的查询字符串中增加pretty参数，类似于上面的例子。会让Elasticsearch美化输出(pretty-print)JSON响应以便更加容易阅读。_source字段不会被美化，它的样子与我们输入的一致。 检索部分文档通常，GET请求将返回文档的全部，存储在_source参数中。 请求个别字段可以使用_source参数。多个字段可以使用逗号分隔，如GET /website/blog/123?_source=title,text； 只想得到_source字段而不要其他的元数据，你可以这样请求：GET /website/blog/123/_source； 检查文档是否存在如果你想做的只是检查文档是否存在——你对内容完全不感兴趣——使用HEAD方法来代替GET。HEAD请求不会返回响应体，只有HTTP头： 存在返回200 OK状态： 不存在返回404 Not Found： mget批量查询从Elasticsearch中检索多个文档，相对于逐条get检索，更快的方式是在一个请求中使用multi-get或者mget API。 更新文档在Elasticsearch中是不可变的——我们不能修改他们。如果需要更新已存在的文档，可以使用index API 重建索引(reindex) 或者替换掉它。在内部，Elasticsearch已经标记旧文档为删除并添加了一个完整的新文档。旧版本文档不会立即消失，但你也不能去访问它。Elasticsearch会在你继续索引更多数据时清理被删除的文档。 创建当索引一个文档，如何确定是完全创建了一个新的还是覆盖了一个已经存在的呢？请记住_index、_type、_id三者唯一确定一个文档。 Elasticsearch自动生成唯一_id所以要想保证文档是新加入的，最简单的方式是使用POST方法让Elasticsearch自动生成唯一_id：POST /website/blog/ 自定义的_id如果使用自定义的_id，必须告诉Elasticsearch应该在_index、_type、_id三者都不同时才接受请求。有两种实现方式 使用op_type查询参数：PUT /website/blog/123?op_type=create； 在URL后加/_create做为端点：PUT /website/blog/123/_create； 响应结果 如果请求成功的创建了一个新文档，Elasticsearch将返回正常的元数据且响应状态码是201 Created； 如果包含相同的_index、_type和_id的文档已经存在，Elasticsearch将返回409 Conflict响应状态码； 删除使用DELETE方法：DELETE /website/blog/123； 如果文档被找到，Elasticsearch将返回200 OK状态码和以下响应体。注意_version数字已经增加了。 如果文档未找到，我们将得到一个404 Not Found状态码， . 尽管文档不存在——“found”的值是false——_version依旧增加了。这是内部记录的一部分，它确保在多节点间不同操作可以有正确的顺序。 . 除一个文档也不会立即从磁盘上移除，它只是被标记成已删除。Elasticsearch将会在你之后添加更多索引的时候才会在后台进行删除内容的清理。 批量操作就像mget允许一次性检索多个文档一样，bulk API允许我们使用单一请求来实现多个文档的create、index、update或delete‘；这对索引类似于日志活动这样的数据流非常有用，它们可以以成百上千的数据为一个批次按序进行索引。bulk请求体如下，它有一点不同寻常： 1234&#123; action: &#123; metadata &#125;&#125;\n&#123; request body &#125;\n&#123; action: &#123; metadata &#125;&#125;\n&#123; request body &#125;\n 这种格式类似于用”\n”符号连接起来的一行一行的JSON文档流(stream)。两个重要的点需要注意： 每行必须以”\n”符号结尾，包括最后一行。这些都是作为每行有效的分离而做的标记。 每一行的数据不能包含未被转义的换行符，它们会干扰分析——这意味着JSON不能被美化打印。 为什么bulk API需要带换行符的奇怪格式，而不是像mget API一样使用JSON数组？在分布式环境下，Elasticsearch则从网络缓冲区中一行一行的直接读取数据。它使用换行符识别和解析action/metadata行，以决定哪些分片来处理这个请求。这利于任务分解，可以减少JSON序列化RAM消耗，从而降低JVM GC时间； 行为(action)action/metadata这一行定义了文档行为(what action)发生在哪个文档(which document)之上。 行为(action)必须是以下几种： 行为 解释 create 当文档不存在时创建之。 index 创建新文档或替换已有文档。 update 局部更新文档。 delete 删除一个文档。 在索引、创建、更新或删除时必须指定文档的_index、_type、_id这些元数据(metadata)。 请求体(request body)请求体由文档的_source组成——文档所包含的一些字段以及其值，即提供文档用来检索。 update操作需要请求体，请求体的组成应该与update API（doc, upsert, script等等）一致。 delete操作不需要请求体(request body)。 响应结果每个子请求都被独立的执行，所以一个子请求的错误并不影响其它请求。如果任何一个请求失败，顶层的error标记将被设置为true，然后错误的细节将在相应的请求中被报告： 请求最佳大小（sweetspot）整个批量请求需要被加载到接受我们请求节点的内存里，所以请求越大，给其它请求可用的内存就越小。有一个最佳的bulk请求大小。超过这个大小，性能不再提升而且可能降低。最佳大小，当然并不是一个固定的数字。它完全取决于硬件、文档的大小和复杂度以及索引和搜索的负载。 如何找到最佳点(sweetspot)： 试着批量索引标准的文档，随着大小的增长，当性能开始降低，说明每个批次的大小太大了。 开始的数量可以在1000~5000个文档之间，如果文档非常大，可以使用较小的批次。 通常着眼于请求批次的物理大小是非常有用的。一千个1kB的文档和一千个1MB的文档大不相同。一个好的批次最好保持在5-15MB大小间。 Shard和Replicashard分片每个Index（对应Database）包含多个Shard，默认是5个，分散在不同的Node上，但不会存在两个相同的Shard存在一个Node上，这样就没有备份的意义了。Shard是一个最小的Lucene索引单元。当插入document的时候，Elasticsearch通过对docid进行hash来确定其放在哪个shard上面，然后在shard上面进行索引存储。shard代表索引分片，es可以把一个完整的索引分成多个分片，这样的好处是可以把一个大的索引拆分成多个，分布到不同的节点上。构成分布式搜索。分片的数量只能在索引创建前指定，并且索引创建后不能更改。 replica副本replicas就是备份，Elasticsearch采用的是Push Replication模式，当你往 master主分片上面索引一个文档，该分片会复制该文档(document)到剩下的所有 replica副本分片中，这些分片也会索引这个文档。es可以设置多个索引的副本，副本的作用一是提高系统的容错性，当个某个节点某个分片损坏或丢失时可以从副本中恢复。二是提高es的查询效率，es会自动对搜索请求进行负载均衡。 gateway代表es索引的持久化存储方式，es默认是先把索引存放到内存中，当内存满了时再持久化到硬盘。当这个es集群关闭再 重新启动时就会从gateway中读取索引数据。es支持多种类型的gateway，有本地文件系统（默认），分布式文件系统，Hadoop的HDFS和 amazon的s3云存储服务。 discovery.zen代表es的自动发现节点机制，es是一个基于p2p的系统，它先通过广播寻找存在的节点，再通过多播协议来进行节点之间的通信，同时也支持点对点的交互。 Transport代表es内部节点或集群与客户端的交互方式，默认内部是使用tcp协议进行交互，同时它支持http协议（json格式）、thrift、servlet、memcached、zeroMQ等的传输协议（通过插件方式集成）。 analyzer分布式搜索elasticsearch中文分词集成：elasticsearch官方只提供smartcn这个中文分词插件，效果不是很好，好在国内有medcl大神写的两个中文分词插件，一个是IKAnalyzer分词插件的，一个是mmseg的 IKAnalyzer分词安装下载elasticsearch-analysis-ik源码并打包12345git clone https://github.com/medcl/elasticsearch-analysis-ikcd elasticsearch-analysis-ikmvn cleanmvn compilemvn package 拷贝和解压release下的文件: #{project_path}/elasticsearch-analysis-ik/target/releases/elasticsearch-analysis-ik-*.zip 到你的 elasticsearch 插件目录, 如: plugins/ik 重启elasticsearch IKAnalyzer测试中文分词IKAnalyzer分词测试：http://es1.es.com:9200/mycompany/_analyze?analyzer=ik&amp;pretty=true&amp;text=中华人民共和国国歌 对指定field建全文索引 对index=cloudx_web_v3,type=T_EVENT_LOG的name字段以IKAnalyzer建全文索引curl -XPUT http://es1.es.com:9200/cloudx_web_v3/T_EVENT_LOG/_mapping?pretty -d ‘{“T_EVENT_LOG”:{“properties”:{“name”:{“type”:”string”,”analyzer”:”ik”,”search_analyzer”:”ik”}}}}’ 测试索引：curl -XPOST http://es1.es.com:9200/cloudx_web_v3/_search?pretty -d ‘{“query”:{“match”:{“name”:{“query”:”规则 专用”,”operator”:”and”}}}}’ 问题记录field must be set when search_analyzer is set 请求参数：curl -XPUT http://es1.es.com:9200/cloudx_web_v3/T_EVENT_LOG/_mapping?pretty -d ‘{“T_EVENT_LOG”:{“properties”:{“name”:{“type”:”string”,”indexAnalyzer”:”ik”,”searchAnalyzer”:”ik”}}}}’ 问题日志：{“error”:{“root_cause”:[{“type”:”mapper_parsing_exception”,”reason”:”analyzer on field [name] must be set when search_analyzer is set”}],”type”:”mapper_parsing_exception”,”reason”:”analyzer on field [name] must be set when search_analyzer is set”},”status”:400} 问题解决：参考旧版本教程的坑，V2.3.4的参数改了，应为curl -XPUT http://es1.es.com:9200/cloudx_web_v3/T_EVENT_LOG/_mapping?pretty -d ‘{“T_EVENT_LOG”:{“properties”:{“name”:{“type”:”string”,”analyzer”:”ik”,”search_analyzer”:”ik”}}}}’ 常见问题如何合理设置shard和replicaElasticSearch性能调优-Shard数 shard数 shard数==node数时此时性能最佳，但是由于ElasticSearch的不可变性（Immutable）的限制，系统无法对Shard进行重新拆分分配，除非重新索引这个文件集合，而重建索引开销巨大,所以，为了支持未来可能的水平扩展，一般会为集群分配比node数更多的shard数，也就是说每个节点会有多个Shard。 shard数==node数*2shard过多就会引入另外一系列的性能问题，如对于任意一次完整的搜索，ElasticSearch会分别对每个shard进行查询，最后进行汇总。当节点数和shard数是一对一的时候，所有的查询可以并行运行。但是，对于具有多个shard的节点，如果磁盘是15000RPM或SSD，可能会相对较快，但是这也会存在等待响应的问题，所以通常不推荐一个节点超过2个shard。 replica数Replica也是Shard，与shard不同的是，replica只会参与读操作，同时也能提高集群的可用性。对于Replica来说，它的主要作用就是提高集群错误恢复的能力，所以replica的数目与shard的数目以及node的数目相关，与shard不同的是，replica的数目可以在集群建立之后变更，且代价较小，所以相比shard的数目而言，没有那么重要。3 node, 3 shard, 1 replica (each)，2个node宕机，服务仍然正常运行。 如何防止elasticsearch的脑裂问题 如果刚开始使用elasticsearch，建议配置一个3节点集群。这样你可以设置minimum_master_nodes为2，减少了脑裂的可能性，但仍然保持了高可用的优点：你可以承受一个节点失效但集群还是正常运行的。但如果已经运行了一个两节点elasticsearch集群怎么办？可以选择为了保持高可用而忍受脑裂的可能性，或者选择为了防止脑裂而选择高可用性。为了避免这种妥协，最好的选择是给集群添加一个节点。这听起来很极端，但并不是。对于每一个elasticsearch节点你可以设置node.data参数来选择这个节点是否需要保存数据。缺省值是“true”，意思是默认每个elasticsearch节点同时也会作为一个数据节点。在一个两节点集群，你可以添加一个新节点并把node.data参数设置为“false”。这样这个节点不会保存任何分片，但它仍然可以被选为主（默认行为）。因为这个节点是一个无数据节点，所以它可以放在一台便宜服务器上。现在你就有了一个三节点的集群，可以安全的把minimum_master_nodes设置为2，避免脑裂而且仍然可以丢失一个节点并且不会丢失数据。 If discovery.zen.master_election.filter_client is true pings from client nodes (nodes where node.client is true, or both node.data and node.master are false) are ignored during master election; the default value is true. pings from non-master-eligible data nodes (nodes where node.data is true and node.master is false) are ignored during master election; the default value is false. Pings from master-eligible nodes are always observed during master election. Nodes can be excluded from becoming a master by setting node.master to false.Note, once a node is a client node (node.client set to true), it will not be allowed to become a master (node.master is automatically set to false). master和data同时配置会产生一些神奇的效果： 当master为false，而data为true时，会对该节点产生严重负荷； 当master为true，而data为false时，该节点作为一个协调者； 当master为false，data也为false时，该节点就变成了一个负载均衡器。 如何给Elasticsearch设置合适的Heap内存设置Elastic Heap内存（for Elasticsearch faster GCs）：export ES_HEAP_SIZE=3g，建议分配可用内存的50%给ElasticSearch（如aggregating on analyzed string fields比较少的话，可以设置更小，以留下足够的空闲内存给lucene），注意Lucene的segments是不可变的，为提高聚合和倒排索引的性能，lucene需要占用系统内存作为缓存 如何删除document中的重复数据？minimizing-document-duplication-in-elasticsearchhttp://es1.es.com:9200/cloudx_web_v3/T_EVENT_LOG/_search?pretty=true -d {json}123456789101112131415&#123; "aggs": &#123; "duplicateCount": &#123; "terms": &#123; "field": "name", "min_doc_count": 2 &#125;, "aggs": &#123; "duplicateDocuments": &#123; "top_hits": &#123;&#125; &#125; &#125; &#125; &#125;&#125; ES参考资料 Richaaaard的Elasticsearch系列教程 ES调优参数列表]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日志分析ELK]]></title>
    <url>%2F2016%2F07%2F21%2F%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90ELK%2F</url>
    <content type="text"><![CDATA[Elasticsearch 架构以及源码概览 开源实时日志分析 ELK 平台由 ElasticSearch 、Logstash 和 Kiabana 三个开源工具组成。官方网站 Elasticsearch 是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制， restful 风格接口，多数据源，自动搜索负载等。 Logstash 是一个完全开源的工具，他可以对你的日志进行收集、分析，并将其存储供以后使用（如，搜索）。 kibana 也是一个开源和免费的工具，他 Kibana 可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。 安装配置Docker…]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j索引笔记之SchemaIndex和LegacyIndex]]></title>
    <url>%2F2016%2F05%2F01%2FNeo4j%E7%B4%A2%E5%BC%95%E7%AC%94%E8%AE%B0%E4%B9%8BSchemaIndex%E5%92%8CLegacyIndex%2F</url>
    <content type="text"><![CDATA[neo4j包含schema indexes 和 legacy indexes两种类型，两者理念不同且不可互换或兼容，实际应用中应明确检索需求后采用合适的索引。 schema index vs legacy index参考neo4j index-confusion schema index和legacy index 都是基于lucene实现； 如果你正在使用Neo4j 2.0或者更高版本并且不需要支持2.0版本之前legacy index的代码，那么请只使用schema index同时避免legacy index； 如果你不得不使用Neo4j的早期版本，并且无法升级，无论如何你都只有一种索引可以选择（legacy index）； 如果你需要全文检索的索引，不管是什么版本，都将使用legacy index。 schema index（schema based indexes）Neo4j is a schema-optional graph database. You can use Neo4j without any schema. Optionally you can introduce it in order to gain performance or modeling benefits. This allows a way of working where the schema does not get in your way until you are at a stage where you want to reap the benefits of having one. 在Neo4j 2.0版本之前，Legacy index被称作indexes。这个索引是在graph外部通过Lucene实现，允许“节点”和“关系”以键值对的形式被检索。从Neo4j 提供的REST接口来看，被称作index的变量通常是指Legacy indexes； Legacy index能够提供全文本检索的能力。这个功能并没有在schema index中被提供，这也是Neo4j 2.0* 版本保留legacy indexes的原因之一。 新建索引create index on :Node(property)，会对指定label property的所有node新建index ，index新建成功后，当graph更新时index会自动更新，index默认存储在根目录的/schema/index/lucene目录；如：1234# 新建索引CREATE INDEX ON :AddressNode( preAddressNodeGUIDs)# 删除索引DROP INDEX ON :AddressNode(_id) 存储方式schema index存储方式为复合索引（Compound Index），除了段信息文件，锁文件，以及删除的文件外，其他的一系列索引文件压缩一个后缀名为cfs的文件，即所有的索引文件会被存储成一个单例的Directory，此方式有助于减少索引文件数量，减少同时打开的文件数量，从而获取更高的效率。比如说，查询频繁，而不经常更新的需求，就很适合这种索引格式。 legacy indexNeo4j Legacy Index配置参数 参数 值 描述 type exact, fulltext exact采用Lucene keyword analyzer是默认配置. fulltext采用white-space tokenizer in its analyzer. to_lower_case true, false type=fulltext时生效，在新建索引和查询时会自动进行字母的大小写转换，默认为小写 analyzer Analyzer类全名 自定义Lucene Analyzer，注意：to_lower_case配置会默认将查询参数转换为小写.如果自定义analyzer索引写入的字母为大写，查询结果将会不匹配 新建索引分exact和fulltext两类，两者可结合使用，可新建relationship索引，默认存储在根目录的index/lucene目录；fulltext索引新建方式参考笔记Neo4j中实现自定义中文全文索引 注意：使用legacy index查询往往需要一个start node； 存储方式legacy index采用非复合索引，更灵活，可以单独的访问某几个索引文件 Neo4j联合索引参考：https://dzone.com/articles/indexing-neo4j-overviewNeo4j不支持联合索引，可采用拼接字段实现 Neo4j 3.0开始支持联合索引，但需要升级至JDK8https://github.com/neo4j/neo4j/issues/6841]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene学习笔记]]></title>
    <url>%2F2016%2F04%2F29%2FLucene%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Neo4j图数据库的索引采用的是Lucene全文索引，特别是LegacyIndex部分，需要深入了解Lucene进行索引定制，之前以IK分词在Solr中建索引和检索浅尝辄止，对Lucene也是停留在概念层。Solr对Lucene商业封装后的易用性很强，提供了比Lucene更为丰富的查询语言，同时实现了可配置、可扩展并对查询性能进行了优化，并且提供了一个完善的功能管理界面。Solr的封装屏蔽了许多技术细节，但是对于开发人员来说，最好还是自下而上循序渐进比较好。lucene（全文检索引擎工具包）&gt;solr（企业级搜索应用服务器）&gt;nutch（分布式检索引擎）和打Boss一样，得一个个来。 Apache Lucene简介The Apache LuceneTM project develops open-source search software, including: Lucene Core, our flagship sub-project, provides Java-based indexing and search technology, as well as spellchecking, hit highlighting and advanced analysis/tokenization capabilities. SolrTM is a high performance search server built using Lucene Core, with XML/HTTP and JSON/Python/Ruby APIs, hit highlighting, faceted search, caching, replication, and a web admin interface. PyLucene is a Python port of the Core project. Lucene Core简介Apache LuceneTM is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform. Lucene Features可扩展、高性能索引 over 150GB/hour on modern hardware small RAM requirements – only 1MB heap incremental indexing as fast as batch indexing index size roughly 20-30% the size of text indexed 强大、精准、高效的检索算法 ranked searching – best results returned first many powerful query types: phrase queries, wildcard queries, proximity queries, range queries and more fielded searching (e.g. title, author, contents) sorting by any field multiple-index searching with merged results allows simultaneous update and searching flexible faceting, highlighting, joins and result grouping fast, memory-efficient and typo-tolerant suggesters pluggable ranking models, including the Vector Space Model and Okapi BM25 configurable storage engine (codecs) 跨平台的解决方案 Available as Open Source software under the Apache License which lets you use Lucene in both commercial and Open Source programs 100%-pure Java Implementations in other programming languages available that are index-compatible Lucene的总体架构http://www.cnblogs.com/forfuture1978/archive/2009/12/14/1623596.html Lucene索引结构Lucene的索引结构是有层次结构的，主要分以下几个层次： 索引(Index)在Lucene中一个索引是放在一个文件夹中的，同一文件夹中的所有的文件构成一个Lucene索引。 段(Segment)一个索引可以包含多个段，段与段之间是独立的，添加新文档可以生成新的段，不同的段可以合并。具有相同前缀文件的属同一个段，segments.gen和segments_5是段的元数据文件，也即它们保存了段的属性信息。 文档(Document)A document is a sequence of fields.文档是我们建索引的基本单位，不同的文档是保存在不同的段中的，一个段可以包含多篇文档。新添加的文档是单独保存在一个新生成的段中，随着段的合并，不同的文档合并到同一个段中。 域(Field)A field is a named sequence of terms.一篇文档包含不同类型的信息，可以分开索引，比如标题，时间，正文，作者等，都可以保存在不同的域里（不同域的索引方式可以不同）。 Field类型 field的text以文本形式存储在index中，field倒排后即为index，也可配置为只存储不建index；Field.Store.* field存储选项通过倒排序索引来控制文本是否可以搜索； field的text看分词为term后建立index，或者field的text直接以原始文本作为term存储为index；大多数field是分词后建立索引的，但有时候指定一些identifier field只存储原始文本是很有用的；Field.Index.* field索引选项确定是否要存储域的真实值；词元(Term)A term is a string.词元是索引的最小单位，是经过词法分析和语言处理后的字符串。在不同field中的相同字符串是不同的term，因此term表示一对字符串，第一个用以命名field，第二个用以命名field中的text；文档是Lucene搜索和索引的原子单位，文档为包含一个或者多个域的容器，而域则是依次包含“真正的”被搜索的内容，域值通过分词技术处理，得到多个词元。 索引可视化工具Luke 倒排/反向索引（Inverted Indexing）定义：存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射为了使得基于term的检索更高效，index中存储了term的统计数据；lucene的索引在索引家族中被称为倒排/反向索引，这是因为它能列出所有包含某个term的document，而这与根据document列出terms的自然联系是倒置的 Lucene索引中的正向信息正向信息按层次保存了从index一直到term的包含关系：索引(Index) –&gt; 段(segment) –&gt; 文档(Document) –&gt; 域(Field) –&gt; 词(Term)也即此索引包含了那些段，每个段包含了那些文档，每个文档包含了那些域，每个域包含了那些词。既然是层次结构，则每个层次都保存了本层次的信息以及下一层次的元信息，也即属性信息。包含正向信息的文件有： segments_N保存了此索引包含多少个段，每个段包含多少篇文档。 XXX.fnm保存了此段包含了多少个域，每个域的名称及索引方式。 XXX.fdx，XXX.fdt保存了此段包含的所有文档，每篇文档包含了多少域，每个域保存了那些信息。 XXX.tvx，XXX.tvd，XXX.tvf保存了此段包含多少文档，每篇文档包含了多少域，每个域包含了多少词，每个词的字符串，位置等信息。 示例：如一本介绍中国地理的书，应该首先介绍中国地理的概况，以及中国包含多少个省，每个省介绍本省的基本概况及包含多少个市，每个市介绍本市的基本概况及包含多少个县，每个县具体介绍每个县的具体情况。 Lucene索引中的反向信息反向信息保存了词典到倒排表的映射：词(Term) –&gt; 文档(Document)包含反向信息的文件有： XXX.tis，XXX.tii保存了词典(Term Dictionary)，也即此段包含的所有的词按字典顺序的排序。 XXX.frq保存了倒排表，也即包含每个词的文档ID列表。 XXX.prx保存了倒排表中每个词在包含此词的文档中的位置。 倒排索引的应用 反向索引数据结构是典型的搜索引擎检索算法重要的部分。 一个搜索引擎执行的目标就是优化查询的速度：找到某个单词在文档中出现的地方。以前，正向索引开发出来用来存储每个文档的单词的列表，接着掉头来开发了一种反向索引。 正向索引的查询往往满足每个文档有序频繁的全文查询和每个单词在校验文档中的验证这样的查询。 实际上，时间、内存、处理器等等资源的限制，技术上正向索引是不能实现的。 为了替代正向索引的每个文档的单词列表，能列出每个查询的单词所有所在文档的列表的反向索引数据结构开发了出来。 随着反向索引的创建，如今的查询能通过立即的单词标示迅速获取结果（经过随机存储）。随机存储也通常被认为快于顺序存储。 Lucene索引文件的基本类型Lucene索引文件中，用一下基本类型来保存信息： Byte：是最基本的类型，长8位(bit)。 UInt32：由4个Byte组成。 UInt64：由8个Byte组成。 VInt：变长的整数类型，它可能包含多个Byte，对于每个Byte的8位，其中后7位表示数值，最高1位表示是否还有另一个Byte，0表示没有，1表示有。越前面的Byte表示数值的低位，越后面的Byte表示数值的高位。例如130化为二进制为 1000, 0010，总共需要8位，一个Byte表示不了，因而需要两个Byte来表示，第一个Byte表示后7位，并且在最高位置1来表示后面还有一个Byte，所以为(1) 0000010，第二个Byte表示第8位，并且最高位置0来表示后面没有其他的Byte了，所以为(0) 0000001。 Chars：是UTF-8编码的一系列Byte。 String：一个字符串首先是一个VInt来表示此字符串包含的字符的个数，接着便是UTF-8编码的字符序列Chars。 Lucene索引存储的基本规则Lucene为了使的信息的存储占用的空间更小，访问速度更快，采取了一些特殊的技巧. 前缀后缀规则(Prefix+Suffix)Lucene在反向索引中，要保存词典(Term Dictionary)的信息，所有的词(Term)在词典中是按照字典顺序进行排列的，然而词典中包含了文档中的几乎所有的词，并且有的词还是非常的长的，这样索引文件会非常的大，所谓前缀后缀规则，即当某个词和前一个词有共同的前缀的时候，后面的词仅仅保存前缀在词中的偏移(offset)，以及除前缀以外的字符串(称为后缀)。 差值规则(Delta)在Lucene的反向索引中，需要保存很多整型数字的信息，比如文档ID号，比如词(Term)在文档中的位置等等。整型数字是以VInt的格式存储的。随着数值的增大，每个数字占用的Byte的个数也逐渐的增多。所谓差值规则(Delta)就是先后保存两个整数的时候，后面的整数仅仅保存和前面整数的差即可。 或然跟随规则(A, B?)Lucene的索引结构中存在这样的情况，某个值A后面可能存在某个值B，也可能不存在，需要一个标志来表示后面是否跟随着B。一般的情况下，在A后面放置一个Byte，为0则后面不存在B，为1则后面存在B，或者0则后面存在B，1则后面不存在B。 跳跃表规则(Skip list)为了提高查找的性能，Lucene在很多地方采取的跳跃表的数据结构。跳跃表(Skip List)是如图的一种数据结构，有以下几个基本特征： 元素是按顺序排列的，在Lucene中，或是按字典顺序排列，或是按从小到大顺序排列。 跳跃是有间隔的(Interval)，也即每次跳跃的元素数，间隔是事先配置好的，如图跳跃表的间隔为3。 跳跃表是由层次的(level)，每一层的每隔指定间隔的元素构成上一层，如图跳跃表共有2层。 TF-IDFTF-IDF（term frequency–inverse document frequency）是一种用于信息检索与数据挖掘的常用加权技术。 Lucene词元权重计算 Term Frequency（tf）：此term在文档中出现的次数，tf越大则该词元越重要。 Document Frequency（df）：有多少文档包含此term，df越大该词元越不重要。计算夹角的余弦值，夹角越小，余弦值越大，分值越大，从而相关性越大。 Lucene检索TokenStreamTokenStream extends AttributeSource implements Closeable:incrementToken,end,reset,close TokenizerTokenizers perform the task of breaking a string into separate tokens.Tokenizer直接继承至TokenStream,其输入input是一个reader TokenFilterToken filters act on each token that is generated by a tokenizer and apply some set of operations that alter or normalize them.TokenFilter也直接继承TokenStream,但input是一个TokenStream。 TokenStreamComponentsTokenStreamComponents其实是将tokenizer和tokenfilter包装起来的(也可以只是tokenizer,两个成员叫source和sink),可以setReader,getTokenStream方法返回sink。 Analyzer如何自定义AnalyzerAnalyzer就是一个TokenStreamComponents的容器，因此需要确定ReuseStrategy,重写createComponents(fieldName,reader)方法,使用时调用tokenStream(fieldName,reader)方法获取TokenStream就可以了。 Lucene常用组件 lucene-core lucene-analyzers-common lucene-analyzers lucene-queryparser lucene-codecs]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j中实现自定义中文全文索引]]></title>
    <url>%2F2016%2F04%2F21%2FNeo4j%E4%B8%AD%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E4%B8%AD%E6%96%87%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[数据库检索效率时，一般首要优化途径是从索引入手，然后根据需求再考虑更复杂的负载均衡、读写分离和分布式水平/垂直分库/表等手段；索引通过信息冗余来提高检索效率，其以空间换时间并会降低数据写入的效率；因此对索引字段的选择非常重要。 Neo4j可对指定Label的Node Create Index，当新增/更新符合条件的Node属性时，Index会自动更新。Neo4j Index默认采用Lucene实现（可定制，如Spatial Index自定义实现的RTree索引），但默认新建的索引只支持精确匹配（get），模糊查询（query）的话需要以全文索引，控制Lucene后台的分词行为。 Neo4j全文索引默认的分词器是针对西方语种的，如默认的exact查询采用的是lucene KeywordAnalyzer（关键词分词器）,fulltext查询采用的是 white-space tokenizer（空格分词器），大小写什么的对中文没啥意义；所以针对中文分词需要挂一个中文分词器，如IK Analyzer,Ansj，至于类似梁厂长家的基于深度学习的分词系统pullword，那就更厉害啦。 本文以常用的IK Analyzer分词器为例，介绍如何在Neo4j中对字段新建全文索引实现模糊查询。 IKAnalyzer分词器IKAnalyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包。IKAnalyzer3.0特性: 采用了特有的“正向迭代最细粒度切分算法“，支持细粒度和最大词长两种切分模式；具有83万字/秒（1600KB/S）的高速处理能力。 采用了多子处理器分析模式，支持：英文字母、数字、中文词汇等分词处理，兼容韩文、日文字符优化的词典存储，更小的内存占用。支持用户词典扩展定义 针对Lucene全文检索优化的查询分析器IKQueryParser(作者吐血推荐)；引入简单搜索表达式，采用歧义分析算法优化查询关键字的搜索排列组合，能极大的提高Lucene检索的命中率。IK Analyser目前还没有maven库，还得自己手动下载install到本地库，下次空了自己在github做一个maven私有库，上传这些maven central库里面没有的工具包。 IKAnalyzer自定义用户词典 词典文件自定义词典后缀名为.dic的词典文件，必须使用无BOM的UTF-8编码保存的文件。 词典配置词典和IKAnalyzer.cfg.xml配置文件的路径问题，IKAnalyzer.cfg.xml必须在src根目录下。词典可以任意放，但是在IKAnalyzer.cfg.xml里要配置对。如下这种配置，ext.dic和stopword.dic应当在同一目录下。1234567891011&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd"&gt;&lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt;&lt;!--用户可以在这里配置自己的扩展字典 --&gt;&lt;entry key="ext_dict"&gt;/ext.dic;&lt;/entry&gt;&lt;!--用户可以在这里配置自己的扩展停止词字典--&gt;&lt;entry key="ext_stopwords"&gt;/stopword.dic&lt;/entry&gt;&lt;/properties&gt; Neo4j全文索引构建指定IKAnalyzer作为luncene分词的analyzer，并对所有Node的指定属性新建全文索引1234567891011121314151617@Overridepublic void createAddressNodeFullTextIndex () &#123; try (Transaction tx = graphDBService.beginTx()) &#123; IndexManager index = graphDBService.index(); Index&lt;Node&gt; addressNodeFullTextIndex = index.forNodes( "addressNodeFullTextIndex", MapUtil.stringMap(IndexManager.PROVIDER, "lucene", "analyzer", IKAnalyzer.class.getName())); ResourceIterator&lt;Node&gt; nodes = graphDBService.findNodes(DynamicLabel.label( "AddressNode")); while (nodes.hasNext()) &#123; Node node = nodes.next(); //对text字段新建全文索引 Object text = node.getProperty( "text", null); addressNodeFullTextIndex.add(node, "text", text); &#125; tx.success(); &#125;&#125; Neo4j全文索引测试对关键词（如’有限公司’），多关键词模糊查询（如’苏州 教育 公司’）默认都能检索，且检索结果按关联度已排好序。123456789101112131415161718192021222324252627282930313233343536373839404142434445package uadb.tr.neodao.test;import org.junit.Test;import org.junit.runner.RunWith;import org.neo4j.graphdb.GraphDatabaseService;import org.neo4j.graphdb.Node;import org.neo4j.graphdb.Transaction;import org.neo4j.graphdb.index.Index;import org.neo4j.graphdb.index.IndexHits;import org.neo4j.graphdb.index.IndexManager;import org.neo4j.helpers.collection.MapUtil;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import org.wltea.analyzer.lucene.IKAnalyzer;import com.lt.uadb.tr.entity.adtree.AddressNode;import com.lt.util.serialize.JsonUtil;/** * AddressNodeNeoDaoTest * * @author geosmart */@RunWith(SpringJUnit4ClassRunner. class)@ContextConfiguration(locations = &#123; "classpath:app.neo4j.cfg.xml" &#125;)public class AddressNodeNeoDaoTest &#123; @Autowired GraphDatabaseService graphDBService; @Test public void test_selectAddressNodeByFullTextIndex() &#123; try (Transaction tx = graphDBService.beginTx()) &#123; IndexManager index = graphDBService.index(); Index&lt;Node&gt; addressNodeFullTextIndex = index.forNodes("addressNodeFullTextIndex" , MapUtil. stringMap(IndexManager.PROVIDER, "lucene", "analyzer" , IKAnalyzer.class.getName())); IndexHits&lt;Node&gt; foundNodes = addressNodeFullTextIndex.query("text" , "苏州 教育 公司" ); for (Node node : foundNodes) &#123; AddressNode entity = JsonUtil.ConvertMap2POJO(node.getAllProperties(), AddressNode. class, false, true); System. out.println(entity.getAll地址实全称()); &#125; tx.success(); &#125; &#125;&#125; CyperQL中使用自定义全文索引查询正则查询1234profile match (a:AddressNode&#123;ruleabbr:'TOW',text:'唯亭镇'&#125;)&lt;-[r:BELONGTO]-(b:AddressNode&#123;ruleabbr:'STR'&#125;)where b.text=~ '金陵.*'return a,b 全文索引查询12345profileSTART b=node:addressNodeFullTextIndex("text:金陵*")match (a:AddressNode&#123;ruleabbr:'TOW',text:'唯亭镇'&#125;)&lt;-[r:BELONGTO]-(b:AddressNode)where b.ruleabbr='STR'return a,b LegacyIndex中建立联合exact和fulltext索引对label为AddressNode的节点，根据节点属性ruleabbr的分类addressnode_fulltext_index（省-&gt;市-&gt;区县-&gt;乡镇街道-&gt;街路巷/物业小区）/addressnode_exact_index(门牌号-&gt;楼幢号-&gt;单元号-&gt;层号-&gt;户室号)，对属性text分别建不同类型的索引1234profileSTART a=node:addressnode_fulltext_index("text:商业街"),b=node:addressnode_exact_index("text:二期19")match (a:AddressNode&#123;ruleabbr:'STR'&#125;)-[r:BELONGTO]-(b:AddressNode&#123;ruleabbr:'TAB'&#125;)return a,b limit 10]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j空间索引学习笔记]]></title>
    <url>%2F2016%2F04%2F03%2FNeo4j%E7%A9%BA%E9%97%B4%E7%B4%A2%E5%BC%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Neo4j采用Neo4j Spatial插件实现空间索引，Neo4j Spatial可使用API或Cypher执行空间查询操作，另作为插件可部署于GeoServer与uDig；与Oracle/MySQL Spatial Extention/MongoDB 2dSphere等空间模块相比，这种结合关系与空间的分析更值得尝试！ 相关资料neo4j spatial github官网lyonwj博客 Neo4j Spatial简介Neo4j Spatial is a library of utilities for Neo4j that faciliates the enabling of spatial operations on data. Utilities for importing from ESRI Shapefile as well as Open Street Map files Support for all the common geometry types An RTree index for fast searches on geometries Support for topology operations during the search (contains, within, intersects, covers, disjoint, etc.) The possibility to enable spatial operations on any graph of data, regardless of the way the spatial data is stored, as long as an adapter is provided to map from the graph to the geometries. Ability to split a single layer or dataset into multiple sub-layers or views with pre-configured filters neo4j spatial 安装 在neo4j spatial github maven库下载最新服务端Neo4j Spatial Server插件，下载后解压到neo4j plugin目录； 验证安装状态：以http://localhost:7474/db/data/ext/SpatialPlugin验证是否成功安装，将返回以下几类graphdb的操作 addSimplePointLayer,addEditableLayer,addCQLDynamicLayer,addGeometryWKTToLayer addNodeToLayer,addNodesToLayer,updateGeometryFromWKT getLayer,findClosestGeometries, findGeometriesWithinDistance,findGeometriesInBBox 索引新建 Create a Spatial index Create nodes with lat/lon data as properties Add these nodes to the Spatial index RTree关系可视化Neo4j Spatial REST服务可参考Neo4j Spatial v0.12-neo4j-2.0.0-SNAPSHOT文档 neo4j spatial应用The technology industry and open source groups are building Spatial tools (“where” analysis) and Graph tools (relationship analysis) so that businesses can improve their insight on patterns, trends, and (perhaps most importantly) outliers in the networks. using-neo4j-spatial-and-leaflet-js-with-mapbox neo4j-spatial-part1-finding-things-close-to-other-thing Mapping the World’s Airports With Neo4j Spatial and Openflights Geospatial Indexing US Congressional Districts with Neo4j-spatial Webinar: Recommend Restaurants Near Me: Introduction to Neo4j Spatial Finding Valuable Outliers and Opportunities Using Graph and Spatial legis-graph-spatial Java构建Neo4j 空间索引参考distance-queries-with-neo4j-spatialgist代码示例：Neo4j Emberded 嵌入式SpringBean配置gist代码示例：Java实现Neo4j Spatial新建索引和空间查询测试用例 关于withinDistance查询结果排序问题 球面距离计算采用OrthodromicDistance算法OrthodromicDistance算法：d = acos( sin(lat1)*sin(lat2) + cos(lat1)*cos(lat2)*cos(lon2-lon1) ) * R，Neo4j-Spatial中的实现：org.neo4j.gis.spatial.pipes.processing.OrthodromicDistance 返回结果默认以命中目标坐标与查询中心点坐标的距离进行排序参考Neo4j Spatial 源码测试用例中的：TestSimplePointLayer中的checkPointOrder，查询示例：List&lt;GeoPipeFlow&gt; res = GeoPipeline. startNearestNeighborLatLonSearch( layer, start, distance).sort(&quot;OrthodromicDistance&quot;).toList(); neo4j spatial query 示例withinDistance缓存区查询查询点120.678966,31.300864周边0.1km范围内的Node格式：START n = node:&lt;layer&gt;(&quot;withinDistance:[&lt;y&gt;, &lt;x&gt;, &lt;max distance in km&gt;]&quot;) 1start n = node:geom('withinDistance:[31.331937,120.638154,0.1]') return n limit 10 bbox矩形查询查询由点1(120.678966,31.300864)与点2(120.978966,31.330864)构成的BBox矩形范围内的Node格式：START n = node:&lt;layer&gt;(&quot;bbox:[&lt;min x&gt;, &lt;max x&gt;, &lt;min y&gt;, &lt;max y&gt;]&quot;) 1start n = node:geom('bbox:[120.678966,120.978966,31.300864,31.330864]') return n limit 10 withinWKTGeometry查询查询由点1(120.678966,31.300864)与点2(120.978966,31.330864)构成的Polygon多边形范围内的Node格式：START n = node:&lt;layer&gt;(&quot;withinWKTGeometry:POLYGON((&lt;x1&gt; &lt;y1&gt;, ..., &lt;xN&gt; &lt;yN&gt;, &lt;x1&gt; &lt;y1&gt;))&quot;) 1start n = node:geoindex('withinWKTGeometry:POLYGON ((120.678966 31.300864, 120.678966 31.330864, 120.978966 31.330864, 120.978966 31.300864, 120.678966 31.300864))') return n limit 10 空间索引和关系遍历联合查询联合geom索引图层和match进行查询 查询指定范围&amp;&amp;指定path路径中的节点 1234start n = node:geom('withinDistance:[31.331937,120.638154,0.1]')match path=(:DIS&#123;text:'工业园区'&#125;)-[:BELONGTO ]-(:POI&#123;text:'拙政别墅'&#125;)where n in nodes(path)return n,path 优化后 123profile start n = node:geom('withinDistance:[31.331937,120.638154,0.1]')match path=(:DIS&#123;text:'工业园区'&#125;)&lt;-[:BELONGTO ]-(n)return path 查询结果可视化效果图 联合查询：withinWKTGeometry空间过滤与match属性过滤 12345profile start n = node:geoindex('withinWKTGeometry:POLYGON ((120.678966 31.300864, 120.678966 31.330864, 120.978966 31.330864, 120.978966 31.300864, 120.678966 31.300864))')match (n)where (n.ruleabbr in ['POI','STR']) and n.spapriority=1and ANY(adtext IN n.adtext WHERE adtext =~ '.*公司.*' )return n limit 10 CypherQL必须先执行空间索引，再执行Relation过滤，这样每个空间围内的Node都要进行Relationship过滤，效率较低； 若能先执行Match再执行空间过滤，可提高SpatialIndex命中率 若无分页需求，可临时采用NativeAPI进行Match过滤，再以SpatialIndex withinDiatance过滤。 若需要分页的话skip limit必须在CypherQL中实现，但是空间索引与关系遍历并行的CQL怎么写？暂时无解！ 问题建空间索引内存溢出问题neo4j transaction优化方案：每n条手动提交事物 1234567891011121314151617181920212223242526272829303132 // 获取所有地址节类型，针对不同地址节分别构建R树索引public void createAddressNodeIndex_spatial(Set&lt; String&gt; addressNodes) &#123;truefinal long commitInterval = 10000;trueTransaction tx = graphDBService.beginTx();truetry &#123;truetruelong i = 0L;truetruelong startTime = System.currentTimeMillis();truetruefor (String addressNodeLabel : addressNodes) &#123;truetruetrueIndex&lt; Node&gt; index = getSpatialIndex(UADBLabel.valueOf(addressNodeLabel));truetruetrueResourceIterator&lt; Node&gt; nodes = graphDBService.findNodes(createAddresseNodeLable(addressNodeLabel));truetruetruewhile (nodes.hasNext()) &#123;truetruetruetrueNode node = nodes.next();truetruetruetrueif (node.getProperty("lon", null) != null &amp;&amp; node.getProperty("lat", null) != null) &#123;truetruetruetruetrueindex.add(node, "", "");truetruetruetruetruei++;truetruetruetrue&#125;truetruetruetrue// 处理内存溢出truetruetruetrueif (i % commitInterval == 0) &#123;truetruetruetruetruetx.success();truetruetruetruetruetx.close();truetruetruetruetruelog.info("indexing (&#123;&#125; nodes added) ... time in seconds:&#123;&#125;", i,truetruetruetruetruetruetrueDateUtil.convertMillis2DateStr(System.currentTimeMillis() - startTime));truetruetruetruetruetx = graphDBService.beginTx();truetruetruetrue&#125;truetruetrue&#125;truetrue&#125;truetruetx.success();true&#125; finally &#123;truetruetx.close();true&#125;&#125; 建空间索引速度还是偏慢，35万左右的数据量建索引花了将近1.5小时。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j常用CypherQL语句]]></title>
    <url>%2F2016%2F03%2F30%2FNeo4j%E5%B8%B8%E7%94%A8CypherQL%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[记录常用Cypher语句 参考文档cypher-query-langcypher-refcard create Node12345CREATE (root:User &#123; type:'admin', name: 'root'&#125;)CREATE (u1:User &#123; type:'guest', name : 'user1'&#125;)CREATE (u2:User &#123; type:'guest', name: 'user2'&#125;)CREATE (u3:User &#123; type:'guest', name: 'user3'&#125;)CREATE (u4:User &#123; type:'guest', name: 'user4'&#125;) Create RelationShip123MATCH (root&#123;type:'admin' &#125;),(guest&#123;type:'guest'&#125;)CREATE (root)-[r:knows]-&gt;(guest)RETURN r Create Unique RelationShip123MATCH (root&#123;type:'admin' &#125;)CREATE UNIQUE (root)-[r:knows]-(u5:User&#123;name:'user5'&#125;)RETURN u5 match Node match by property 12MATCH (root &#123; name : 'root' &#125;)return root match by ID identifier 123MATCH (s)WHERE ID(s) = 65110RETURN s complex query 12345678MATCH (d:District &#123;state: &#123;state&#125;, district: &#123;district&#125;&#125;)MATCH (d)&lt;-[:REPRESENTS]-(l:Legislator)MATCH (l)-[:SERVES_ON]-&gt;(c:Committee)MATCH (c)&lt;-[:REFERRED_TO]-(b:Bill)MATCH (b)-[:DEALS_WITH]-&gt;(s:Subject)WITH l.govtrackID AS govtrackID, l.lastName AS lastName, l.firstName AS firstName, l.currentParty AS party, s.title AS subject, count(*) AS strength, collect(DISTINCT c.name) AS committees ORDER BY strength DESC LIMIT 10WITH &#123;lastName: lastName, firstName: firstName, govtrackID: govtrackID, party: party, committees: committees&#125; AS legislator, collect(&#123;subject: subject, strength: strength&#125;) AS subjectsRETURN &#123;legislator: legislator, subjects: subjects&#125; AS r match relationNode12MATCH (root&#123; type:'admin' &#125;)--&gt;(user)RETURN user match Node and relationNode12MATCH (root &#123; type:'admin' &#125;)-[r]-(user)RETURN r match collectioncollection contain string123match (an)where all (x IN ['709908DCF9D24734BA8FEF8A831F1BA4'] where x in an.preAddressNodeGUIDs)return count(an) collection equal12match (an&#123;preAddressNodeGUIDs:['709908DCF9D24734BA8FEF8A831F1BA4 ']&#125;)return count(an) delete relationshipdelete a node with its relationships1MATCH (n &#123; name:'Andres' &#125;)DETACH DELETE n delete all relationships12Match (:AddressNode)-[r:parent]-&gt;(:AddressNode)delete r startThe START clause should only be used when accessing legacy indexes Legacy Indexing.In all other cases, use MATCH instead (see Section 11.1, “Match”).In Cypher, every query describes a pattern, and in that pattern one can have multiple starting points.A starting point is a relationship or a node where a pattern is anchored. Using START you can only introduce starting points by legacy index seeks.Note that trying to use a legacy index that doesn’t exist will generate an error. indexcreate indexCREATE INDEX ON :PRO( preAddressNodeGUIDs) drop indexDROP INDEX ON :PRO( preAddressNodeGUIDs) Neo4j联合索引Neo4j2.3.x不支持联合索引，可采用拼接字段实现，参考indexing-neo4j-overview；Neo4j 3.0开始支持联合索引，但需要升级至JDK8，参考github neo4j Issue123456profileMATCH (p:AddressNode &#123;text:"拙政别墅"&#125;)WITH pMATCH (o:AddressNode&#123; ruleabbr:"POI"&#125;)WHERE id(p) = id(o)RETURN p 123profileMATCH (p:AddressNode &#123;ruleabbr:"POI",text:"拙政别墅"&#125;)RETURN p 暂测试，疑neo4j由于采用lucene全文索引的缘故，在2个字段各有索引，但无联合索引的情况下，索引倒排会提高检索命中率。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB与Neo4j数据同步]]></title>
    <url>%2F2016%2F03%2F23%2FMongoDB%E4%B8%8ENeo4j%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[采用mongo-connector及Neo4j Doc Manager将MongoDB中数据导入Neo4j（嵌套结构形成关系） 参考文档neo4j_doc_manager项目地址 MongoDB启用副本Windows安装MongoDB服务 bat脚本1234@echo offtitle 卸载MongoDBsc delete MongoDBcmd /k Windows卸载MongoDB服务 bat脚本1234@echo offtitle 安装MongoDBD:\mongodb\bin\mongod --logpath "D:\mongodb\log\mongo.log" --logappend --dbpath "D:\mongodb\data" --directoryperdb --replSet myDevReplSet --serviceName "MongoDB" --serviceDisplayName "MongoDB" --installcmd /k 初始化MongoDB Replica set进入mongo shell执行rs.initiate() 安装Neo4j Doc Manager123456# 新增python环境neo4j.venvvirtualenv --no-site-packages neo4j.venv# 进入neo4j.venvworkon neo4j.venv# 安装neo4j-doc-manager --prepip install -i http://pypi.douban.com/simple neo4j-doc-manager --trusted-host pypi.douban.com 启动mongo-connector进入Python环境：workon neo4j.venv运行neo4j_doc_manager：mongo-connector -m 192.168.1.188:27017 -t http://127.0.0.1:7474/db/data -d neo4j_doc_manager同步指定Databse.Collection：mongo-connector -m 127.0.0.1:27017 -n uadb_suzhou_gyyq.AddressNode -t http://127.0.0.1:7474/db/data -d neo4j_doc_managerneo4j_doc_manager运行后，当MongoDB插入数据时，mongodb Document将会实时转换为图结构存储到Neo4j，文档Key会转换为Node,值对象作为Node的属性值。 问题记录No handlers could be found for logger “mongo_connector.util”实际错误：py2neo.database.status.Unauthorized: http://127.0.0.1:7474/db/manage/server/jmx/domain/org.neo4j解决方案： 停用 authorization考虑到性能和测试便捷可停用Neo4j安全授权机制。在neo4j-server.properties中设置dbms.security.auth_enabled=false 设置NEO4J_AUTH环境变量若生产环境已启用授权，设置NEO4J_AUTH环境变量export NEO4J_AUTH=user:password AttributeError: ‘Graph’ object has no attribute ‘cypher’解决方案：neo4j_doc_manager github issue参考官网文档，安装时附加–pre参数，然而运行dev版有问题，老实安装stable版本即可 OplogThread: Last entry no longer in oplog cannot recove修改mongo-connector配置参数后报错解决：删除日志文件（mongo-connector.log）所在根目录的oplog.timestamp文件，上次异常终止mongo-connector写入了xxx，导致无法正常运行 如何提高同步速度how-do-i-increase-the-speed-of-mongo-connectormongo-connector Configuration-Options]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL常用函数（UDF）]]></title>
    <url>%2F2016%2F03%2F18%2FMySQL%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%EF%BC%88UDF%EF%BC%89%2F</url>
    <content type="text"><![CDATA[数据分析/特定业务逻辑MySQL内置的Function无法满足需求，只能祭出UDF。 字符串分割(split_string) 函数定义 CREATE DEFINER = ‘ugcdb’@’%’FUNCTION ugcdb.split_string(x VARCHAR(255),delim VARCHAR(12),pos INT)RETURNS varchar(255) CHARSET utf8RETURN REPLACE(SUBSTRING(SUBSTRING_INDEX(x, delim, pos), LENGTH(SUBSTRING_INDEX(x, delim, pos -1)) + 1), delim, ‘’) 函数调用 update tableName set 门牌号= SPLIT_STR(门牌号,&#39;号&#39;,1) ; 获取行号(get_rownum)参考create-a-view-with-column-num-rows-mysql 函数定义 CREATE DEFINER=geocodingdb@% FUNCTION geocodingdb.get_rownum() RETURNS int(11)BEGIN SET @temp_rowNumber := IFNULL(@temp_rowNumber,0)+1; return @temp_rowNumber;END 函数调用 SET @temp_rowNumber=0;select fieldA , get_rownum() AS rownum from tableName;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM学习笔记（三）垃圾收集器与内存分配策略]]></title>
    <url>%2F2016%2F03%2F09%2FJVM%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[Java与C++之间有一堵由内存动态分配和垃圾收集技术所围成的“高墙”，墙外面的人想进去，墙里面的人想出来。 概述垃圾收集（Garbage Collection，GC）需要完成的三件事情： 哪些内存需要回收？-[] 内存区域-回收条件 什么时候回收？-[] 多线程/安全点 如何回收？-[] 回收算法 当要排查各种内存溢出、内存泄漏问题时，当垃圾收集称为系统达到更高并发量的瓶颈时，我们就需要对这些”自动化”的技术实施必要的监控和调节。 程序计数器、虚拟机栈、本地方法栈3个区域随线程而生，随线程而灭；每一个栈帧中分配多少内存基本上在类结构确定下来的时候就已知。因此这几个区域的内存分配和回收都具有确定性，不需过多考虑回收问题，方法结束或者线程结束时，内存自然就随之回收了。 Java堆和方法区则不一样，一个接口中的多个实现类需要的内存可能不一样，一个方法中的多个分支需要的内存也可能不一样，只有在程序处于运行期间才知道会创建哪些对象，这部分内存的分配和回收都是动态的，垃圾收集器所关注的是这部分内存！ 对象已死吗？垃圾回收器在对堆进行回收前，首要确定的事情就是这些对象之间哪些还存活着，哪些已经死去？ 引用计数算法 定义：引用计数算法（Reference Counting）:给对象添加一个引用计数器，每当一个地方引用它时，计数器值就+1；当引用失效时，计数器值就-1；任何时刻计数器为0的对象就是不可能被再使用的； 优点：实现简单，判定效率高；微软的COM技术、Python中都使用了Reference Couting算法进行内存管理； 缺点：由于其很难解决对象之间相互循环引用的问题，主流Java虚拟机里面都没有选用Refrence Couting算法来管理内存； 可达性分析算法 定义：可达性分析（Reachability Analysis）判断对象存活的基本思路：通过一系列的称为GC Roots的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain）,当一个对象到GC Roots没有任何引用链相连（即GC Roots到这个对象不可达）时，则证明此对象是不可用的； Java语言中，可作为GC Roots对象包括： 虚拟机栈（栈帧中的本地变量表）中引用的对象； 方法区中类静态属性引用的对象； 方法区中产量引用的对象； 本地方法栈中JNI（即一般的Native方法）引用的对象 再谈引用JDk1.2之后，Java对引用概念进行了扩充，将引用分为强引用、软引用、弱引用、虚引用4种，4种强度一次逐渐减弱。 强引用（Strong Reference）是指在程序代码之中普遍存在的，类似Object obj=new Object()这类的引用，只要强引用存在，对象就不会发生GC； 软引用（Soft Reference）是用来描述一些还有用但并非必须的对象。对于软引用关联着的对象，在系统将要发生OOM异常之前，将会把这些对象列进回收范围之中进行第二次回收，如果这次回收后还没有足够的内存，才会抛出OOM异常。 弱引用（Weak Reference）是用来描述非必须对象的，强度比软引用更弱，被弱引用关联的对象只能生存到下一次GC发生之前。当垃圾回收器工作时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。 虚引用（Phantom Reference）也称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。 回收方法区 在方法区中进行垃圾收集的性价比一般比较低；而在Heap中，尤其是在新生代，常规应用进行一次垃圾收集一般回收70%~95%的空间，而永久代的垃圾收集效率远低于此； 永久代的垃圾收集主要回收两部分内容：废弃常量和无用的类； 回收废弃常量与回收Java堆中的对象类似； 判定一个类是否是无用的类条件相对苛刻： 该类所有实例都已被回收，即Java堆中不存在该类的任何实例； 加载该类的ClassLoader已经被回收； 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该方法。 在大量使用反射、动态代理、CGLib等ByteCode框架、动态生成JSP以及OSGi这类自定义ClassLoader的场景都需要虚拟机具备类卸载的功能，以保证永久代不会溢出。 垃圾收集算法只介绍内存回收的方法论（算法思想及发展过程），不讨论具体算法实现。 标记-清除算法（Mak-Sweep） 定义：MS算法分标记和清除两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。 两点不足： 效率问题，标记和清除两个过程的效率都不高； 空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多后导致以后程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前出发一次垃圾收集动作； 复制算法（Coping） 定义：Coping算法将可用内存按容量划分为大小相等的两块，每次使用其中一块。当这一块的内存用完了，就将还存活的对象复制到另一块上面，然后再把已使用的内存清理掉。 优点：每次对整个半区进行回收，内存分配时不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效； 不足：提高效率的代价是将内存缩小到原来的一半；现代商业虚拟机都采用这种收集算法来回收新生代，但新生代中的对象一般98%是朝生夕死，无需按照1:1比例来划分内存空间，而是将内存分为1块较大的Eden（伊甸园）空间和2块较小的Survivor（幸存者）空间，每次使用Eden和其中1块Survivor。 回收时，将Eden和Survivor中还存活的对象一次性复制到另外一个Survivor空间中，最后清理掉Eden和刚才用过的Survivor空间。 HotSpot VM默认Eden和Survivor的比例是8:1:1，即只浪费10%的内存。 98%的对象可回收只是一般场景下的数据，无法保证每次回收都只有不多于10%的对象存活，所以当Survivor空间不足时，需要依赖其他内存（老年代）进行分配担保（Handle Promotion），让对象进入老年代。 标记-整理算法（Mark-Compact） 出场背景：复制算法在对象存活率较高时复制操作较多，效率会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以对应被使用内存中的所有对象都100%存活的极端情况，所以老年代一般不直接选用这种算法。 定义：根据老年代的特点，提出标记-整理（Mark-Compact）算法，标记过程仍然与标记-清除算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理调用端边界以外的内存。 内存碎片整理 标记-清除算法 vs 标记-整理算法 分代收集算法当前商业虚拟机的垃圾收集都采用分代收集(Generational Collection)算法，根据对象存活周期的不同将内存分为几块。 一般把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法； 新生代每次垃圾回收时都发现有大批对象死去,只有少量对象存活，故采用复制算法，以少量对象复制的成本即可完成收集； 老年代中因为对象存活率高、没有额外空间对其进行分配担保，必须采用标记-清理或标记-整理算法来进行回收。 HotSpot的算法实现HotSpot虚拟机上实现对象存活判断算法和垃圾收集算法时，必须对算法的执行效率有严格的考量，才能保证虚拟机高效运行。 枚举根基点（GC Roots） 可作为GC Roots的节点主要在全局性的引用（如常量或静态变量）与执行上下文（如栈帧中的本地变量表）中； 可达性分析对执行时间的敏感体还现在GC停顿上，因为这项分析工作必须在一个能确保一致性的快照中进行（即对象引用关系在某个时间点冻结），这点是导致GC必须停顿所有Java执行线程（Stop The World）的一个重要原因，即使号称不会发生停顿的CMS收集器中，GC Roots也是必须要停顿的。 准确式内存管理（Exact Memory Management）,即虚拟机可以知道内存中某个位置的数据具体是什么类型。 HotSpot VM采用OopMap(oop,Ordinary Object Pointer,普通对象指针)数据结构，在类加载完成的时候，将对象内什么偏移量上是什么类型的数据计算出来，在JIT(Just-In-Time Compiler)编译过程中，也会在特定的位置（Safepoint）记录下栈和寄存器中哪些位置是引用。 安全点（Safepoint） 关于OopMap 在OopMap的协助下，HotSpot可以快速且准确地完成GC Roots枚举，但是可能会导致引用关系变化； OopMap内容变化的指令非常多，如果为每一条指令都生成对应的OopMap，将会需要大量的额外空间，这样GC的空间成本将会变得很高。 安全点定义HotSpot没有为每条指令都生成OopMap，只是在特定的位置记录了这些信息，这些位置称为安全点(Sapfepoint)，即程序执行时并非在所有地方都能停顿下来开始GC，只有在到达安全点时才能暂停。 安全点的选定 安全点的选定既不能太少以至于让GC等待时间太长，也不能过于频繁以至于过分增大运行时的负荷。 安全点的选定基本上是以程序是否具有让程序长时间执行的特性为标准选定的，长时间执行的最明显特性就是指令序列复用，如方法调用、循环跳转、异常跳转等，所以具有这些功能的指令才会产生Safepoint。 如何在GC发生时让所有线程（这里不包括执行JNI调用的线程）都跑到最近的安全点上再停顿下来，有两张方案可供选择： 抢先式中断（Preemptive Suspension）：不需要线程的执行代码主动去配合，在GC发生时，首先把所有线程全部中断，如果发现有线程中断的地方不在安全点上，就恢复线程，让它跑到安全点。现在几乎没有虚拟机实现采用抢先式中断来暂停线程从而响应GC事件。 主动式中断（Voluntary Suspension）：当GC需要中断线程的时候，不直接对线程进行操作，仅仅简单的设置一个标志，各个线程执行时主动去轮询这个标志，发现中断标志为真时就自己中断挂起（VM将内存页设置为不可读，线程会产生自陷异常，在预先注册异常处理器中暂停线程实现等待），轮询标志的地方和安全点是重合的。 安全区域（Safe Region） 安全区域产生背景Safepoint机制保证了程序执行时，在不太长时间内就会遇到可进入GC的Safepoint；但是当程序不执行（没有CPU分配时间）的时候（如线程出于Sleep状态或者Block状态），这时线程无法响应JVM的中断请求，走到安全的地方中断挂起，JVM也不可能等待线程重新分配CPU时间。对于这种情况，就需要安全区域（Safe Region）来解决。 安全区域定义安全区域是指在一段代码片中，引用关系不会发生变化。在这个区域中的任意地方开始GC都是安全的。可以将Safe Region看作被扩展了的Safepoint。 工作原理执行函数在进入安全区域时设置ready flag。在它离开安全区域以前，它先检查GC是否完成了枚举（或者收集），并且不再需要执行函数呆在阻塞状态。如果是真，它就向前执行，离开安全区域； 否则，它就像安全点一样阻塞他自己。 垃圾收集器介绍垃圾收集器之前，需要明确一点，就是在新生代采用的停止复制算法中，“停 止（Stop-the-world）”的意义是在回收内存时，需要暂停其他所 有线程的执行。这个是很低效的，现在的各种新生代收集器越来越优化这一点，但仍然只是将停止的时间变短，并未彻底取消停止。 Serial收集器 新生代收集器，使用停止复制算法，使用一个线程进行GC，串行，其它工作线程暂停。 使用-XX:+UseSerialGC可以使用Serial+Serial Old模式运行进行内存回收（这也是虚拟机在Client模式下运行的默认值） ParNew收集器（Parallel New） 新生代收集器，使用停止复制算法，Serial收集器的多线程版，用多个线程进行GC，并行，其它工作线程暂停，关注缩短垃圾收集时间。 使用-XX:+UseParNewGC开关来控制使用ParNew+Serial Old收集器组合收集内存；使用-XX:ParallelGCThreads来设置执行内存回收的线程数。 Parallel Scavenge收集器 新生代收集器，使用停止复制算法，关注CPU吞吐量，即运行用户代码的时间/总时间，比如：JVM运行100分钟，其中运行用户代码99分钟，垃圾收集1分钟，则吞吐量是99%， 这种收集器能最高效率的利用CPU，适合运行后台运算（关注缩短垃圾收集时间的收集器，如CMS，等待时间很少，所以适 合用户交互，提高用户体验）。 使用-XX:+UseParallelGC开关控制使用Parallel Scavenge+Serial Old收集器组合回收垃圾（这也是在Server模式下的默认值）； 使用-XX:GCTimeRatio来设置用户执行时间占总时间的比例，默认99，即1%的时间用来进行垃圾回收。 使用-XX:MaxGCPauseMillis设置GC的最大停顿时间（这个参数只对Parallel Scavenge有效）， 用开关参数-XX:+UseAdaptiveSizePolicy可以进行动态控制，如自动调整Eden/Survivor比例，老年代对象年龄，新生代大小等，这个参数在ParNew下没有。 Serial Old收集器 老年代收集器，单线程收集器，串行， 使用标记整理（整理的方法是Sweep（清理）和Compact（压缩），清理是将废弃的对象干掉，只留幸存的对象，压缩是将移动对象，将空间填满保证内存分为2块，一块全是对象，一块空闲）算法， 使用单线程进行GC，其它工作线程暂停（注意，在老年代中进行标记整理算法清理，也需要暂停其它线程）， 在JDK1.5之前，Serial Old收集器与ParallelScavenge搭配使用。 Parallel Old收集器 老年代收集器，多线程，并行，多线程机制与Parallel Scavenge差不错， 使用标记整理（与Serial Old不同，这里的整理是Summary（汇总）和Compact（压缩），汇总的意思就是将幸存的对象复制到预先准备好的区域，而不是像Sweep（清理）那样清理废弃的对象）算法， 在Parallel Old执行时，仍然需要暂停其它线程。 Parallel Old在多核计算中很有用。 Parallel Old出现后（JDK 1.6），与Parallel Scavenge配合有很好的效果，充分体现Parallel Scavenge收集器吞吐量优先的效果。 使用-XX:+UseParallelOldGC开关控制使用Parallel Scavenge +Parallel Old组合收集器进行收集。 CMS收集器（Concurrent Mark Sweep） 老年代收集器，致力于获取最短回收停顿时间（即缩短垃圾回收的时间），使用标记清除算法，多线程，优点是并发收集（用户线程可以和GC线程同时工作），停顿小。 使用-XX:+UseConcMarkSweepGC进行ParNew+CMS+Serial Old进行内存回收， 优先使用ParNew+CMS，当用户线程内存不足时，采用备用方案Serial Old收集。 CMS收集的执行过程是：初始标记(CMS-initial-mark) -&gt; 并发标记(CMS-concurrent-mark) –&gt;预清理(CMS-concurrent-preclean)–&gt;可控预清理(CMS-concurrent-abortable-preclean)-&gt; 重新标记(CMS-remark) -&gt; 并发清除(CMS-concurrent-sweep) -&gt;并发重设状态等待下次CMS的触发(CMS-concurrent-reset) 在CMS清理过程中，只有初始标记和重新标记需要短暂停顿，并发标记和并发清除都不需要暂停用户线程，因此效率很高，很适合高交互的场合。 CMS也有缺点，它需要消耗额外的CPU和内存资源，在CPU和内存资源紧张，CPU较少时，会加重系统负担（CMS默认启动线程数为(CPU数量+3)/4）。 在并发收集过程中，用户线程仍然在运行，仍然产生内存垃圾，所以可能产生“浮动垃圾”，本次无法清理，只能下一次Full GC才清理，因此在GC期间，需要预留足够的内存给用户线程使用。 使用CMS的收集器并不是老年代满了才触发Full GC，而是在使用了一大半（默认68%，即2/3，使用-XX:CMSInitiatingOccupancyFraction来设置）的时候就要进行Full GC，如果用户线程消耗内存不是特别大，可以适当调高-XX:CMSInitiatingOccupancyFraction以降低GC次数，提高性能，如果预留的用户线程内存不够，则会触发Concurrent Mode Failure，此时，将触发备用方案：使用Serial Old 收集器进行收集，但这样停顿时间就长了，因此-XX:CMSInitiatingOccupancyFraction不宜设的过大。 CMS采用的是标记清除算法，会导致内存碎片的产生，可以使用-XX：+UseCMSCompactAtFullCollection来设置是否在Full GC之后进行碎片整理，用-XX：CMSFullGCsBeforeCompaction来设置在执行多少次不压缩的Full GC之后，来一次带压缩的Full GC。 G1收集器（Garbage-First）面向服务器端应用的垃圾收集器，计划未来替代CMS收集器。 理解GC日志垃圾收集器参数总结内存分配与回收策略对象优先在Eden分配大对象直接进入老年代长期存活的对象将进入老年代动态对象年龄判定空间分配担保]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL存储过程学习笔记]]></title>
    <url>%2F2016%2F03%2F08%2FMySQL%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[团队一直有小伙伴顶着数据库相关的工作，ETL数据整合分析事情多了，研发开发工作相应减少，终于有机会也来写写存储过程了。以SP进行SQL业务逻辑封装，执行性能能大大提高，在注意合理拆分SP、SQL书写简洁规范和注释到位的情况下，也能做到易于维护。特别是对于海量数据分析追求时效性的业务，效率第一，就算逻辑复杂不易维护也得认。如Hibernate VS Mybatis，产品 VS 项目，现实世界丰富多彩，存在即合理。 MySQL从V5.0开始支持存储过程，V5.0~V5.7版本之间自带的function有所差别，查资料的时候注意过滤，本文以MySQL5.5.4作为测试环境。 关于MySQL存储过程MySQL 存储过程(Stored Procedure) 是通过给定的语法格式编写自定义的数据库API, 包含一系列sql语句的集合, 完成一个复杂的功能. MySQL5.5官方文档 存储过程 调试工具：dbForge Studio for MySQL Professional Edition 变量定义mysql存储过程中，定义变量有两种方式： 会话变量使用set或select直接赋值，变量名以 @ 开头，可以在一个会话的任何地方声明，作用域是整个会话，称为会话变量。如:set @var=1; 存储过程变量以 DECLARE 关键字声明的变量，只能在存储过程中使用，称为存储过程变量，主要用在存储过程中，或者是给存储传参数中。如：DECLARE var1 INT DEFAULT 0; 两者的区别 在调用存储过程时，以DECLARE声明的变量都会被初始化为 NULL。 会话变量（即@开头的变量）则不会被再初始化，在一个会话内，只须初始化一次，之后在会话内都是对上一次计算的结果，就相当于在是这个会话内的全局变量。 在存储过程中，使用动态语句，预处理时，动态内容必须赋给一个会话变量。 注意事项 变量命名需与表字段不一致； 输出日志信息SELECT concat(‘Comment:’,’—Comment—‘); 临时表参考-MySQL临时表的简单用法参考-internal-temporary-tables临时表将在你连接MySQL期间存在。当连接断开时，MySQL将自动删除表并释放所用的空间。当然也可以在仍然连接的时候删除表并释放空间。1234-- 新建CREATE TEMPORARY TABLE IF NOT EXISTS sp_output_tmp ENGINE = MEMORY SELECT... from... where... ;-- 删除DROP TEMPORARY TABLE IF EXISTS sp_output_tmp; 游标嵌套循环（nested cursor loop）参考/multiple-cursors-in-nested-loops-in-mysql 1234567891011121314151617181920212223242526272829303132333435363738394041424344BEGIN DECLARE done1 int default false; -- 批量更新计数器 DECLARE cachSize_matchedAddress int DEFAULT 0; -- 定义Cursor1 DECLARE cursor_bizId CURSOR FOR select * from table1; DECLARE CONTINUE HANDLER FOR NOT FOUND SET done1 = TRUE; -- 打开Cursor1 OPEN cursor_bizId; -- 循环获取业务ID loop_getBizId:LOOP FETCH cursor_bizId INTO bizId; IF done1 THEN LEAVE loop_getBizId; # 关闭Cursor1 CLOSE cursor_bizId; END IF; -- TODO Cursor1相关业务逻辑 -- 根据业务ID获取业务数据 block_matchedAddress:BEGIN DECLARE done2 int default false; -- 定义Cursor2 DECLARE cursor_matchedAddress CURSOR FOR SELECT * FROM temp_matchedAddress; DECLARE continue handler for not found set done2 = true; OPEN cursor_matchedAddress; -- 循环获取业务数据 loop_getMatchedAddress:LOOP FETCH cursor_matchedAddress INTO _rownum, _matchID, _matchedType, _matchedAddress, _x, _y; IF done2 THEN LEAVE loop_getMatchedAddress; -- 关闭Cursor2 CLOSE cursor_matchedAddress; END IF; -- TODO Cursor2相关业务逻辑 END LOOP loop_getMatchedAddress; END block_matchedAddress; END LOOP loop_getBizId;END 动态sqlPREPARE命令：PREPARE stmt_name FROM preparable_stmtThe PREPARE statement prepares a SQL statement and assigns it a name, stmt_name, by which to refer to the statement later. The prepared statement is executed with EXECUTE and released with DEALLOCATE PREPARE. For examples, see Section 13.5, “SQL Syntax for Prepared Statements”. 1234SET @sql = "select * from table";PREPARE stmt from @sql ;EXECUTE stmt;DEALLOCATE PREPARE stmt; 注意：The text must represent a single statement, not multiple statements. 批量更新在sp中类似insert into (column1) values (value1)这样循环单条insert执行速度太慢，可采用temporary table将数据先插入临时表；设置一个计数器，当临时表达到limit记录数时，关联更新目标表并重置临时表以释放资源。 问题记录Json序列化函数JSON_Array不存在问题问题描述：JSON_Array does not existed解决：我用的V5.5，不支持,V5.7.8才支持; As of MySQL 5.7.8, MySQL supports a native JSON data type that enables efficient access to data in JSON (JavaScript Object Notation) documents. The JSON data type provides these advantages over storing JSON-format strings in a string column: 嵌套Cusor问题问题描述：子Cursor fetch的values总是父Cursor的第一个值，见gist存储过程记录解决：View的问题，换成temporary table解决，见gist-batchUpdateMatchedAddress.sql 游标动态sql问题问题描述：动态设置的条件与静态条件相比，少返回1条记录原因：游标中查询条件不支持动态条件。参考sql-syntax-prepared-statements SQL syntax for prepared statements can be used within stored procedures, but not in stored functions or triggers. However, a cursor cannot be used for a dynamic statement that is prepared and executed with PREPARE and EXECUTE. The statement for a cursor is checked at cursor creation time, so the statement cannot be dynamic. 解决：参考dynamic-cursor-in-stored-procedure A cursor will only accept a select statement, so if the SQL really needs to be dynamic make the declare cursor part of the statement you are executing. 以View视图或Temporary Table临时表形式间接实现 新建视图时select中不能带有动态参数原因： Within a stored program, the SELECT statement cannot refer to program parameters or local variables.解决：View嵌套子查询，参考View’s SELECT contains a subquery in the FROM clause1234567891011121314create view view_clients_credit_usage as select client_id, sum(credits_used) as credits_used from credit_usage group by client_id;create view view_credit_status as select credit_orders.client_id, sum(credit_orders.number_of_credits) as purchased, ifnull(t1.credits_used,0) as used from credit_orders left outer join view_clients_credit_usage as t1 on t1.client_id = credit_orders.client_id where credit_orders.payment_status='Paid' group by credit_orders.client_id);]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Stored Procedure</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM学习笔记（二）Java内存区域与内存溢出异常]]></title>
    <url>%2F2016%2F03%2F07%2FJVM%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E4%B8%8E%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[《深入理解Java虚拟机 JVM高级特性与最佳实践》 第二章 Java内存区域与内存溢出异常 运行时的数据区域 程序计数器 程序计数器（Program Counter Register）是一块比较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器； PCR为线程私有内存； 是唯一一个在Java虚拟机规范中没有规定任何OOM情况的区域； Java虚拟机栈 Java虚拟机栈（Java Virtual Machine Stacks）描述的是Java方法执行的内存模型：每个方法在在执行的同时都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法接口等信息。每个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈出栈的过程。 Java虚拟机栈也是线程私有，它的生命周期与线程相同。 Java内存区常分为堆内存（Heap）和栈内存（Stack）； OOM情况：（1）线程请求的栈深度&gt;虚拟机所运行的最大深度；（2）虚拟机动态扩展时无法申请到足够的内存 本地方法栈 本地方法栈（Native Method Stack）与虚拟机所发挥的作用非常相似的，他们之间的区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机所使用的Native方法服务。 HotSpot虚拟机把本地方法栈和虚拟机栈合二为一； 此区域会抛StackOverflowError 和 OutofMemoryError异常 Java堆 Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块，Java Heap是所有线程共享的一块内存区域，在VM启动时创建。 所有的对象实例以及数组都要在堆上分配（不绝对：栈上分配、标量替换优化技术）； Java堆是垃圾收集器管理的主要区域，也可称做GC堆（Garbage Collected Heap） 从内存回收的角度，现代收集器基本都采用分代收集算法，Java Heap可细分为新生代和老年代，再细致可分为Eden空间、From Survivor空间、To Survivor空间等–&gt;更好回收内存。 从内存分配的角度，线程共享的Java堆中可能分出多个线程私有的分配缓存区（TLAB：Thread Local Allocation Buffer）–&gt;更快分配内存。 Java堆出于逻辑连续的内存空间中，物理上可不连续，如磁盘空间一样； Java堆在实现上可时，可以实现成固定大小的，也可以按照可扩展实现（-Xmx和-Xms控制）； OOM情况：堆中没有内存完成实例分配，堆也无法再扩展时 方法区方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 也称为永久代（Permanent Generation）但随着Java8的到来，已放弃永久代改为采用Native Memory来实现方法区的规划。 此区域回收目标主要是针对常量池的回收和对类型的卸载。 运行时常量池 运行时常量池（Runtime Constants Pool）是方法区的一部分 Class文件中除了有类的版本、字段、方法、接口等描述的信息外，还有一项信息是常量池（Constant Pool Table）,用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 直接内存直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域。 能在一些场景中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。 直接内存的分配不会受到Java堆大小的限制，但会收到本机总内存（RAM以及SWAP/分页文件）大小以及处理器寻址空间的限制。 设置Xmx等参数信息时注意不能忽略直接内存，不然会引起OOM。 HotSpot虚拟机对象的创建为新生对象分配内存的分配方式由Java堆是否规整决定，而Java堆是否规整又由所采用的垃圾回收器是否带有压缩整理功能决定。 指针碰撞（Bump the Pointer）分配方式：Serial、ParNew等带有Compact过程的收集器 空闲列表（Free List）分配方式：类CMS这种基于Mark-Sweep算法的收集器 对分配内存空间的动作进行同步处理—VM采用CAS配上失败重试的方式保证更新操作的原子性； 本地线程分配缓冲（Thread Local Allocation Buffer,TLAB）：把内存分配动作按线程划分在不同空间中进行，即每个线程在Java堆中预先分配一小块内存，虚拟机是否启用TLAB，可由-XX:+/-UseTLAB参数设定； 对象的内存布局 对象在内存中存储的布局可以分为3块区域：对象头（Header）、实例数据（Instance Data）、和对齐填充（Padding）; 对象头包含2部分信息 Mark Word,存储对象自身的运行时数据（如哈希码、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳）；由于对象头与对象自身定义的数据存储大小无关，考虑到VM的空间效率，Mark Word被设计成非固定的数据结构以便在极小的空间内存储尽量多的信息，他会根据对象的状态复用自己的存储空间。 类型指针，即对象指向它的类元数据的指针，VM通过这个指针来确定这个对象是哪个类的实例。 实例数据是对象真正存储的有效信息，也似乎程序代码中定义的各种类型的字段内容。 对齐填充，并不必然存在，没有特别含义，仅仅起占位符的作用，8byte对齐。 对象的访问定位Java程序需要通过栈上的reference数据来操作堆上的具体对象，对象访问方法取决于VM实现而定，目前主流访问方式有使用句柄和直接指针2种： 句柄访问Java堆中划分出一块内存作为句柄池，reference中存储对象的句柄地址，句柄中包含对象实例数据与类型数据各自的具体地址信息； 直接指针访问Java堆对象的布局中必须考虑如何放置访问类型数据的相关信息，reference中存储对象地址； 两种访问方式各有优势 使用句柄访问最大的好处是reference中存储的是稳定的句柄地址，在对象被移动（GC时移动对象是很普遍的行为）时只会改变句柄中的实例数据指针，而reference本身不需要修改； 使用直接指针访问方式的最大好处是速度更快，它节省了一次指针定位的时间开销，由于对象访问在Java中非常频繁，因此这类开销积少成多后也是一项非常可观的执行成本； HotSpot虚拟机采用指针访问方式进行对象访问，从整个软件开发范围看，各种语言和框架使用句柄来访问的情况也非常常见。 实战OOM异常Java堆溢出Java堆用于存储对象实例，只要不断创建对象，并保证GC Roots到对象之间有可达路径来避免回收机制清除这些对象，那么当对象数量到达最大堆的容量限制后就会产生OOM。 控制参数 -Xms：堆最小值 -Xmx：堆最大值 -XX:+HeapDumpOnOutOfMemoryError：让虚拟机在出现OOM异常时Dump出当前内存堆转储快照以便事后进行分析 异常信息Java.lang.OutOfMemory + Java Heap Space 解决办法以内存映像分析工具（Eclipse Memory Analyzer）对Dump出来的堆转储快照进行分析，重点是确认内存中的对象是否是必要的，即判断是内存泄漏（Memory Leak）还是内存溢出（Memory Overflow） 如果是内存泄漏：通过工具查看泄漏对象到GC Roots的引用链，掌握泄漏对象的类型信息及引用链的信息后可较准确的定位代码位置； 如果是内存溢出：可通过检查VM的堆参数（-Xmx和-Xms），与机器物理内存对比看是否可以调大；从代码检查是否存在某些对象生命周期过长，持有状态时间过长的情况，尝试减少程序运行期的内存消耗 虚拟机栈和本地方法栈溢出控制参数HotSpot虚拟机不区分虚拟机栈和本地方法栈， -Xoss（设置本地方法栈大小）：参数设置无效; -Xss（栈容量）; 异常信息关于虚拟机栈和本地方法栈，在Java虚拟机规范中描述了两种异常： 如果线程请求的栈深度 &gt; 虚拟机所允许的最大深度，抛出StackOverFlowError异常 如果虚拟机在扩展栈时无法申请到足够的内存空间，抛出OutOfMemoryError异常 解决办法操作系统分配给每个进程的内存是有限制的，如32位Windwos限制为2G。虚拟机提供了参数来控制Java堆和方法区这两部分内存的最大值， 虚拟机栈和本地方法栈可瓜分的剩余内存=2G（操作系统限制）-Xmx（最大堆容量）-MaxPermSize（最大方法区容量）-虚拟机进程本身耗费内存；程序计数器消耗内存很小，可以忽略。 每个线程分配到的栈容量越大，可以建立的线程数就越少，建立线程时候就越容易耗尽剩余内存。 按虚拟机默认参数，栈深度在大多数情况下达到1000~2000完全没问题，对于正常方法调用（包括递归），这个深度应该完全够用；但如果是建立过多线程导致内存溢出，在不能减少线程数或者更换X64位虚拟机的情况下，就只能通过减少最大堆和减少栈容量来换取更多的线程 方法区和运行时常量区溢出运行时常量池是方法区的一部分，因此这两个区域的溢出可放在一起进行。 控制参数 -XX:PermSize（方法区最小容量） -XX:MaxPermSize （方法区最大容量） 异常信息OutOfMemoryError 后面跟随PermGen space 说明运行时常量池属于方法区（HotSpot虚拟机中的永久代）的一部分 本机直接内存溢出控制参数DirectMemory容量可通过-XX:MaxDirectMemorySize指定，不指定默认与-Xmx(Java堆最大值)一样。 异常信息由DirectMemory导致的内存溢出，一个明显的特征是在Heap Dump文件中不会看见明显的异常； 如果发现OOM之后Dump文件很小，而程序又直接或简介使用了NIO，可以考虑是不是这方面的原因。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS使用笔记]]></title>
    <url>%2F2016%2F03%2F03%2FHDFS%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Hadoop框架中最核心的设计就是：MapReduce和HDFS。MapReduce的思想是分而治之（任务的分解与结果的汇总）。HDFS是Hadoop分布式文件系统（Hadoop Distributed File System）的缩写，为分布式计算存储提供了底层支持。 HDFS的基本概念数据块(block)HDFS默认的最基本的存储单位是64M的数据块。和普通文件系统相同的是，HDFS中的文件是被分成64M一块的数据块存储的。不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间。 元数据节点(Namenode)元数据节点用来管理文件系统的命名空间。其将所有的文件和文件夹的元数据保存在一个文件系统树中。这些信息也会在硬盘上保存成以下文件：命名空间镜像(namespace image)及修改日志(edit log)其还保存了一个文件包括哪些数据块，分布在哪些数据节点上。然而这些信息并不存储在硬盘上，而是在系统启动的时候从数据节点收集而成的。 数据节点(datanode)数据节点是文件系统中真正存储数据的地方。客户端(client)或者元数据信息(namenode)可以向数据节点请求写入或者读出数据块。其周期性的向元数据节点回报其存储的数据块信息。 从元数据节点(secondary namenode)从元数据节点并不是元数据节点出现问题时候的备用节点，它和元数据节点负责不同的事情。其主要功能就是周期性将元数据节点的命名空间镜像文件和修改日志合并，以防日志文件过大。合并过后的命名空间镜像文件也在从元数据节点保存了一份，以防元数据节点失败的时候，可以恢复。 HDFS读文件原理//TODO HDFS写文件原理//TODO HDFS文件操作上传本地文件到hdfs使用Java实现在HDFS中创建文件夹123hdfs dfs -rm /user/uadb/etl/geocodinghdfs dfs -rm /user/uadb/etl/geocodinghdfs dfs -put /user/uadb/etl/geocoding /user/uadb/etl/ 删除hdfs文件 rm命令：-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]删除hdfs文件夹内所有文件：hdfs dfs -rm -R /user/hive/warehouse/geocodingdb.db/addressnodesgroupbyskeleton/* 删除hdfs文件夹 rmdir命令：[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]，不能删除非空文件夹hdfs dfs -rmdir /user/hive/warehouse/geocodingdb.db/addressnodesgroupbyskeleton HDFS相关为什么HDFS不适合大量小文件1）在HDFS中，namenode将文件系统中的元数据存储在内存中，因此，HDFS所能存储的文件数量会受到namenode内存的限制。一般来说，每个文件、目录、数据块的存储信息大约占150个字节，根据当前namenode的内存空间的配置，就可以计算出大约能容纳多少个文件了。2）有一种误解就是，之所以HDFS不适合大量小文件，是因为即使很小的文件也会占用一个块的存储空间。这是错误的，HDFS与其它文件系统不同，小于一个块大小的文件，不会占用一个块的空间。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python开发常见问题]]></title>
    <url>%2F2016%2F02%2F24%2FPython%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[记录Python开发过程中的遇到的问题 ImportError: No module named MySQLdbYou can find binary installers here (Python 2.6-3.2), here (2.7) or here (2.6). Note that you don’t have to use 64bit Python on Windows x64. You can just as well use a 32bit build of Python, for which there are more pre-built 3rd party packages around. TypeError: ‘_Callable’ object is not callableThat error occurs when you try to call, with (), an object that is not callable.A callable object can be a function or a class (that implements call method). According toPython Docs:解决：在init构造函数删除初始化对象的代码 TypeError: getPointByBounds() takes exactly 4 arguments (5 given)def getPointByBounds(lng0, lat0, lng1, lat1, step=0.001) :应该为def getPointByBounds(self,lng0, lat0, lng1, lat1, step=0.001) :第一个参数为self apply_async最后一个process调用的函数被中止暂未发现原因 hangs on ‘scanning files to index’ background taskgo to the “File” on the left top, then select “invalidate caches/restart…”, and press “invalidate and restart”. pycharm添加已有virtualEnv如通过virtualEnvWarpper创建的env，默认在pycharm中无法选择已有virtualEnv，只能新建，可通过add local手动完成虚拟环境导入File&gt;setting&gt;Project Interpreter&gt;add local&gt;选择virtualEnv\Scripts\python.exe pycharm5源代码管理之svn配置svn1.8下载地址 svn安装：注意安装路径不能带空格： pycharm配置svn：在version contro&gt;svn&gt;command line client设置为C:\Dev\SVN\bin\svn.exe]]></content>
      <categories>
        <category>脚本工具</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sqoop使用笔记]]></title>
    <url>%2F2016%2F02%2F24%2FSqoop%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Sqoop是Apache顶级项目，主要用来在Hadoop和关系数据库中传递数据。通过sqoop，可以方便的将数据从关系数据库导入到HDFS，或将数据从HDFS导出到关系数据库。 关于Sqoop官网Sqoop架构整合了Hive、Hbase和Oozie，通过map-reduce任务来传输数据，从而提供并发特性和容错。Sqoop主要通过JDBC和关系数据库进行交互。理论上支持JDBC的database都可以使用sqoop和hdfs进行数据交互。但只有一小部分经过sqoop官方测试，如：HSQLDB（1.8.0+），MySQL（5.0+），Oracle（10.2.0+），PostgreSQL（8.3+ ）；MySQL和PostgreSQL支持direct；较老的版本有可能也被支持，但未经过测试。出于性能考虑，sqoop提供不同于JDBC的快速存取数据的机制，可以通过–direct使用。 Sqoop与MySQL数据交换版本：sqoop-1.4.5-cdh5.4.0sqoop-1.4.5-cdh5.4.0官方文档数据导入示例 mysql drive导入sqoopcp /tmp/mysql-connector-java-5.1.36-bin.jar /opt/cloudera/parcels/CDH-5.4.7-1.cdh5.4.7.p0.3/lib/sqoop/libcp /opt/cloudera/parcels/CDH-5.4.7-1.cdh5.4.7.p0.3/lib/sqoop/lib/mysql-connector-java-5.1.36-bin.jar /opt/cloudera/parcels/CDH-5.4.7-1.cdh5.4.7.p0.3/lib/hadoop/lib/备注：官方文档是要导入到sqoop2目录，但copy到sqoop2目录无效，sqoop目录生效 MySQL表导入HDFS然后导入Hive 切换到hdfs用户执行：su hdfs 将MySQL数据库geocodingdb的MatchingAddress表导入HDFS用户目录 123456sqoop import --connect jdbc:mysql://192.168.1.161:3306/geocodingdb \--driver com.mysql.jdbc.Driver \--username geocodingdb --password geocodingdb \--table MatchingAddress \--fields-terminated-by '\t' --lines-terminated-by '\n' --optionally-enclosed-by '\"'--direct 附加--direct参数快速完成MySQL数据导入/导出操作与selects和inserts操作相比，MySQL Direct Connector可以用mysqldump and mysqlimport工具对MySQL数据进行更快的导入和导出操作 hive新建表结构并导入数据 123456DROP TABLE IF EXISTS geocodingdb.MatchingAddress;create external table geocodingdb.MatchingAddress (source_address_id string,source_address string ,head_splitted_address string,splitted_skeleton_addressnode string,skeleton_addressnode string,skeleton_addressnode_type string,tail_address string,tail_splitted_address string)row format delimited fields terminated by '\t' stored as textfile;load data inpath '/user/hdfs/MatchingAddress/*' into table geocodingdb.MatchingAddress; MySQL表直接导入Hive MySQL表授权 12GRANT ALL PRIVILEGES ON *.* TO 'geocodingdb'@'%' IDENTIFIED BY 'geocodingdb' with grant option;FLUSH PRIVILEGES; hive-import命令注意导入MySQL表结构字段顺序需与Hive表结构字段顺序一致 123456sqoop import --connect jdbc:mysql://192.168.1.161:3306/geocodingdb \--driver com.mysql.jdbc.Driver \--username geocodingdb --password geocodingdb \--table MatchingAddress \--fields-terminated-by '\t' --lines-terminated-by '\n' --optionally-enclosed-by '\"' \--direct Hive表导出到MySQL123456sqoop export --direct --connect jdbc:mysql://192.168.1.161:3306/geocodingdb --driver com.mysql.jdbc.Driver \--username geocodingdb --password geocodingdb \--table MatchedAddressGroupbySkeleton \--export-dir /user/hive/warehouse/geocodingdb.db/matchedaddressgroupbyskeleton \--input-fields-terminated-by "\t" \--input-null-string "\\\\N" --input-null-non-string "\\\\N" Sqoop(MySQL)常用命令指定列sqoop import –connect jdbc:mysql://db.foo.com/corp –table EMPLOYEES \–columns “employee_id,first_name,last_name,job_title” 使用8个线程sqoop import –connect jdbc:mysql://db.foo.com/corp –table EMPLOYEES \-m 8 快速模式sqoop import –connect jdbc:mysql://db.foo.com/corp –table EMPLOYEES \–direct 使用sequencefile作为存储方式sqoop import –connect jdbc:mysql://db.foo.com/corp –table EMPLOYEES \–class-name com.foocorp.Employee –as-sequencefile 分隔符sqoop import –connect jdbc:mysql://db.foo.com/corp –table EMPLOYEES \–fields-terminated-by ‘\t’ –lines-terminated-by ‘\n’ \–optionally-enclosed-by ‘\”‘ 导入到hivesqoop import –connect jdbc:mysql://db.foo.com/corp –table EMPLOYEES \–hive-import 条件过滤sqoop import –connect jdbc:mysql://db.foo.com/corp –table EMPLOYEES \–where “start_date &gt; ‘2010-01-01’” 用dept_id作为分个字段sqoop import –connect jdbc:mysql://db.foo.com/corp –table EMPLOYEES \–split-by dept_id 追加导入sqoop import –connect jdbc:mysql://db.foo.com/somedb –table sometable \–where “id &gt; 100000” –target-dir /incremental_dataset –append 问题记录sqoop export –direct导出mysqlimport错误错误描述：Cannot run program “mysqlimport”: error=2, No such file or directory解决办法：附加--driver com.mysql.jdbc.Driver参数 sqoop export –direct导出mapreduce程序错误错误描述1：Caused by: java.lang.RuntimeException: Can’t parse input data: ‘长浜 STR 18119 B316D057CE523018E0430A23A2C13018’解决办法：附加--input-fields-terminated-by &quot;\t&quot;参数 错误描述2：com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry ‘1614’ for key ‘PRIMARY’解决办法：附加--input-null-string &quot;\\\\N&quot; --input-null-non-string &quot;\\\\N&quot;如果遇到空值就插入null Sqoop 导入 Hive 导致发生 Null Pointer Exception (NPE)解决办法：首先通过 Sqoop 将数据导入 HDFS，然后将其从 HDFS 导入 Hive。 MySQL导入Hive表报错Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ‘쀀’ )’ at line 1解决：hive表编码问题；导入时不附加–hcatalog-table，手动新建表，然后导入数据 Sqoop导入MySQL大表内存溢出问题SqoopUserGuide抛出异常java.lang.OutOfMemoryError：GC overhead limit exceeded导致服务起不来 参考：http://www.hadooptechs.com/sqoop/handling-database-fetch-size-in-sqoop 修改yarn的nodemanager xmx还是sqoop 的xmx 分页查询写入123456sqoop import --connect jdbc:mysql://192.168.1.161:3306/geocodingdb --username geocodingdb --password geocodingdb \--query 'select * from MatchingAddress WHERE $CONDITIONS limit 0,100000' \--split-by guid \--fields-terminated-by '\t' --lines-terminated-by '\n' --optionally-enclosed-by '\"' \--target-dir /user/hive/warehouse/geocodingdb.db/matchingaddress \--append 123456sqoop import --connect jdbc:mysql://192.168.1.161:3306/geocodingdb --username geocodingdb --password geocodingdb \--query 'select * from MatchingAddress WHERE $CONDITIONS' \--split-by guid \--fields-terminated-by '\t' --lines-terminated-by '\n' --optionally-enclosed-by '\"' \--target-dir /user/hive/warehouse/geocodingdb.db/matchingaddress \--append sqoop import –connect jdbc:mysql://192.168.1.161:3306/geocodingdb?user=geocodingdb&amp;password=geocodingdb&amp;dontTrackOpenResources=true&amp;defaultFetchSize=10000&amp;useCursorFetch=true –query ‘select * from MatchingAddress WHERE $CONDITIONS’ –split-by guid \–fields-terminated-by ‘\t’ –lines-terminated-by ‘\n’ –optionally-enclosed-by ‘\”‘ \–target-dir /user/hive/warehouse/geocodingdb.db/matchingaddress \–append sqoop import –connect jdbc:mysql://192.168.1.161:3306/geocodingdb \–driver com.mysql.jdbc.Driver \–username geocodingdb –password geocodingdb \–direct \–table MatchingAddress1 \–fields-terminated-by ‘\t’ –lines-terminated-by ‘\n’ –optionally-enclosed-by ‘\”‘ \–target-dir /user/hive/warehouse/geocodingdb.db/matchingaddress \–append sqoop import –connect jdbc:mysql://192.168.1.161:3306/geocodingdb \–driver com.mysql.jdbc.Driver \–username geocodingdb –password geocodingdb \–direct \–table MatchingAddress2 \–fields-terminated-by ‘\t’ –lines-terminated-by ‘\n’ –optionally-enclosed-by ‘\”‘ \–target-dir /user/hive/warehouse/geocodingdb.db/matchingaddress \–append sqoop import –connect jdbc:mysql://192.168.1.161:3306/geocodingdb \–driver com.mysql.jdbc.Driver \–username geocodingdb –password geocodingdb \–direct \–table MatchingAddress3 \–fields-terminated-by ‘\t’ –lines-terminated-by ‘\n’ –optionally-enclosed-by ‘\”‘ \–target-dir /user/hive/warehouse/geocodingdb.db/matchingaddress \–append sqoop import –connect jdbc:mysql://192.168.1.161:3306/geocodingdb \–driver com.mysql.jdbc.Driver \–username geocodingdb –password geocodingdb \–direct \–table MatchingAddress4 \–fields-terminated-by ‘\t’ –lines-terminated-by ‘\n’ –optionally-enclosed-by ‘\”‘ \–target-dir /user/hive/warehouse/geocodingdb.db/matchingaddress \–append sqoop import –connect jdbc:mysql://192.168.1.161:3306/geocodingdb \–driver com.mysql.jdbc.Driver \–username geocodingdb –password geocodingdb \–direct \–table MatchingAddress5 \–fields-terminated-by ‘\t’ –lines-terminated-by ‘\n’ –optionally-enclosed-by ‘\”‘ \–target-dir /user/hive/warehouse/geocodingdb.db/matchingaddress \–append Stack trace: ExitCodeException exitCode=255:]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Sqoop</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《深入理解Java虚拟机》读书笔记]]></title>
    <url>%2F2016%2F02%2F22%2F%E3%80%8A%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[2016上半年花点时间深入了解JVM，读《《深入理解Java虚拟机 JVM高级特性与最佳实践》，整理遇到过的内存泄漏，性能优化问题 第一章 走近JavaJava技术体系 Java程序设计语言、Java虚拟机、Java API类库三部分统称为JDK(Java Development Kit) ,JDK是Java程序开发的最小环境 Java API类库中的Java SE API子集和Java虚拟机两部分统称为JRE(Java Runtime Environment)，JRE是支持Java程序运行的标准环境 按照Java技术关注的重点业务领域来划分，Java技术体系可分为4个平台 Java Card：支持Applets(Java小程序)运行在小内存设备（如智能卡）上的平台； Java ME(Micro Edition)：支持Java运行在移动终端上的平台；（今有Android SDK） Java SE(Standard Edition)：支持面向桌面级应用的Java平台； Java EE(Enterprise Edition)：支持使用多层架构的企业级应用(如ERP、CRM应用)的Java平台； Java发展史Java虚拟机发展史 Sun Classic/Extract VM Sun HotSpot VM Sun Mobile-Embedded VM/Meta-Circular VM Bea Jrockit/IDM J9 VM Azul VM/BEA Liquid VM Apache Harmony/Google Android Dalvik VM Microsoft JVM… Java技术的未来展望 模块化（Jigsaw） 混合语言：多语言混合编程，通过特定领域发语言去解决特定领域的问题 多核并行（concurrent.forkjoin；Lambada；函数式编程） 丰富语法（Coin子项目） 64位虚拟机（计算机终究完全过渡到64位的时代） JDK编译实战OpenJDK7下载Building the source code for the OpenJDK requires a certain degree of technical expertise. 第二章 Java内存区域与内存溢出异常运行时的数据区域 程序计数器 程序计数器（Program Counter Register）是一块比较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器； PCR为线程私有内存； 是唯一一个在Java虚拟机规范中没有规定任何OOM情况的区域； Java虚拟机栈 Java虚拟机栈（Java Virtual Machine Stacks）描述的是Java方法执行的内存模型：每个方法在在执行的同时都会创建一个栈帧（Stack Frame）用于存储 局部变量表、操作数栈、动态链接、方法接口 等信息。每个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈出栈的过程。 Java虚拟机栈也是线程私有，它的生命周期与线程相同。 Java内存区常分为 堆内存（Heap）和栈内存（Stack）； OOM情况：（1）线程请求的栈深度&gt;虚拟机所运行的最大深度；（2）虚拟机动态扩展时无法申请到足够的内存 本地方法栈本地方法栈（Native Method Stack）与虚拟机所发挥的作用非常相似的，他们之间的区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机所使用的Native方法服务。 HotSpot虚拟机把本地方法栈和虚拟机栈合二为一； 此区域会抛StackOverflowError 和 OutofMemoryError异常 Java堆Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块，Java Heap是所有线程共享的一块内存区域，在VM启动时创建。 所有的对象实例以及数组都要在堆上分配（不绝对：栈上分配、标量替换优化技术）； Java堆是垃圾收集器管理的主要区域，也可称做GC堆（Garbage Collected Heap） 从内存回收的角度，现代收集器基本都采用分代收集算法，Java Heap可细分为新生代和老年代，再细致可氛围Eden空间、From Survivor空间、To Survivor空间等–&gt;更好回收内存。 从内存分配的角度，线程共享的Java堆中可能分出多个线程私有的分配缓存区（TLAB：Thread Local Allocation Buffer）–&gt;更快分配内存。 Java堆出于逻辑连续的内存空间中，物理上可不连续，如磁盘空间一样； Java堆在实现上可时，可以实现成固定大小的，也可以按照可扩展实现（-Xmx和-Xms控制）； OOM情况：堆中没有内存完成实例分配，堆也无法再扩展时 方法区 Non-Heap：在Java虚拟机规范中，将方法区作为堆的一个逻辑部分来对待，但事实上，方法区并不是堆（Non-Heap）； 永久代：JavaGC的分代收集机制分为3个代：年青代，老年代，永久代，将方法区定义为“永久代”，这是因为，对于之前的HotSpot Java虚拟机的实现方式中，将分代收集的思想扩展到了方法区，并将方法区设计成了永久代。不过，除HotSpot之外的多数虚拟机，并不将方法区当做永久代，随着Java8的到来，已放弃永久代改为采用Native Memory来实现方法区的规划。 线程共享区域：方法区（Method Area）与Java堆一样，是各个线程共享的内存区域， 方法区用于存储已经被虚拟机加载的类信息（即加载类时需要加载的信息，包括版本、field、方法、接口等信息）、final常量、静态变量、编译器即时编译后的代码等数据。 方法区在物理上也不需要是连续的，可以选择固定大小或可扩展大小，并且方法区比堆还多了一个限制：可以选择是否执行垃圾收集。 一般的，方法区上执行的垃圾收集是很少的，这也是方法区被称为永久代的原因之一（HotSpot），但这也不代表着在方法区上完全没有垃圾收集，此区域回收目标主要是针对常量池的回收和对类型的卸载。 运行时常量池运行时常量池（Runtime Constants Pool）是方法区的一部分 Class文件中除了有类的版本、字段、方法、接口等描述的信息外，还有一项信息是常量池（Constant Pool Table）,用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 直接内存直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域。 能在一些场景中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。 直接内存的分配不会受到Java堆大小的限制，但会收到本机总内存（RAM以及SWAP/分页文件）大小以及处理器寻址空间的限制。 设置Xmx等参数信息时注意不能忽略直接内存，不然会引起OOM。 HotSpot虚拟机]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM学习笔记（一）走近Java]]></title>
    <url>%2F2016%2F02%2F22%2FJVM%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%E8%B5%B0%E8%BF%91Java%2F</url>
    <content type="text"><![CDATA[2016上半年花点时间深入了解JVM，读《《深入理解Java虚拟机 JVM高级特性与最佳实践》，整理遇到过的内存泄漏，性能优化问题 第一章 走近JavaJava技术体系 Java程序设计语言、Java虚拟机、Java API类库三部分统称为JDK(Java Development Kit) ,JDK是Java程序开发的最小环境 Java API类库中的Java SE API子集和Java虚拟机两部分统称为JRE(Java Runtime Environment)，JRE是支持Java程序运行的标准环境 按照Java技术关注的重点业务领域来划分，Java技术体系可分为4个平台 Java Card：支持Applets(Java小程序)运行在小内存设备（如智能卡）上的平台； Java ME(Micro Edition)：支持Java运行在移动终端上的平台；（今有Android SDK） Java SE(Standard Edition)：支持面向桌面级应用的Java平台； Java EE(Enterprise Edition)：支持使用多层架构的企业级应用(如ERP、CRM应用)的Java平台； Java发展史Java虚拟机发展史 Sun Classic/Extract VM Sun HotSpot VM Sun Mobile-Embedded VM/Meta-Circular VM Bea Jrockit/IDM J9 VM Azul VM/BEA Liquid VM Apache Harmony/Google Android Dalvik VM Microsoft JVM… Java技术的未来展望 模块化（Jigsaw） 混合语言：多语言混合编程，通过特定领域发语言去解决特定领域的问题 多核并行（concurrent.forkjoin；Lambada；函数式编程） 丰富语法（Coin子项目） 64位虚拟机（计算机终究完全过渡到64位的时代） JDK编译实战OpenJDK7下载 Building the source code for the OpenJDK requires a certain degree of technical expertise.]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyEclipse常用插件]]></title>
    <url>%2F2016%2F01%2F27%2FMyEclipse%E5%B8%B8%E7%94%A8%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[记录以J2EE开发采用MyEclipse IDE的常用插件 SVNsvn update site URLsvn offline package FatJarFatJar update site URL Freemarker EditorFreemarker Editor update site URL安装时选择Jboss IDE即可 Drools插件drools 5.5.0 update site URL OneJarOneJar官网 maven配置onejar打包1234567891011121314151617181920212223242526272829&lt;!-- Make this jar executable --&gt; &lt;plugin &gt; &lt;groupId &gt;org.apache.maven.plugins &lt;/groupId &gt; &lt;artifactId &gt;maven-jar-plugin &lt;/artifactId &gt; &lt;configuration &gt; &lt;archive &gt; &lt;manifest &gt; &lt;mainClass &gt;gto.amo.mapper.app.form.Main &lt;/mainClass &gt; &lt;/manifest &gt; &lt;/archive &gt; &lt;/configuration &gt; &lt;/plugin &gt; &lt;!-- Includes the runtime dependencies --&gt; &lt;plugin &gt; &lt;groupId &gt;com.jolira &lt;/groupId &gt; &lt;artifactId &gt;onejar-maven-plugin &lt;/artifactId &gt; &lt;version &gt;1.4.4 &lt;/version &gt; &lt;executions &gt; &lt;execution &gt; &lt;configuration &gt; &lt;attachToBuild &gt;true &lt;/attachToBuild &gt; &lt;classifier &gt;onejar &lt;/classifier &gt; &lt;/configuration &gt; &lt;goals &gt; &lt;goal &gt;one-jar &lt;/goal &gt; &lt;/goals &gt; &lt;/execution &gt; &lt;/executions &gt; &lt;/plugin &gt;]]></content>
      <categories>
        <category>IDE</category>
      </categories>
      <tags>
        <tag>J2EE</tag>
        <tag>Eclipse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ne04j单机版和集群版部署]]></title>
    <url>%2F2016%2F01%2F25%2FNe04j%E5%8D%95%E6%9C%BA%E7%89%88%E5%92%8C%E9%9B%86%E7%BE%A4%E7%89%88%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Neo4j HA(Neo4j High Availability)，高可用性主要指其包含容错机制和可进行水平扩展，即Neo4j Cluster 部署Ne04j单机版(windows)java se 8安装java8下载安装java se8后，并临时设置环境变量：set JAVA_HOME=E:\Software\jdk8x64\jre1.8（避免与本机java7冲突） 下载Neo4j-3.0.0neo4j-enterprise-3.0.0-M02-windows 下载Neo4j-2.3.2neo4j-enterprise-2.3.2-windows设置NEO4J_HOME， Neo4j Browser 运行bin\Neo4j.bat，如cd F:\Dev\neo4j-enterprise-3.0.0-M02\bin &amp;&amp; Neo4j.bat 在浏览器打开Neo4j的在线REPL，即Neo4j Browser,在命令行输入Cypher query语句进行查询 在浏览器打开Neo4j Guide了解Neo4j 老版本的在线入口：neo4j webAdmin 在Windows PowerShell运行Neo4j12345678910111213141516# 权限配置Set-ExecutionPolicy -ExecutionPolicy RemoteSigned# 导入Neo4j模块Import-Module C:\Neo4j\bin\Neo4j-Management.psd1# 查询Neo4j命令Get-Command -Module Neo4j-Management# 安装Neo4j服务Install-Neo4jServer# 查询NEO4J_HOME路径Get-Neo4jServer C:\Neo4j# 启动Neo4j服务Start-Neo4jServer# 关闭Neo4j服务Stop-Neo4jServer# 重启Neo4j服务Restart-Neo4jServer Neo4j Browser常用脚本:help 帮助shift+enter 多行书写ctrl+enter 执行:clear 清空执行结果:play 打开入门教程 部署Neo4j集群(Linux)neo4j manual doc TODONeo4j HA has been designed to make the transition from single machine to multi machine operation simple, by not having to change the already existing application.Consider an existing application with Neo4j embedded and running on a single machine. To deploy such an application in a multi machine setup the only required change is to switch the creation of the GraphDatabaseService from GraphDatabaseFactory to HighlyAvailableGraphDatabaseFactory. Since both implement the same interface, no additional changes are required.]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j图数据库学习笔记]]></title>
    <url>%2F2016%2F01%2F25%2FNeo4j%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Explore the World of Graphs – From Query Efficiency to Business Performance 关于图形数据库图形数据库是一种非关系型数据库，它应用图形理论存储实体之间的关系信息。最常见的一个例子，就是社会网络中人与人之间的关系。 当前有流行图形数据库：Neo4j、FlockDB、AllegroGraph、GraphDB、InfiniteGraph、OrientDB、InfoGrid和HypergraphDB等等，另有自称比MongoDB和Neo4j性能更佳的多模型数据库ArangoDB,见nosql-tests 关系型数据库用于存储“关系型”数据的效果并不好，其查询复杂、缓慢、超出预期，而图形数据库的独特设计恰恰弥补了这个缺陷。 Neo4j简介官方Manual参考full-stack-web-development-using-neo4jNeo4j是一个用Java实现、完全兼容ACID的图形数据库。数据以一种针对图形网络进行过优化的格式保存在磁盘上。Neo4j的内核是一种极快的图形引擎，具有数据库产品期望的所有特性，如恢复、两阶段提交、符合XA等。自2003年起，Neo4j就已经被作为24/7的产品使用。Neo4j是目前主流的一个图数据库，相比传统的关系型数据库，它可以快速的进行基于人际社交网络类的查询查询和检索;它同时提供了cypher语言来方便进行图数据库的操作和查询，该查询语言类似SQL语言。Neo4j的数据并非保存在表或集合中，而是保存为节点以及节点之间的关系。在Neo4j中，节点以及关系都能够包含保存值的属性，此外： 可以为节点设置零或多个标签（例如Author或Book） 每个关系都对应一种类型（例如WROTE或FRIEND_OF） 关系总是从一个节点指向另一个节点（但可以在不考虑指向性的情况下进行查询） 为什么要选择Neo4j？在考虑为web应用选择某个数据库时，我们需要考虑对它有哪些方面的期望，其中最重要的一些条件包括： 它是否易于使用？ 它是否允许你方便地回应对需求的变更？ 它是否支持高性能查询？ 是否能够方便地对其进行数据建模？ 它是否支持事务？ 它是否支持大规模应用？ 它是否足够有趣（很遗憾的是对于数据库的这方面要求经常被忽略）？ 从这以下几个方面来说，Neo4j是一个合适的选择。Neo4j…… 自带一套易于学习的查询语言（名为Cypher） 不使用schema，因此可以满足你的任何形式的需求 与关系型数据库相比，对于高度关联的数据（图形数据）的查询快速要快上许多 它的实体与关系结构非常自然地切合人类的直观感受 支持兼容ACID的事务操作 提供了一个高可用性模型，以支持大规模数据量的查询，支持备份、数据局部性以及冗余 提供了一个可视化的查询控制台，你不会对它感到厌倦的 什么时候不应使用Neo4j？作为一个图形NoSQL数据库，Neo4j提供了大量的功能，但没有什么解决方案是完美的。在以下这些用例中，Neo4j就不是非常适合的选择： 记录大量基于事件的数据（例如日志条目或传感器数据） 对大规模分布式数据进行处理，类似于Hadoop 二进制数据存储 适合于保存在关系型数据库中的结构化数据 虽然Neo4j也能够处理“大数据”，但它毕竟不是Hadoop、HBase或Cassandra，通常来说不会在Neo4j数据库中直接处理海量数据（以PB为单位）的分析。但如果你乐于提供关于某个实体及其相邻数据关系（比如你可以提供一个web页面或某个API返回其结果），那么它是一种良好的选择。无论是简单的CRUD访问，或是复杂的、深度嵌套的资源视图都能够胜任。 虽然关系型数据库对于保存结构化数据来说依然是最佳的选择，但NoSQL数据库更适合于管理半结构化数据、非结构化数据以及图形数据。如果数据模型中包括大量的关联数据，并且希望使用一种直观的、有趣的并且快速的数据库进行开发，那么可以考虑尝试Neo4j。 Neo4j图模型Neo4J中的图形模型要点：Nodes与Relationships可以被赋予Properties(key-value); Nodes可按label分组；Relationships可赋予direction和type并最终构成数据形态；Neo4j可存储10亿级别的数据量Neo4J使用以下索引机制：一个超级参考节点通过一条特殊类别的边线“REFERENCE”与所有节点相连。这实际上允许创建多个索引，借以通过不同的边线类别对其加以区分。Neo4J还提供了一些特殊功能，如列出特定节点的相邻诸节点或是两节点间长度最短的诸类路径等。注意要使用上述各类“遍历”功能，Neo4J要求指定路径中经过的边线类别。 版本enterprise和community公共特性12345678Property Graph ModelNative Graph Processing &amp; StorageACIDCypher – Graph Query LanguageLanguage Drivers most popular languagesREST APIHigh-Performance Native APIHTTPS (via Plug-in) enterprise性能扩展特性1234567Enterprise Lock ManagerCache ShardingClustered ReplicationCypher Query TracingProperty Existence ConstraintsHot BackupsAdvanced Monitoring 嵌入式使用可不必将Neo4J作为软件加以安装。可在项目中导入JAR文件来建立嵌入式图形数据库，该操作将在硬盘上建立对应的目录，类似sqlite。 High AvailabilityNeo4j HA(Neo4j High Availability)，高可用性主要指其包含容错机制和可进行水平扩展，即Neo4j Cluster Neo4j相关技术选型所有主流的编程语言都通过HTTP_API的方式支持Neo4j，或者采用基本的HTTP类库，或是通过某些原生的类库提供更高层的抽象。此外，由于Neo4j是以Java语言编写的，因此所有包含JVM接口的语言都能够充分利用Neo4j中的高性能API。 Neo4j本身也提供了一个“技术栈”，它允许你选择不同的访问方式，包括简单访问乃至原生性能等等。它提供的特性包括： 通过一个HTTP API执行Cypher查询，并获取JSON格式的结果 一种“非托管扩展”机制，允许你为Neo4j数据库编写自己的终结点 通过一个高层Java API指定节点与关系的遍历 通过一个低层的批量加载API处理海量初始数据的获取 通过一个核心Java API直接访问节点与关系，以获得最大的性能 Jcypher 集成Remote、Emberded和InMemmory三种Neo4j数据库访问形式，在程序测试和Neo4j Browserz间切换很方便； 无需在POJO中手动标注实现OGM，会自动将对象嵌套关系转换为Graph Relationship，可更专注与业务逻辑； NativeAPINeo4j官方原生API，需手动进行事物管理，实现较为繁琐； Spring Data for Neo4jdoc-apiThe Spring Data Neo4j Guide Bookdemo.neo4j.springdata spring-data-neo4jSpring Data Neo4j spring-data-neo4j-restpring Data Neo4j Wrapper for the Neo4j REST API, provides a Graph Database proxy for the remote invocation spring-data-neo4j-aspectsAdvanced Mapping support for Spring Data Neo4j CypherCypher(Neo4j’s graph query language)，类似SQLCypher为ASCII风格的语法，它在括号内表示节点名称，并用箭头表示一个节点指向另一个节点的关系。Cypher通过这种方式允许你匹配某个指定的子图形模式。 CreateCreate(Create Nodes and Relationships)123456789101112131415CREATE (ee:Person &#123; name: "Emil", from: "Sweden", klout: 99 &#125;)# () parenthesis to indicate a node# ee:Person a variable 'ee' and label 'Person' for the new node# &#123;&#125; brackets to add properties to the nodeCREATE多个Nodes和RelationshipsMATCH (ee:Person) where ee.name = "Emil"CREATE (js:Person &#123; name: "Johan", from: "Sweden", learn: "surfing" &#125;),(ir:Person &#123; name: "Ian", from: "England", title: "author" &#125;),(rvb:Person &#123; name: "Rik", from: "Belgium", pet: "Orval" &#125;),(ally:Person &#123; name: "Allison", from: "California", hobby: "surfing" &#125;),(ee)-[:KNOWS &#123;since: 2001&#125;]-&gt;(js),(ee)-[:KNOWS &#123;rating: 5&#125;]-&gt;(ir),(js)-[:KNOWS]-&gt;(ir),(js)-[:KNOWS]-&gt;(rvb),(ir)-[:KNOWS]-&gt;(js),(ir)-[:KNOWS]-&gt;(ally),(rvb)-[:KNOWS]-&gt;(ally) MATCHMATCH(Finding nodes)12345MATCH (ee:Person) WHERE ee.name = "Emil" RETURN ee;# (ee:Person) a single node pattern with label 'Person' which will assign matches to the variable 'ee'# WHERE clause to constrain the results# ee.name = "Emil" compares name property to the value "Emil"# RETURN clause used to request particular results Pattern matchingPattern matching(Describe what to find in the graph) 123456# find Emil's friends:MATCH (ee:Person)-[:KNOWS]-(friends) WHERE ee.name = "Emil" RETURN ee, friends# MATCHclause to describe the pattern from known Nodes to found Nodes# (ee)starts the pattern with a Person (qualified by WHERE)# -[:KNOWS]-matches "KNOWS" relationships (in either direction)# (friends)will be bound to Emil's friends RecommendPattern matching can be used to make recommendations，这个赞，天然的可以进行推荐，如六度分割（Six Degrees of Kevin Bacon），即Bacon Path最短路径问题12345# Johan is learning to surf, so he may want to find a new friend who already does:MATCH (js:Person)-[:KNOWS]-()-[:KNOWS]-(surfer) WHERE js.name = "Johan" AND surfer.hobby = "surfing" RETURN DISTINCT surfer# ()empty parenthesis to ignore these nodes# DISTINCTbecause more than one path will match the pattern# surferwill contain Allison, a friend of a friend who surfs Analyze可视化查询计划理解Cypher查询如何工作（EXPLAIN/PROFILE）123PROFILE MATCH (js:Person)-[:KNOWS]-()-[:KNOWS]-(surfer)WHERE js.name = "Johan" AND surfer.hobby = "surfing"RETURN DISTINCT surfer Visualizationvisualization Neo4j可用性测试Neo4j in Production读写性能测试 Neo4j性能优化配置neo4j-configuration-introduction 确认JVM没有在GC垃圾回收上耗费太多时间,以确保有足够的heap避免heavy/peak引起GC-trashing时，GC-trashing时性能会下降两个数量级 JVM以-server参数启动，并设置一个合理的heap size，太大的heap也会损害性能，所以需要尝试不同的heap size 用并发的GC垃圾回收器，在大多数情况下-XX:+UseG1GC是最佳实践 给Neo4j page cache 设置大量的内存，在一个专用服务器上，需要平衡4大部分内存分配：操作系统、Neo4j JVM、Neo4j page cache和Neo4j Lucene全文索引用到的paging memory 服务器操作系统一般需要1到2GB的内存，服务器物理内存越大，操作系统需要分配的内存越大； 由于Neo4j JVM和内存回收器的head-room消耗，需要足够大的heap memory用来进行事物状态和查询处理；因为工作负载非常依赖heap memory，所以配置heap memory从1G到32G都很常见； Neo4j page cache 最好有足够的内存来保持整个数据集在内存中，也就是说page cache应该足够大，以适应所有的neostore. 文件（不是neostore.transaction.db. 文件）； 配置足够的操作系统page cache以适应索引的内容和schema目录,因为如果索引不能装入内存,它将会影响索引查找性能；]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python并行编程笔记]]></title>
    <url>%2F2016%2F01%2F23%2FPython%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[大数据时代，并发/并行是不变的话题，以并行计算来提高程序运行效率，可充分利用硬件资源（CPU，内存，磁盘/网络IO）。Python作为脚本语言，解释器简洁轻量，软件研发后无需编译且更新部署方便，对于实现一些类似爬取的工具十分友好。 关于多进程问题http://blog.tankywoo.com/2015/09/06/cant-pickle-instancemethod.html如果你的代码是CPU密集型，多个线程的代码很有可能是线性执行的。所以这种情况下多线程是鸡肋，效率可能还不如单线程因为有context switch,如果你的代码是IO密集型，多线程可以明显提高效率。例如制作爬虫（我就不明白为什么Python总和爬虫联系在一起…不过也只想起来这个例子…），绝大多数时间爬虫是在等待socket返回数据。这个时候C代码里是有release GIL的，最终结果是某个线程等待IO的时候其他线程可以继续执行。 python线程python协程协程，又称微线程，纤程。英文名Coroutine。最大的优势就是协程极高的执行效率。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。第二大优势就是不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。因为协程是一个线程执行，那怎么利用多核CPU呢？最简单的方法是多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。Python对协程的支持还非常有限，用在generator中的yield可以一定程度上实现协程。虽然支持不完全，但已经可以发挥相当大的威力了。 threading生产者-消费者模型python多线程monkey.patchmonkey patch指的是在运行时动态替换,一般是在startup的时候.用过gevent就会知道,会在最开头的地方gevent.monkey.patch_all();把标准库中的thread/socket等给替换掉.这样我们在后面使用socket的时候可以跟平常一样使用,无需修改任何代码,但是它变成非阻塞的了.之前做的一个游戏服务器,很多地方用的import json,后来发现ujson比自带json快了N倍,于是问题来了,难道几十个文件要一个个把import json改成import ujson as json吗?其实只需要在进程startup的地方monkey patch就行了.是影响整个进程空间的.同一进程空间中一个module只会被运行一次. geventgevent 并发实现：from gevent import monkey; monkey.patch_socket() python多进程multiprocessingOne can create a pool of processes which will carry out tasks submitted to it with the Pool class.参考：http://www.davidmoodie.com/python-multiprocessing-fbalbumdownloader/ 注意：multiprocessing与gevent同时使用时，如果运行了gevent.monkey.patch_thread()或patch_all(),pool进程池将无效]]></content>
      <categories>
        <category>脚本工具</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>并行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建Python工程化开发框架]]></title>
    <url>%2F2016%2F01%2F20%2F%E6%90%AD%E5%BB%BAPython%E5%B7%A5%E7%A8%8B%E5%8C%96%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[2016的元月以python作为开端。采用python进行网络数据聚合抽取，需调研并搭建python工程化开发框架，几番迭代，一个适用于数据采集的开发环境搭建完成： 开发环境：python2.7.10 32位/pycharm5 项目构建：virtualenv/virtualenvWrapper 虚拟运行环境；pip 依赖项管理；pyBuilder项目构建，其中pyBuilder以disutils用于项目打包 项目文档：mkdocs/sphinx；参考python-guide-writing Web框架：Tornado/web,py（非阻塞式web服务器，精简）；django（文档功能齐全，但生态封闭） 单元测试：unittest/coverage(测试覆盖率统计) 并行框架：gevent(多线程)+monkey patch(运行时动态替换模块)，multiprocessing(多进程) 爬虫框架：scrapy/selenium 接口设计： zope.interface 编码风格：google-python-styleguide pip install -i http://pypi.douban.com/simple Shapely –trusted-host pypi.douban.com pip源配置临时换源临时换源只在某一条命令中生效，只要在命令中加上”-i“，指定使用的源即可pip install scrapy -i url，如安装pandas：pip install -i http://pypi.douban.com/simple pandas --trusted-host pypi.douban.com 永久换源要是想永久更改pip源，在pip的配置文件（~/.pip/pip.conf）中增加12[global]index-url=http://pypi.douban.com/simple 一些国内的pip源12345http://mirrors.aliyun.com/pypi/simple/ # 阿里云http://pypi.douban.com/simple #豆瓣http://pypi.hustunique.com/simple #华中理工大学http://pypi.sdutlinux.org/simple #山东理工大学http://pypi.mirrors.ustc.edu.cn/simple #中国科学技术大学 打包部署问题python打包部署历史distutils&gt;setuptools/easyinstall(.egg)&gt;pip/wheel(.whl) pip导出dependency:pip freeze &gt; requirements.txt安装dependency:pip install -r requirements.txt whl二进制文件whl 如何cmd中运行开发的*.py程序模块新增workspace.path文件到virtualenv目录（如E:\PythonWorkspace\ugc\ugc_venv\Lib\site-packages）12E:\PythonWorkspace\ugc\ugc.aggregatorE:\PythonWorkspace\ugc\ugc.aggregator\src\main\python 注意path文件中的模块目录必须有init.py文件 virtualenv习惯了J2EE下的maven开发，对于python默认的module都安装到site-packages下的混乱不能理解，好在原来有virtualenv，它比maven本地repositoy库更具有独立性，当然冗余module是代价,好在python intepreter足够小巧。virtualenv可以用来创建隔离的python环境 ，但新建出来的virtualenv都依赖本机安装的python底层dll等库。 安装：pip install virtualenv 新建virtualEnv：virtualenv --no-site-packages venv 进入venvShel：E:\PythonWorkspace\ugc\ugc_venv\Scripts\activate 问题记录 问题描述：执行pip install MySQL-python报错： fatal error C1083: Cannot open include file: &#39;config-win.h&#39;: No such file or directory或者报错No module named MySQLdb解决方案：从C:\Python27\Lib\site-packages复制mysql相关的文件到虚拟环境的site-packages目录 virtualenvwrappervirtualenv创建的环境都是零散的，而且还要执行cd，执行source 来激活环境。 如此繁琐十分影响工作效率，于是有了virtualenvwrapper。vw可以进行环境的管理，把创建的环境记录下来，并进行管理。 安装 linux：pip install virtualenvwrapper windows：pip install virtualenvwrapper-win 配置 安装完毕过后在环境变量里面新建一个WORKON_HOME字段存储虚拟python环境,WORKON_HOME：E:\PythonWorkspace\venv 环境变量立即生效：cmd中运行set WORKON_HOME=E:\PythonWorkspace\venv 常用命令virtualenvwrapper运行bat默认安装在C:\Python27\Scripts\*.bat 创建虚拟环境:mkvirtualenv VirtualenvName 列出所有虚拟环境:Lsvirtualenv 移除虚拟环境:rmvirtualenv VirtualenvName 切换到VirtualenvName环境:workon VirtualenvName 退出当前虚拟环境:deactivate 问题记录 问题描述：执行virtualenv报错：SyntaxError: Non-ASCII character &#39;\x90&#39; in file C:\Python27\Scripts\virtualenv.exe on line 1, but no encoding declared;解决方案：卸载virtualenvpip uninstall virtualenv；卸载virtualenvwarpperpip uninstall virtualenvwarpper-win；重新安装virtualenvwarpperpip install virtualenvwarpper-win,要是还不行那就重装python！ 问题描述： File “E:\PythonWorkspace\venv\ugc.venv\Scripts\pip.exe”, line 1 SyntaxError: Non-ASCII character ‘\x90’ in file E:\PythonWorkspace\venv\ugc.venv\Scripts\pip.exe on line 1, but no encoding declared;解决方案：原因是pip安装python包会加载我的用户目录，我的用户目录恰好是中文的，ascii不能编码。解决办法是：python目录 Python27\Lib\site-packages 建一个文件sitecustomize.py,python会自动运行这个文件。 12import syssys.setdefaultencoding('gb2312') pybuilderpybuilder官网经常在java/c#/javascript之间切着敲代码，今年又多了python这个数据分析神器，习惯了Maven约定俗成的构建环境，为了实现单元测试打包一体化的高效，于是决定采用pybuidler进行工程构建 安装在venv环境安装：pip install pybuilder pybuilder项目目录结构src/main/python：源码src/main/scripts：可执行脚本src/main/unittest：单元测试 常用命令 进入venvShell：workon ugc.venv 执行默认build文件：pyb_.exe (参考官方文档执行pyb报错，暂未找到办法) 执行默认build文件，并打印unittest错误详情：pyb_.exe -v 新增测试项目：pyb_.exe --start-project 发布：pyb_.exe install_dependencies publish 问题记录 单元测试执行错误：BUILD FAILED - &#39;module&#39; object has no attribute &#39;FileUtil_test]]></content>
      <categories>
        <category>脚本工具</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB集群学习笔记]]></title>
    <url>%2F2015%2F12%2F29%2FMongoDB%E9%9B%86%E7%BE%A4%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[考虑部署实施的复杂度，一直没上MongoDB集群，但现在海量数据一来，单机性能就扛不住了，本文记录MongoDB集群的基础知识。 Mongodb 有三种集群方式的搭建： Replica Set ，Sharding 和 Master-Slaver 基本概念 Chunck（块）：一个区间的数据称为一个数据块,是一个逻辑概念，物理存储并不连续，默认64M,可通过启动时附加’–chunkSize N’参数设置块大小 Vertical Scaling（垂直扩展）：CPU/RAM/IO等硬件层扩展，有云端部署和硬件扩展的瓶颈 Sharding（水平分片）：逻辑上是一个数据库，但物理存储上分开独立存储 Balancing（平衡）：当存在多个可用的分片，且块的数量足够多，mongodb的balancer（平衡器）会把数据迁移到其他分片上 mongos：mongos是用户和群集间的交互点，其职责是隐藏分片内部的复杂性并向用户提供一个简洁的单服务器接口，mongos会将所有用户请求转发到恰当的分片上。 config server（配置服务器）：配置服务器包含了有关集群的最完整可靠的信息以供所有人（分片、mongos进程和系统管理员）访问。 集群的构造一个MongoDB集群基本由3类进程组成： shards（存储数据）, mongos(路由器）、 config servers（配置服务器） Shard Server即存储实际数据的分片每个Shard 可以是一个mongod实例也可以是一组mongod实例构成的Replica Set。为了实现每个Shard内部的auto-failover，MongoDB官方建议每个Shard为一组Replica Set。 Config Server为了将一个特定的collection 存储在多个shard 中需要为该collection指定一个shard key例如{age: 1} shard key 可以决定该条记录属于哪个chunk。Config Servers 就是用来存储所有shard 节点的配置信息、每个chunk 的shard key 范围、chunk 在各shard 的分布情况、该集群中所有DB 和collection 的sharding 配置信息。 Route Process路由客户端由此接入，然后询问Config Servers 需要到哪个Shard 上查询或保存记录再连接相应的Shard 进行操作，最后将结果返回给客户端。客户端只需要将原本发给mongod的查询或更新请求原封不动地发给Routing Process而不必关心所操作的记录存储在哪个Shard 上。 Replica Set（复制）定义复制是在多台服务器之间同步数据的过程。 容灾性由于在不同的数据库服务器上拥有多个数据镜像，复制可以有效的防止由于单台服务器故障而导致的数据丢失。复制还能够帮助我们从硬件故障或是服务中断中恢复数据。我们也可以通过增加复制节点来将其用于灾难恢复、报表或是备份。 读写分离在某些情况中，我们可以通过复制的方式来提高读的性能。客户端可以将读与写请求分别发送到不同的服务器上。我们还能够通过在其他数据中心建立分布式复制节点的方式来做异地冗灾，以进一步提高可用性。 Sharding（分片）shard官方QA 定义sharding（分片）是使用多个机器存储数据的方法,MongoDB使用分片以支持巨大的数据存储量与对数据操作.为了解决这些问题,有两个基本的方法: 纵向扩展 和 分片 .分片的目的：高数据量和吞吐量的数据库应用会对单机的性能造成较大压力,大的查询量会将单机的CPU耗尽,大的数据量对单机的存储压力较大,最终会耗尽系统的内存而将压力转移到磁盘IO上. 一个分片可由多台服务器组成（每台服务器都有一份分片数据的Replica set副本）根据片键（key）分片[a,b)，MongoDB会在不同分片区间移动数据子集MongoDB集群（分片）安装与配置方法图解 shard key(片键)shard key大小不能超过512 bytes.分片后shard key不可改变，除非重建collection 小基数片键小基数片键：片键值数量有限适用的键：应使用组合片键(一个片键包含2个字段），请确保第二个字段有足够的值供MongoDB用来进行分割 升序片键适用于任何升序排列的键值，而并不必须是时间戳，包括日期、自增主键。只要键值趋于无穷大，就会面临单一且不可分散的热点问题 随机片键初衷是为了避免热点，会选择一个随机值的字段来分片。数据量变大后会给RAM增加压力，且会引发大量的磁盘IO 好片键具有良好的数据局部性（data locality）特征，但又不会太局部而导致热点出现。准升序键+搜索键｛coarselyAscending:1，search:1｝，coarselyAscending用来控制数据局部化，search字段则是数据上常用的一个检索字段。coarselyAscending键的每个值最好能对应几十到几百个数据块，如月份（2015-12）search键则应当是应用程序通常都会依据其进行查询的字段，如用户信息、文件名称、或GUID等 Master-Slaver（主从）对于Mongodb来说，并不推荐使用Master-Slave架构，因为Master-Slave其中Master宕机后不能自动恢复，推荐使用Replica Set，除非Replica的节点数超过50，才需要使用Master-Slave架构，正常情况是不可能用那么多节点的。主从架构一般用于备份或者做读写分离。由两种角色构成： 主(Master)可读可写，当数据有修改的时候，会将oplog同步到所有连接的salve上去。 从(Slave)只读不可写，自动从Master同步数据。 mongodb集群监控在线：Mongodb Cloud Manager离线：MongoDB Management Service(MMS) mongodb gui toolsMongoVUE1.6.9 在Mongodb3.2.0版本不可用了，不得不寻找替代品官方推荐工具列表 亲测觉得不错的GUI工具 3T MongoChef 提供64位 mongobooster]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB3离线部署]]></title>
    <url>%2F2015%2F12%2F28%2FMongoDB3%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[基于Ops Manager的MongoDB 3.2集群离线部署笔记 MongoDB3带来的改变MongoDB 3.0新版重点主要在效能的提升以及可扩展性，这些改变来自于储存层（Storage Layer）的强化，MongoDB 2.8版本开始引入支持Latch-free、Non-blocking算法的WiredTiger储存引擎，因此可以开始使用新硬件才有的功能，比如大量的高速缓存（On-chip Cache）和多执行架构。MongoDB 3.0 还提供了企业Ops Manager管理工具，用来管理大规模的 MongoDB 架构。 个人测试 数据恢复（mongorestore）支持并行导入了，性能有所提升； 数据存储变成了*.wt后缀的，待确认具体更新； MongoDB的管理服务（MMS）是用于监控和备份MongoDB的基础设施服务。其中监控的服务是免费的，备份的服务是需要收费的。 Ops ManagerThe Best Way to Run MongoDB: Ops ManagerOps Manager官网 Ops Manager能做什么？ Deployment. Any topology, at any scaleManagement. Deploy new clusters. Manage, monitor, and back up existing onesUpgrades. In minutes, with no downtimeScaling. Add capacity, without taking the application offlinePoint-in-time, Scheduled Backups. Restore to any point in time, because disasters aren’t predictablePerformance Alerts. Monitor 100+ system metrics and get custom alerts before the system degradesAdd Query Optimization. Identify slow-running queries, get index suggestions, automate index builds 安装步骤服务器准备123456789#mms应用服务器192.168.1.80#mms监控数据服务器，Monitor Agent部署192.168.1.51192.168.1.52：shard192.168.1.54：config server192.168.1.58：mongos 域名解析配置HostName配置/FQDN配置FQDN是Fully Qualified Domain Name的缩写, 含义是完整的域名. 例如, 一台机器主机名(hostname)是www, 域后缀(domain)是example.com, 那么该主机的FQDN应该是www.example.com.注意：hosts配置不当，后面server和agent间通讯会存在问题，参考host配置如下： server/agent配置(master) 1234567891011121314# vim /etc/hosts# localhost127.0.0.1 localhost.localdomain localhost#mongodb ops manager server192.168.1.80 opsserver.lt.com opsserver#mongodb ops monitor agent192.168.1.51 opsagent1.lt.com opsagent1192.168.1.52 opsagent2.lt.com opsagent2192.168.1.53 opsagent3.lt.com opsagent3192.168.1.54 opsagent4.lt.com opsagent4192.168.1.58 opsagent8.lt.com opsagent8 network配置1234# vim /etc/sysconfig/networkNETWORKING=yesHOSTNAME=opsserverNTPSERVERARGS=iburst 安装Ops Manager Application Database可选：Backup Database 设置最大文件打开数12#vim /etc/rc.localulimit -n 65536 ## 安装Ops Manager集群监控 关于Ops Manager视频教程视频教程 介绍文档MongoDB Ops Manager is a service for managing, monitoring and backing up a MongoDB infrastructure. Ops Manager provides the services described here. 备注：目前Ops Manager只能完成从无到有的集群部署，sharding集群的shard key等配置需在xshell中手动配置 centos6安装Ops ManagerRPM安装参考教程中文配置教程 配置ops manager application data数据服务器（192.168.1.51），安装mongodb并启动服务从其他机器复制mongodb：scp -r root@192.168.1.52:/usr/local/mongodb /usr/local 安装ops manager server 复制到mongodb-mms-2.0.0.327-1.x86_64.rpm到/mnt 执行sudo rpm -ivh /mnt/mongodb-mms-2.0.0.327-1.x86_64.rpm，mms将默认安装到/opt/mongodb/mms/ 配置/opt/mongodb/mms/conf/conf-mms.properties,设置mongo.mongoUri等参数 启动服务sudo service mongodb-mms start 设置mongodb-mms为开机自启动：chkconfig mongodb-mms on 登录http://:8080/注册用户 agent节点分别配置安装automation agent 1234567891011121314151617181920212223#以下具体参数参考http://192.168.1.80:8080/settings/agents/中的Automation选项卡的操作步骤#Download the agentcurl -OL http://192.168.1.80:8080/download/agent/automation/mongodb-mms-automation-agent-manager-2.5.11.1484-1.x86_64.rpm#and install the package.sudo rpm -U mongodb-mms-automation-agent-manager-2.5.11.1484-1.x86_64.rpm#Open the config filesudo vi /etc/mongodb-mms/automation-agent.config#配置唯一 API key, Group ID, and Ops Manager Base URLmmsGroupId=568486a424ac2c591d35e00cmmsApiKey=e4db7d0b6757a704271d04e25748ac41mmsBaseUrl=http://192.168.1.80:8080#Prepare a directory in which to store your MongoDB data. This directory must be owned by the mongod user. Any directory is fine, but the default is /data. This directory can be created with a command similar to below.sudo mkdir /datasudo chown mongod:mongod /data#Start the agent.sudo service mongodb-mms-automation-agent start#设置mongodb-mms-automation-agent为开机自启动chkconfig mongodb-mms-automation-agent on 其他shard节点可从monitor节点复制automation-agent配置文件：scp root@192.168.1.51:/etc/mongodb-mms/automation-agent.config /etc/mongodb-mms/automation-agent.config重启automation-agent服务：sudo service mongodb-mms-automation-agent restart 在web控制台Deployment中选1台性能较好的服务器配置monitoring agent（如在ops manager application data数据服务器） zai Deployment模块Add New Cluster,配置mongos,config server和shards 集群测试服务器主机名regex过滤:(opsagent2|opsagent4|opsagent8)集群服务器配置:mongos:1/mongod:3/config server:1端口配置，shards(27000-28000),mongos(27017)config server(27019)分片数配置配置完成确认执行后，ops会按顺序自动安装部署shard&gt;config server&gt;mongos，再也不用kb手动去安装了！mongodb安装的默认位置：/var/lib/mongodb-mms-automation windows server安装Ops Manager参考教程 安装mongodb 安装Install Ops Manager 配置C:\MMSData\Server\Config\conf-mms.properties（默认路径）中的mongo.mongoUri等连接配置属性 在windows服务列表启动服务MongoDB Ops Manager HTTP Service 登录http://:8080/注册用户，admin/1qaz@WSX 问题记录 No MongoDB versions have been made available for use in your deployment. Visit the Version Manager to enable MongoDB versions.重新配置，将version manager参数设置为internet而非local There are a mix of Agents that are installed manually and managed by your Automation Agents. This configuration is not supported.You will not be able to add new hosts to monitoring or create new automated hosts while in this state. The Agents that should be removed are indicated on the table above.After stopping an Agent it can take up to 5 minutes to take effect. 添加集群报错： Unable to create member 2 of myShard_0. Unable to find enough servers. Requested port range was 27000 to 27000. Excluding port ranges 27019 to 27019, 27017 to 27017. Please ensure all agents are running.shard server port设置在27000-28000范围内，不能设置为27000 mongos服务无法启动错误日志：not master or secondary; cannot currently read from this replSet member ns,config.settings query,config.shards query解决方案：pkill mongod 强制关闭mongo进程，删除/data目录内的shard数据(不能删automation的数据)，重新添加集群 shard集群分片测试，目前数据量小，数据存储表现为replica set，待测试分片效果！ MongoDB离线部署配置Configure Local Mode if Servers Have No Internet Access 设置为Local在Ops Manager控制台，单击右上角Admin，General&gt;Ops Manager Config&gt; Miscellaneous&gt;设置Version Manifest Source 为Local设置 Versions Directory存储MongoDB binaries，默认/opt/mongodb/mms/mongodb-releases 下载MongoDB离线Binariescd /opt/mongodb/mms/mongodb-releasescurl -OL http://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-3.2.0.tgz 设置Versions Directory目录权限 查看目录权限：ls -l /opt/mongodb/mms/mongodb-releases 设置mongodb-mms用户对Versions Directory中文件有读写权限：sudo chown -R mongodb-mms:mongodb-mms /opt/mongodb/mms/mongodb-releases 编辑conf-mms.properties在conf-mms.propertie最后新增两行123#vim /opt/mongodb/mms/conf/conf-mms.propertiesautomation.versions.source=localautomation.versions.directory=/opt/mongodb/mms/mongodb-releases/ 重启服务sudo service mongodb-mms restart 部署截图 Version Manager配置MongoDB版本在Ops Manager控制台，Deployment&gt;Version Manager&gt;选择离线的mongodb版本&gt;Review &amp; Deploy&gt;Confirm &amp; Deploy. MongoDB 数据备份与恢复backup-small-sharded-cluster-with-mongodump在mongodb primary节点执行mongorestore123cd /var/lib/mongodb-mms-automation/mongodb-linux-x86_64-3.2.0/binmongorestore -drop -d uadb /mnt/dump/dump/uadb_dump/uadbmongorestore -drop -d uadb_attachment /mnt/dump/dump/uadb_attachment_dump/uadb_attachment Route Server配置chunk size参考文档：modify-chunk-size-in-sharded-cluster 注意事项 Automatic splitting only occurs on insert or update. If you lower the chunk size, it may take time for all chunks to split to the new size. Splits cannot be undone. If you increase the chunk size, existing chunks grow only through insertion or updates until they reach the new size. The allowed range of the chunk size is between 1 and 1024 megabytes, inclusive. 修改步骤1234567#连接Route Server(mongos shell)/var/lib/mongodb-mms-automation/mongodb-linux-x86_64-3.2.0/bin/mongo opsagent2.lt.com:27017 #切换到config数据库 use config #修改chunk size db.settings.save( &#123; _id:"chunksize", value: &lt;sizeInMB&gt; &#125; ) 如 db.settings.save( &#123; _id:"chunksize", value: 64 &#125; ) 数据库启用分片：enableSharding参考文档：deploy-shard-cluster123456789101112131415161718#连接以mingo连接route server（mongo shell）mongo --host &lt;hostname of machine running mongos&gt; --port &lt;port mongos listens on&gt;如/var/lib/mongodb-mms-automation/mongodb-linux-x86_64-3.2.0/bin/mongo --host opsagent2.lt.com --port 27017#启用分片sh.enableSharding("&lt;database&gt;") 或者 db.runCommand( &#123; enableSharding: &lt;database&gt; &#125; )如sh.enableSharding("uadb")sh.enableSharding("uadb_attachment")或use admindb.runCommand( &#123; enableSharding: "uadb"&#125; );db.runCommand( &#123; enableSharding: "uadb_attachment"&#125; );# collection分片sh.shardCollection("&lt;database&gt;.&lt;collection&gt;", shard-key-pattern)如 sh.shardCollection("uadb.AddressNode", &#123; "_id": 1, "ruleabbr": 1 &#125; )#查看是否成功启用分片use uadb;db.AddressNode.stats(); shard集群测试Sharding 分片分片测试123456789101112#mongo连接admin库/var/lib/mongodb-mms-automation/mongodb-linux-x86_64-3.2.0/bin/mongo admin --host opsagent2.lt.com --port 27017#设置分片存储数据库sh.enableSharding("test")sh.shardCollection('test.users', &#123; "_id": 1&#125; )#插入测试数据use testfor(var i=1;i&lt;50000;i++) db.users.insert(&#123;age:i,name:'geosmart',address:'anhui_chuzhou',country:'china'&#125;)#查询分片状态db.users.stats();]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB权限配置]]></title>
    <url>%2F2015%2F12%2F28%2FMongoDB%E6%9D%83%E9%99%90%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[在未开启auth模式下新建sa用户//进入admin数据库mongo admin//新建sa超级用户12345678910111213141516171819202122db.createUser( &#123; user: "sa", pwd: "1qaz2wsx", roles: [ &#123; db: "uadb", role: "readWrite" &#125;, &#123; db: "uadb", role: "dbAdmin" &#125;, &#123; db: "uadb", role: "userAdmin" &#125;, &#123; db: "uadb", role: "dbOwner" &#125;, &#123; db: "uadb_attachment", role: "readWrite" &#125;, &#123; db: "uadb_attachment", role: "dbAdmin" &#125;, &#123; db: "uadb_attachment", role: "userAdmin" &#125;, &#123; db: "uadb_attachment", role: "dbOwner" &#125;, &#123; db: "admin", role: "readWrite" &#125;, &#123; db: "admin", role: "dbAdmin" &#125;, &#123; db: "admin", role: "userAdmin" &#125;, &#123; db: "admin", role: "dbOwner" &#125;, &#123; db: "admin", role: "root" &#125; ] &#125;) //sa用户授权测试db.auth(“sa”,”1qaz2wsx”) 启用MongoDB权限控制Windows卸载现有MongoDB服务C:\WINDOWS\system32&gt;sc delete “MongoDB” 启动服务按照MongoDB服务（设置权限控制）：E:\mongodb\bin\mongod –logpath “E:\mongodb\log\mongo.log” –logappend –dbpath “E:\mongodb\data” –directoryperdb –auth –serviceName “MongoDB” –serviceDisplayName “MongoDB” –install Linux启动服务/mnt/data/mongodb/bin/mongod –dbpath /mnt/data/mongodb/data –logpath /mnt/data/mongodb/log/mongodb.log –auth 开启auth后新建用户1234567891011121314//以admin登陆获取权限use admin//sa用户授权db.auth("sa","1qaz2wsx")//切换到uadb库新建用户uadbuse uadbdb.auth("uadb","1c63129ae9db9c60c3e8aa94d3e00495")db.dropUser("uadb");db.createUser( &#123;user: "uadb", pwd: "1c63129ae9db9c60c3e8aa94d3e00495", roles: [ "readWrite" , "dbAdmin" ] &#125;)//切换到uadb库新建用户uadb_attachmentuse uadb_attachmentdb.dropUser("uadb_attachment");db.createUser( &#123;user: "uadb_attachment", pwd: "1c63129ae9db9c60c3e8aa94d3e00495", roles: [ "readWrite" , "dbAdmin" ] &#125;)]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB空间分析]]></title>
    <url>%2F2015%2F12%2F28%2FMongoDB%E7%A9%BA%E9%97%B4%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[用MongoDB与其他NoSQL数据库之间一个大的差别就是她的空间数据存储，2dsphere空间索引（WGS84），用于通用的空间分析（如缓冲区）会很方便。 新建空间索引连接到数据库：/mnt/data/mongodb/bin/mongo uadb新建空间索引：db.AddressNode.ensureIndex( { 空间位置 : &quot;2dsphere&quot; });新建空间联合索引：db.AddressNode.ensureIndex( { 空间位置 : &quot;2dsphere&quot;, 规范地址节简称: 1 }); 几何查询MongoDB查询关键词：http://docs.mongodb.org/manual/reference/operator/query-geospatial/ geoWithin多边形范围查询123456789101112131415161718192021222324&#123;空间位置: &#123; $geoWithin: &#123; $geometry: &#123; "type":"Polygon", "coordinates":[[ [110,30], [110, 60], [120, 60], [120, 30], [110,30] ]] &#125; &#125; &#125;&#125;&#123;空间位置:&#123; $geoWithin:&#123; $geometry: &#123;type:'Polygon',coordinates:[[[110,30],[110, 60],[120, 60],[120, 30],[110,30]]]&#125;&#125;&#125;, 规范地址节简称:&#123;$in:["DIS","POI"] &#125;&#125; geoWithin圆范围查询（距离单位：弧度）12345678910111213141516&#123;空间位置:&#123; $geoWithin: &#123; $centerSphere: [[119.22426261, 31.61467114],0.0025] &#125; &#125;&#125;联合索引查询&#123;空间位置:&#123; $geoWithin: &#123; $centerSphere: [[119.22426261, 31.61467114],0.0025] &#125; &#125;, "规范地址节简称":"POI"&#125; 矩形范围查询$box，针对2d索引，不能针对GeoJson数据进行查询 nearSphere缓冲区范围查询（距离单位：米）1234567891011&#123;空间位置: &#123; $nearSphere: &#123; $geometry: &#123; "type":"Point", "coordinates":[119.22426261, 31.61467114] &#125;, $maxDistance : 5000 &#125; &#125;&#125; intersect相交查询12345678910111213141516&#123;空间位置: &#123; $geoIntersects: &#123; $geometry: &#123; "type":"Polygon", "coordinates":[[ [110,30], [110, 60], [120, 60], [120, 30], [110,30] ]] &#125; &#125; &#125;&#125; 图解 MongoDB 地理位置索引的实现原理： http://blog.nosqlfan.com/html/1811.html结合MongoDB开发LBS应用：http://www.infoq.com/cn/articles/depth-study-of-Symfony2http://docs.mongodb.org/manual/applications/geospatial-indexes/ MongoDB地理位置索引MongoDB地理位置索引常用的有两种 * 2d 平面坐标索引，适用于基于平面的坐标计算。也支持球面距离计算，不过官方推荐使用2dsphere索引。 * 2dsphere 几何球体索引，适用于球面几何运算 * 2d空间索引也支持Polygon+属性查询，但在组合索引/查询中为串行过程（低效），而2dsphere空间索引支持高效的组合索引/查询（即真正的GIS查询） 查询方式分三种情况： 1. Inclusion。范围查询，如百度地图“视野内搜索”。 2. Inetersection。交集查询。不常用。 3. Proximity。周边查询，如“附近500内的餐厅”。 MongoDB查询地理位置默认有3种距离单位： * 米(meters) * 平面单位(flat units，可以理解为经纬度的“一度”) * 弧度(radians)。 通过GeoJSON格式查询，单位默认是米，通过其它方式则比较混乱geoWithin的查询范围：经/纬度范围之和不能大于18012 double[][] geometry = GeoUtil. createRectangle(-20, 90, 160, -90);&#123;空间位置: &#123;$geoWithin: &#123;$geometry: &#123;type:'Polygon',coordinates:[[[-20.0,90.0],[-20.0,-90.0],[160.0,-90.0],[160.0,90.0],[-20.0,90.0]]]&#125;&#125;&#125;,规范地址节简称:'POI',空间优先级:1&#125;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB常用脚本]]></title>
    <url>%2F2015%2F12%2F28%2FMongoDB%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[记录一些工作过程中常用的MongoDB脚本。 数据库连接与关闭数据库连接mongo uadb 切换数据库use uadb 强制关闭mongodb进程pkill mongod 查询语句模糊查询查询条件中包含like时，格式为：{&quot;地址节全称&quot;:new RegExp(&quot;.*花园&quot;)} 操作关键词1234&gt;, &gt;=, &lt;, &lt;=, !=, =$gt, $gte, $lt, $lte,$ne,And，OR，In，NotIn无关键字, $or, $in，$nin 更新语句MongoDB更新字段名，如将AddressNode的adalias字段改为adtext：db.AddressNode.update({}, {$rename:{&quot;adalias&quot;:&quot;adtext&quot;}}, false, true); 数据备份恢复123456789101112131415161718192021222324252627#数据备份mongodump -d uadb -u uadb -p psd -o /usr/local/mongodb/dump#数据还原mongorestore -drop -d uadb -u uadb -p psd /usr/local/mongodb/dump/uadb``` # 删除数据库db.dropDatabase();# maxConns并发连接数设置备注：V3.0版本以上参数为maxIncomingConnections，默认65536，详见[V3.2官方configuration-options文档](https://docs.mongodb.org/v3.2/reference/configuration-options/)## 查询并发数db.serverStatus().connections## ulimit 设置可以打开最大文件描述符的数量。查看最大文件打开数：ulimite -n临时生效：`ulimit -n 32768`永久生效：`vim /etc/rc.local` 新增`ulimit -n 32768`## 重启mongodb服务，带上--maxConns参数`/usr/local/mongodb/bin/mongod --dbpath /usr/local/mongodb/data --logpath /usr/local/mongodb/log/mongodb.log --maxConns=20000 --fork --smallfiles`# MongoDB全局变量设置```yaml# vim /etc/profileexport MONGODB_HOME=/usr/local/mongodbexport PATH=$MONGODB_HOME/bin:$PATH]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS配置本地Yum源]]></title>
    <url>%2F2015%2F12%2F07%2FCentOS%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0Yum%E6%BA%90%2F</url>
    <content type="text"><![CDATA[现场环境没有网络，有些软件安装简直太痛苦，和maven的依赖链一样，最终耗时耗力不一定能安装好，此时制作本地yum只读光盘就是一个好主意，此文主要介绍如何配置本地yum。 建立本地源目录mkdir /mnt/cdrom 挂载CentOS光盘mount /dev/cdrom /mnt/cdrom 备份repo进入/etc/yum.repos.d目录，可以看到四个文件分别为CentOS-Base.repo、 CentOS-Media.repo 、CentOS-Vault.repo、CentOS-Vault.repo.repo,将其中三个改名或者移走留下CentOS-Media.repo12345cd /etc/yum.repos.dmv CentOS-Base.repo CentOS-Base.repo.bakmv CentOS-Vault.repo CentOS-Vault.repo.bakmv CentOS-Vault.repo CentOS-Vault.repo.bakcp CentOS-Media.repo CentOS-Vault.Media.bak 编辑CentOS-Media.repo编辑CentOS-Media.repo：vi CentOS-Media.repo将以下内容12345678[c6-media]name=CentOS-$releasever - Mediabaseurl=file:///media/CentOS/ file:///media/cdrom/ file:///media/cdrecorder/gpgcheck=1enabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 修改为12345678[c6-media]name=CentOS-$releasever - Mediabaseurl=file:///mnt/cdrom/ #这里为本地源路径 file:///media/cdrom/ file:///media/cdrecorder/gpgcheck=1enabled=1 ##开启本地源gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 修改好保存并退出 清yum缓存yum clean 备注如需要将yum源改为网络，还原/etc/yum.repos.d目录下的四个文件即可！]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Samba安装配置笔记]]></title>
    <url>%2F2015%2F12%2F07%2FSamba%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[一个局域网的项目，需要在Linux上进行文件共享，最终选型samba，即通过jcifs实现java读写共享文件。 关于SMBSMB（Server Message Block）:通信协议是微软（Microsoft）和英特尔(Intel)在1987年制定的协议，主要是作为Microsoft网络的通讯协议。SMB 是在会话层（session layer）和表示层（presentation layer）以及小部分应用层（application layer）的协议。SMB使用了NetBIOS的应用程序接口 （Application Program Interface，简称API）。另外，它是一个开放性的协议，允许了协议扩展——使得它变得更大而且复杂；大约有65个最上层的作业，而每个作业都超过120个函数，甚至Windows NT也没有全部支持到，最近微软又把 SMB 改名为 CIFS（Common Internet ile System），并且加入了许多新的特色。 SMB协议是基于TCP－NETBIOS下的，一般端口使用为139，445 关于CIFSCIFS(Common Internet File System)：通用Internet文件系统在windows主机之间进行网络文件共享是通过使用微软公司自己的CIFS服务实现的。CIFS 是一个新提出的协议，它使程序可以访问远程Internet计算机上的文件并要求此计算机的服务。CIFS 使用客户/服务器模式。客户程序请求远在服务器上的服务器程序为它提供服务。服务器获得请求并返回响应。CIFS是公共的或开放的SMB协议版本，并由Microsoft使用。SMB协议现在是局域网上用于服务器文件访问和打印的协议。象SMB协议一样，CIFS在高层运行，而不象TCP/IP协议那样运行在底层。CIFS可以看做是应用程序协议如文件传输协议和超文本传输协议的一个实现。 关于JCIFSJCIFS是CIFS 在JAVA中的一个实现，是samba组织负责维护开发的一个开源项目，专注于使用java语言对cifs协议的设计和实现。他们将jcifs设计成为一个完整的，丰富的，具有可扩展能力且线程安全的客户端库。这一库可以应用于各种java虚拟机访问遵循CIFS/SMB网络传输协议的网络资源。类似于java.io.File的接口形式，在多线程的工作方式下被证明是有效而容易使用的。 关于SambaSamba，是种用来让UNIX系列的操作系统与微软Windows操作系统的SMB/CIFS（Server Message Block/Common Internet File System）网络协议做链接的自由软件。第三版不仅可访问及分享SMB的文件夹及打印机，本身还可以集成入Windows Server的域，扮演为域控制站（Domain Controller）以及加入Active Directory成员。简而言之，此软件在Windows与UNIX系列OS之间搭起一座桥梁，让两者的资源可互通有无。 CentOS下yum安装配置Samba新建用户sambauseradd samba 设置samba用户密码passwd samba 将samba加入到Samba用户数据库smbpasswd -a samba windows访问samba共享目录时需要输入此用户名和密码 设置目录访问权限chown -R samba:samba /uadb/exchange/importchown -R samba:samba /uadb/exchange/exportchown -R samba:samba /uadb/exchange/export_backup 编辑smb.conf配置文件12345678910111213141516171819#vim/etc/samba/smb.conf[global]workgroup = WORKGROUPnetbios name = sambaServerserver string = Linux Samba Server TestServersecurity = user[import]path = /uadb/exchange/importwriteable = yesbrowseable = yesguest ok = yes[export]path = /uadb/exchange/exportwriteable = yesbrowseable = yesguest ok = yes 设置samba服务开机启动chkconfig smb on 重启服samba服务service smb restart 问题记录连接路径问题 问题描述：cifs.smb.SmbException: The network name cannot be found. 问题定位：不能用绝对路径来访问，需要用samba的配置的共享文件夹名称来访问 解决方案：正确：smb://samba:samba@192.168.1.80/import；错误：smb://samba:samba@192.168.1.80/uadb/exchange/import Samba读写示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/ * jcifs的开发方法类似java的文件操作功能，它的资源url定位：smb://&#123;user&#125;:&#123;password&#125;@&#123;host&#125;/&#123;path&#125;， * smb为协议名，user和password分别为共享文件机子的登陆名和密码，@后面是要访问的资源的主机名或IP地址。最后是资源的共享文件夹名称和共享资源名。 * 例如smb://administrator:122122@192.168.0.22/test/response.txt。 * * SmbFile file = newSmbFile("smb://guest:1234@192.168.3.56/share/a.txt"); */@Testtruepublic void readFile() &#123;truetruetry &#123;truetruetrue// 局域网共享文件，读文件truetruetrueSmbFile smbFile = new SmbFile("smb://samba:samba@192.168.1.80/import/test.db");truetruetrue// 通过 smbFile.isDirectory();isFile()可以判断smbFile是文件还是文件夹truetruetrueint length = smbFile.getContentLength();truetruetruebyte buffer[] = new byte[length];truetruetrueSmbFileInputStream in = new SmbFileInputStream(smbFile);truetruetruewhile ((in.read(buffer)) != -1) &#123;truetruetruetrueSystem.out.write(buffer);truetruetruetrueSystem.out.println("\n" + buffer.length);truetruetrue&#125;truetruetruein.close();truetruetruesmbFile.delete();truetrue&#125; catch (SmbAuthException e) &#123;truetruetruee.printStackTrace();truetrue&#125; catch (IOException e) &#123;truetruetruee.printStackTrace();truetrue&#125;true&#125;true@Testtruepublic void wariteFile() &#123;truetruetry &#123;truetruetrueSmbFile smbFileOut = new SmbFile("smb://samba:samba@192.168.1.80/import/test2.db");truetruetrueif (!smbFileOut.exists())truetruetruetruesmbFileOut.createNewFile();truetruetrueSmbFileOutputStream out = new SmbFileOutputStream(smbFileOut);truetruetrueout.write("abcdefw".getBytes());truetruetrueout.close();truetruetruesmbFileOut.delete();truetrue&#125; catch (SmbAuthException e) &#123;truetruetruee.printStackTrace();truetrue&#125; catch (IOException e) &#123;truetruetruee.printStackTrace();truetrue&#125;true&#125;]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>Samba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive学习笔记]]></title>
    <url>%2F2015%2F12%2F06%2FHive%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[最近处理一个ETL的项目，技术选型是CDH的Hadoop方案，理所当然离不了Hive数据仓库，记录下Hive学习路上的点滴。 Hive简介Apache Hive是一个建立在Hadoop架构之上的数据仓库。它能够提供数据的精炼，查询和分析。Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL）。Hive定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。 Hive Maven库Hive1.1.0离线包 参考资料Hive 官方Wiki hive Maven库有时候中央库的没法下载，但是spring.io提供的CDH的可以。123456789101112&lt;!-- cdh repository--&gt;&lt;repository&gt; &lt;id&gt;cdh-5.3.0&lt;/id&gt; &lt;url&gt;http://repo.spring.io/libs-release-remote/&lt;/url&gt;&lt;/repository&gt;&lt;!-- hive jdbc --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;&lt;/dependency&gt; Hive With HbaseHive存储Hbase数据 测试语句参考资料hbase-via-hive1hbase-via-hive2 示例SQL1234CREATE TABLE User (userId STRING, address STRING,name STRING ,photo STRING ,psd STRING)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES ('hbase.columns.mapping' = ':key,f:data')TBLPROPERTIES ('hbase.table.name' = 'User'); hive 新建表1234567CREATE EXTERNAL TABLE Geocoding_Address (SID String,SAddress String)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\t'STORED AS TEXTFILE;--PARTITIONED BY(STR STRING) hive新增partionalter table alter2 add partition (insertdate=’2008-01-01’) location ‘2008/01/01’; Hive数据导入导入hdfs数据到hive表load data inpath &#39;/user/uadb/test.txt&#39; into table test ; 导入本地文件到hive表load data local inpath &#39;/home/uadb/test.txt&#39; into table test ; Hive自定义函数 UDF:一进一出（ 输入一行输出一行 On-to-one maping ） transformation of one row value into another one, which can be added with UDFs (User Defined Function); UDAF:多进一出（ 输入多行输出一行 Many-to-one maping ） transformation of multiple row values into one, which can be added with UDAFs (User Defined Aggregate Functions); UDTF:一进多出（ 输入一行输出多行 On-to-many maping ） transformation of one row value into many, which can be added with UDTFs (User Defined Table Functions). 查看UDF依赖的jar包查看自定义函数依赖的jar包：list jars; hue导入/删除jar12add jar /user/hive/test/test.jar;delete jar /user/hive/test/test.jar; 新建临时UDF函数12create temporary function testUDF as "com.lt.uadb.match.udf.SkeletonAddressNodeMapUDF";select a.skeleton_addressnode,testUDF(a.skeleton_addressnode,'一') from matchingAddress as a Hive UUIDselect reflect(“java.util.UUID”, “randomUUID”) from table UDF程序打包UDF程序打包有两张方式： 以类fatjar工具将UDF和依赖打成一个jar包，但是打包部署耗时； 将jar包分为稳定和经常更新的两类；通过执行add和delete动态添加依赖 CM中设置Hive自动加载UDTF依赖JAR参考cloudera mamager中配置hive加载自定义的jar包 进入Hive配置页 在高级选型中设置Hive 辅助 JAR 目录：/etc/hive/udtflib 设置Gateway Default Group（hive-env.sh 的 Gateway 客户端环境高级配置代码段（安全阀））：HIVE_AUX_JARS_PATH=/etc/hive/udtflib 重启集群，CM会自动将Hive辅助JAR目录中的jar包分发到Hive客户端 UDF日志查看除了开发环境的Junit单元测试外，生产环境的日志查看非常重要， 通过在hue -jobbrowser中查看syslog； 通过在YARN的ResourceManager UI中查看Mapreduce打印的详细日志，日志会打印syso的内容； Hive JDBCHiveServer和HiveServer2都有两种模式，分别为嵌入式和单机服务器模式， 嵌入式URI为”jdbc:hive://“或者”jdbc:hive2://“； 单机服务器模式的URI为”jdbc:hive://host:port/dbname”或者”jdbc:hive2://host:port/dbname”； HiveServer使用的JDBC驱动类为org.apache.hadoop.hive.jdbc.HiveDriver，HiveServer2使用的驱动类为org.apache.hive.jdbc.HiveDriver； 问题记录/tmp/hive on HDFS should be writable问题日志：Exception in thread “main” java.lang.RuntimeException: java.lang.RuntimeException: The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rwx-wx–x解决方法： 更新权限hdfs目录权限：hadoop fs -chmod 777 /tmp/hive hdfs执行：hadoop fs -rm -r /tmp/hive; local执行：rm -rf /tmp/hive hive query can’t generate result set via jdbc解决：Use stmt.execute() for a query that makes a new table. of executeQuery. The executeQuery() is now only for select queries (DML) while execute is probably for DDL (data definition). DDL（Data Definition Language 数据定义语言）用于操作对象和对象的属性，这种对象包括数据库本身，以及数据库对象，像：表、视图等等，DDL对这些对象和属性的管理和定义具体表现在Create、Drop和Alter上； DML（Data Manipulation Language 数据操控语言）用于操作数据库对象中包含的数据，也就是说操作的单位是记录； Hive Jdbc调用UDTF问题 问题描述：在Java中以Hive的JDBC接口调用UDTF语句，逐行执行到create temporary function就会报错，但在Hue中（客户端连接）能正常执行 问题日志 12org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.FunctionTask at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315) 解决方案：]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hive</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyEclipse安装MapReduceTools插件]]></title>
    <url>%2F2015%2F11%2F20%2FMyEclipse%E5%AE%89%E8%A3%85MapReduceTools%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Windwos10开发环境测试MapReduce程序，需自行编译hadoop2x-eclipse-plugin，生成MyEclipse2014的MapReduceTolls插件，可结合MRUnit进行单元测试。 参考中文教程具体配置步骤如下： 安装MyEclipse2014Myeclipse安装位置：C:\Dev\myeclipse（路径无中文字段/空格） 下载Hadoop lib下载hadoop-2.6.0.tar.gz并解压到目录（路径无中文字段/空格）hadoop-2.6.0.tar.gz下载地址解压后置于F:\Dev\hadoop\hadoop-2.6.0 配置Antant-1.9.0下载地址配置环境变量 12ANT_HOME=F:\Dev\apache-ant-1.9.0PATH 后追加 ;%ANT_HOME%\bin 检验Ant配置：ant -version 编译hadoop-eclipse-plugin插件hadoop-eclipse-plugin下载地址解压后置于F:\Dev\hadoop\hadoop2x-eclipse-plugin-master打开cmd执行以下脚本编译hadoop-eclipse-plugin插件 12cd F:\Dev\hadoop\hadoop2x-eclipse-plugin-master\src\contrib\eclipse-pluginant jar -Dversion=2.6.0 -Declipse.home=C:\Dev\myeclipse -Dhadoop.home=F:\Dev\hadoop\hadoop-2.6.0 执行成功后生成的jar位置：F:\Dev\hadoop\hadoop2x-eclipse-plugin-master\build\contrib\eclipse-plugin\hadoop-eclipse-plugin-2.6.0.jar 配置hadoop-eclipse-plugin-2.6.0.jar 将hadoop-eclipse-plugin-2.6.0.jar剪切到C:\Dev\myeclipse\dropins目录； 重启Myeclipse即完成MyEclipse2014的MapReduceTolls插件； 可在Window&gt;Show View&gt;MapReduce Tools打开插件视图。 配置hadoop locationLocation name ：随便取个名字 比如 hadoop2.6.0Map/Reduce（V2） Master ：根据hdfs-site.xml中配置dfs.datanode.ipc.address的值填写，50020DFS Master： Name Node的IP和端口，根据core-site.xml中配置fs.defaultFS的值填写，8020 CDH配置文件位置：/etc/hadoop/conf.cloudera.yarn]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>分布式</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH使用问题记录]]></title>
    <url>%2F2015%2F10%2F27%2FCDH%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[如何制作CDH Agent 虚拟机模板 问题描述：CDH Manager安装配置好一台Agent机器A（VM虚拟机）后，如果以A复制出B，在集群中B与A会冲突，每次Host Inspector只能检测到一个 问题定位：判断为SCM库中HOSTS表的HOST_IDENTIFIER字段冲突导致 解决思路：ClouderaManager是根据什么自动生成HOST_IDENTIFIER的？ 如何复制VM虚拟机才能不冲突？ 问题解决：由于在复制cm-5.4.7到agent之前启动了cloudera-scm-agent，在/opt/cm-5.4.7/lib/cloudera-scm-agent中生成response.avro和uuid两个文件，cloudera的HOST_IDENTIFIER读取的就是uuid文件的文本，停止agent&gt;删除response.avro和uuid&gt;启动agent，问题解决 Cloudera Manager无法删除某项服务删除依赖关系或在命令中查看正在执行的（卡死）的命令，中止即可 Key-Value Store Indexer服务总是异常终止 问题描述：重新构建的Solr Collection和Index，数据写入少量没问题，程序批量写入时（&gt;250条）服务就自动终止 问题定位：Java OOM虚拟机内存溢出问题 解决方式：hbase-indexer github-issues：Lily Hbase Indexers always auto exit，通过向hbase-indexer官方github提交issue寻求帮助，确认是OOM问题！ 运行hbase-indexer server在单个hbase-server服务器调试运行（不在cloudera管理下运行），不会发生OOM CM集中修改参数Lily HBase Indexer的Java堆栈大小，默认设置的是131M，改为1GB后重新启动服务，往Hbase写入数据时，SOlr索引生成正常，Hbase-Indexer未自动退出。 其他参考资料：LocalMorphlineResultToSolrMapper源码 创建 Hive Metastore 数据库表失败问题日志： 12345678910111213141516171819202122232425262728++ exec /opt/cloudera/parcels/CDH-5.4.7-1.cdh5.4.7.p0.3/lib/hadoop/bin/hadoop jar /opt/cloudera/parcels/CDH-5.4.7-1.cdh5.4.7.p0.3/lib/hive/lib/hive-cli-1.1.0-cdh5.4.7.jar org.apache.hive.beeline.HiveSchemaTool -verbose -dbType mysql -initSchemaorg.apache.hadoop.hive.metastore.HiveMetaException: Failed to load driverorg.apache.hadoop.hive.metastore.HiveMetaException: Failed to load drivertrueat org.apache.hive.beeline.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:79)trueat org.apache.hive.beeline.HiveSchemaTool.getConnectionToMetastore(HiveSchemaTool.java:113)trueat org.apache.hive.beeline.HiveSchemaTool.testConnectionToMetastore(HiveSchemaTool.java:159)trueat org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:257)trueat org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:243)trueat org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:473)trueat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)trueat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)trueat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)trueat java.lang.reflect.Method.invoke(Method.java:606)trueat org.apache.hadoop.util.RunJar.run(RunJar.java:221)trueat org.apache.hadoop.util.RunJar.main(RunJar.java:136)Caused by: java.lang.ClassNotFoundException: com.mysql.jdbc.Drivertrueat java.net.URLClassLoader$1.run(URLClassLoader.java:366)trueat java.net.URLClassLoader$1.run(URLClassLoader.java:355)trueat java.security.AccessController.doPrivileged(Native Method)trueat java.net.URLClassLoader.findClass(URLClassLoader.java:354)trueat java.lang.ClassLoader.loadClass(ClassLoader.java:425)trueat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)trueat java.lang.ClassLoader.loadClass(ClassLoader.java:358)trueat java.lang.Class.forName0(Native Method)trueat java.lang.Class.forName(Class.java:195)trueat org.apache.hive.beeline.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:70)true... 11 moretrue*** schemaTool failed *** CDH5使用端口 CDH 5 组件使用的端口 Hbase运行中止问题 错误日志 123456//cat /var/log/hbase/hbase-cmf-hbase-REGIONSERVER-slave3.lt.com.log.outError syncing, request close of waljava.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "slave3.lt.com/192.168.1.103"; destination host is: "server1.lt.com":8020; trueat com.sun.proxy.$Proxy20.updateBlockForPipeline(Unknown Source)Caused by: java.io.IOException: Connection reset by peer 1Unable to reconnect to ZooKeeper service, session 0x1508e4d86eb8047 has expired, closing socket connection 123Client session timed out, have not heard from server in 40003ms for sessionid 0x1508e4d86eb88ef, closing socket connection and attempting reconnectUnable to reconnect to ZooKeeper service, session 0x1508e4d86eb88f5 has expired, closing socket connection 1Opening socket connection to server server1.lt.com/192.168.1.91:2181. Will not attempt to authenticate using SASL (unknown error) 参数说明hbase参数 1234hbase.regionserver.lease.period默认值：60000说明：客户端租用HRegion server 期限，即超时阀值。调优：这个配合hbase.client.scanner.caching使用，如果内存够大，但是取出较多数据后计算过程较长，可能超过这个阈值，适当可设置较长的响应时间以防被认为宕机 zookeeper参数1234zookeeper.session.timeout默认值：60000说明：ZooKeeper 服务器允许客户端协商的最大会话超时时间（以毫秒为单位）调优：zookeeper的超时时间不要设置太大，在服务挂掉的情况下，会反映很慢。 解决方式在CM的Hbase配置hbase.regionserver.lease.period默认值改为4（分钟）在CM的Hbase配置hbase.rpc.timeout默认值改为5（分钟） Hbase数据操作失败问题描述：connection to slave3.lt.com/192.168.1.103:60020 from geosmart: closing ipc connection to slave3.lt.com/192.168.1.103:60020: Connection refused: no further informationjava.net.ConnectException: Connection refused: no further information Hive UDTF问题问题日志：Error while compliling statement:Failed IndexOutOfBoundException Index.1 Size.1解决方案：init初始化中的ArrayList 和fieldNames个数和类型要对应一致 Hive UDTF code 2问题问题日志：Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask解决方案：/var/log/hive中查看错误日志定位错误 HUE新建HDFS目录问题描述: you are a Hue admin but not a HDFS superuser, “hdfs” or part of HDFS supergroup, “supergroup”解决方案：在hue中新增hdfs用户，以hdfs用户登录创建目录和上传文件 Hive Metastore canary 创建数据库失败问题描述： Hive Metastore canary 创建数据库失败日志： 12Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error :The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver 问题定位：读取hive数据时报找不到mysql驱动问题解决： * 尝试1 /opt/cloudera/parcels/CDH-5.4.7-1.cdh5.4.7.p0.3/lib/hive/libcp /opt/cloudera/parcels/CDH-5.4.7-1.cdh5.4.7.p0.3/lib/sqoop/lib/mysql-connector-java-5.1.36-bin.jar /opt/cloudera/parcels/CDH-5.4.7-1.cdh5.4.7.p0.3/lib/hive/lib * 尝试2 在/etc/hive/conf.cloudera.hive/hive-env.sh中发现一句$(find /usr/share/java/mysql-connector-java.jar于是将驱动拷贝到指定目录解决问题cp /mnt/mysql-connector-java-5.1.36-bin.jar /usr/share/java/mysql-connector-java.jarcp /mnt/mysql-connector-java-5.1.36-bin.jar /opt/cloudera/parcels/CDH-5.4.7-1.cdh5.4.7.p0.3/lib/hadoop解决问题 2）hive数据库初始化问题问题日志： 1234Query for candidates of org.apache.hadoop.hive.metastore.model.MVersionTable and subclasses resulted in no possible candidatesRequired table missing : "`VERSION`" in Catalog "" Schema "". DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable "datanucleus.autoCreateTables"org.datanucleus.store.rdbms.exceptions.MissingTableException: Required table missing : "`VERSION`" in Catalog "" Schema "". DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable "datanucleus.autoCreateTables" at org.datanucleus.store.rdbms.table.AbstractTable.exists(AbstractTable.java:485) 解决参考:Reinstalling-Hive-Metastore-Database 在cm中删除oozie、hue和hive服务，重建数据库1234567# mysql -uroot -prootdrop database hive;drop database hue;drop database oozie;create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci; create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci; 重新添加hive服务配置数据库192.168.1.1(server)/hive(db)/root(user)/root(psd)]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>分布式</tag>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群离线部署]]></title>
    <url>%2F2015%2F10%2F20%2FHadoop%E9%9B%86%E7%BE%A4%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[GFW墙的没人性，只能费时费力搞个离线安装教程，一路遇到很多问题，稍微深入了解了一些ClouderaManager的内部实现，步骤概要：IP配置&gt;Host配置&gt;关闭iptables防火墙&gt;关闭SELinux&gt;配置NTP时钟服务&gt;SSH无密码登陆&gt;安装JDK&gt;配置CM安装包&gt;配置Parcel&gt;配置MySQL&gt;初始化SCM数据库&gt;复制到Agent机器&gt;启动CM Server&gt;配置Service&gt;设置Server/Agent开机启动 部署文档参考离线安装Cloudera Manager5.3.4与CDH5.3.4离线安装Cloudera Manager 5和CDH5(最新版5.1.3) 完全教程离线安装 Cloudera ( CDH 5.x )离线安装 Cloudera官方教程cdh对应hadoop版本 软件准备官方资源地址 JDK7最低版本：1.7.0_67 cloudera-manager-installer：下载地址 cloudera-manager-bin:下载地址 CDH Parcel：下载地址 虚拟机准备Cloudera Manager+MySQL 1234567Cloudera ManagerHostMonitorEvent ServerActivity MonitorService MonitorAlert Publisher MySQL Master 123HDFS：Active NameNodeHbase：Active MasterYARN：Active ResourceManager,JobHistory Server Standby Master 1234HDFS：Standby NameNode、DataNodeHbase：Standby Master、RegionServerYARN：Standby ResourceManager,Node ManagerHUE：Hue Server Slave1 12345678HDFS：DataNode、FailoverController、JournalNodeHbase：RegionServerYARN：Node ManagerImpala：Impala CatalogServer,Impala StateStore,Impala lama ApplicationMasterOozie：Oozie ServerHive：Hive ServerSor：Solr ServerZoomKeeper：zoomKeeper Server Slave2 1234567HDFS：DataNode、FailoverController、JournalNodeHbase：RegionServerYARN：Node ManagerImpala：Impala DaemonHive：Hive Metastore ServerSor：Solr ServerZoomKeeper：zoomKeeper Server Slave3 12345678HDFS：DataNode、FailoverController、JournalNodeHbase：RegionServerYARN：Node ManagerImpala：Impala DaemonSpark：Spark History ServerHive：Hive Metastore ServerSor：Solr ServerZoomKeeper：zoomKeeper Server Slave4 1234HDFS：DataNodeHbase：RegionServerYARN：Node ManagerImpala：Impala Daemon 安装JDK将 JDK 提取至 /usr/java/jdk-version；例如：/usr/java/jdk1.7.0_80将装有 JDK 的目录通过符号链接至 /usr/java/default；例如：ln -s /usr/java/jdk.1.7.0_80 /usr/java/default MySQL数据库配置安装配置MySQL配置SCM数据库123456789101112131415--hive数据库create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;--集群监控数据库create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;--hue数据库create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;--oozie数据库create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;grant all privileges on *.* to 'root'@'%' identified by 'root' with grant option;grant all privileges on *.* to 'root'@'master' identified by 'root' with grant option;grant all privileges on *.* to 'root'@'192.168.1.100' identified by 'root' with grant option;grant all privileges on *.* to 'root'@'192.168.1.183' identified by 'root' with grant option;flush privileges; 安装cloudera-manager主节点解压安装cloudera manager的目录默认位置在/opt下，解压：cd /opt &amp;&amp; tar xzvf cloudera-manager*.tar.gz 添加cloudera-scm用户：Agent节点执行：useradd –system –home=/opt/cm/run/cloudera-scm-server/ –no-create-home –shell=/bin/false –comment “Cloudera SCM User” cloudera-scm 更改cloudera-scm用户目录 查看用户ID:id cloudera-scm 查看用户home：echo ~cloudera-scm 修改用户home：usermod -d /opt/cm/run/cloudera-scm-server/ -u ${用户ID} cloudera-scm如：usermod -d /opt/cm/run/cloudera-scm-server/ -u 493 cloudera-scm 安装 mysql connector：外部MySQL数据库配置mysql-connector-java与mysql的版本对应关系 下载mysql-connector-java-5.1.36.tar.gz文件中提取 JDBC 驱动程序 JAR 文件。 解压：tar zxvf mysql-connector-java-5.1.36.tar.gz， 解压后找到mysql-connector-java-5.1.33-bin.jar，放到/opt/cm/share/cmf/lib/中。 初始化CM5的数据库：在SCM主节点初始化SCM数据库格式:scm_prepare_database.sh 数据库类型 数据库 服务器IP 用户名 密码 –scm-host Cloudera_Manager_Server机器IP scm scm scm如：/opt/cm/share/cmf/schema/scm_prepare_database.sh mysql -h 192.168.1.161 -uroot -proot --scm-host 192.168.1.100 scm scm scm重新执行的话，需要在mysql服务器执行 drop database scm; Agent配置 修改配置文件/opt/cm/etc/cloudera-scm-agent/config.ini中的server_host为主节点的主机名。 务必解压后不能启动server和agent，纯净版本同步Agent到其他节点scp -r /opt/cm root@192.168.1.91:/opt/ 复制运行中的agent需执行以下脚本后复制 123service cloudera-scm-agent stop chkconfig cloudera-scm-agent off poweroff 修改hostname、hosts 准备Parcels，用以安装CDH5将CHD5相关的Parcel包放到主节点的/opt/cloudera/parcel-repo/目录中（parcel-repo需要手动创建）。相关的文件如下： CDH-5.4.7-1.cdh5.4.7.p0.3-el6.parcelCDH-5.4.7-1.cdh5.4.7.p0.3-el6.parcel.sha1manifest.json 将CDH-5.4.7-1.cdh5.4.7.p0.3-el6.parcel.sha1，重命名为CDH-5.4.7-1.cdh5.4.7.p0.3-el6.parcel.sha，这点必须注意，否则，系统会重新下载CDH-5.4.7-1.cdh5.4.7.p0.3-el6.parcel文件。 启动脚本 启动cloudera manager server服务：/opt/cm/etc/init.d/cloudera-scm-server start 启动cloudera manager agent服务：/opt/cm/etc/init.d/cloudera-scm-agent start 设置为 开机自动启动 server端 123echo "/opt/cm/etc/init.d/cloudera-scm-server start" &gt;&gt; /etc/rc.local echo "/opt/cm/etc/init.d/cloudera-scm-agent start" &gt;&gt; /etc/rc.localcat /etc/rc.local agent端 123#启动cloudera manager agent服务echo "/opt/cm/etc/init.d/cloudera-scm-agent start" &gt;&gt; /etc/rc.localcat /etc/rc.local chkconfig服务方式更优，但目前无效，待完善TODO 1234567#启动cloudera manager server服务 cp /opt/cm/etc/init.d/cloudera-scm-server /etc/init.d/cloudera-scm-serverchkconfig cloudera-scm-server on#启动cloudera manager agent服务cp /opt/cm/etc/init.d/cloudera-scm-agent /etc/init.d/cloudera-scm-agentchkconfig cloudera-scm-agent on CDH5的安装配置进入cm网站：192.168.1.100:7180]]></content>
      <categories>
        <category>大数据</category>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>分布式</tag>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker学习笔记]]></title>
    <url>%2F2015%2F10%2F03%2FDocker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[dockerfile就像人体的DNA，提供自动构建一切的元数据。 Docker基本概念镜像（Image）Docker 镜像就是一个只读的模板。镜像可以用来创建 Docker 容器。 容器（Container）Docker 利用容器来运行应用。容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。*注：镜像是只读的，容器在启动的时候创建一层可写层作为最上层。 仓库（Repository）仓库是集中存放镜像文件的场所。有时候会把仓库和仓库注册服务器（Registry）混为一谈，并不严格区分。实际上，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。仓库分为公开仓库（Public）和私有仓库（Private）两种形式。*注：Docker 仓库的概念跟 Git 类似，注册服务器可以理解为 GitHub 这样的托管服务。 理解了这三个概念，就理解了 Docker 的整个生命周期。 Docker相关术语LXCLXC（Linux Container）Linux Container容器是一种内核虚拟化技术，可以提供轻量级的虚拟化，以便隔离进程和资源，而且不需要提供指令解释机制以及全虚拟化的其他复杂性。相当于C++中的NameSpace。容器有效地将由单个操作系统管理的资源划分到孤立的组中，以更好地在孤立的组之间平衡有冲突的资源使用需求。与传统虚拟化技术相比，它的优势在于：（1）与宿主机使用同一个内核，性能损耗小；（2）不需要指令级模拟；（3）不需要即时(Just-in-time)编译；（4）容器可以在CPU核心的本地运行指令，不需要任何专门的解释机制；（5）避免了准虚拟化和系统调用替换中的复杂性；（6）轻量级隔离，在隔离的同时还提供共享机制，以实现容器与宿主机的资源共享。总结：Linux Container是一种轻量级的虚拟化的手段。Linux Container提供了在单一可控主机节点上支持多个相互隔离的server container同时执行的机制。Linux Container有点像chroot，提供了一个拥有自己进程和网络空间的虚拟环境，但又有别于虚拟机，因为lxc是一种操作系统层次上的资源的虚拟化。 HypervisorHypervisor是一种运行在物理服务器和操作系统之间的中间软件层,可允许多个操作系统和应用共享一套基础物理硬件，因此也可以看作是虚拟环境中的“元”操作系统，它可以协调访问服务器上的所有物理设备和虚拟机，也叫虚拟机监视器（Virtual Machine Monitor）。Hypervisor是所有虚拟化技术的核心。非中断地支持多工作负载迁移的能力是Hypervisor的基本功能。当服务器启动并执行Hypervisor时，它会给每一台虚拟机分配适量的内存、CPU、网络和磁盘，并加载所有虚拟机的客户操作系统。 容器VS 虚拟机容器会比虚拟机更高效，因为它们能够分享一个内核和分享应用程序库。相比虚拟机系统，这也将使得 Docker使用的内存更小，即便虚拟机利用了内存超量使用的技术。部署容器时共享底层的镜像层也可以减少存储占用。IBM 的 Boden Russel 已经做了一些基准测试来说明两者之间的不同。 相比虚拟机系统，容器具有较低系统开销的优势，所以在容器中，应用程序的运行效率将会等效于在同样的应用程序在虚拟机中运行，甚至效果更佳。 常用命令 查看所有镜像：docker images： 删除所有镜像：docker rmi $(docker images | grep none | awk ‘{print $3}’ | sort -r) 删除所有容器：docker rm $(docker ps -a -q) 停止/启动/杀死一个容器：stop/start/kill &lt;容器名orID&gt; 问题记录 docker: relocation error: docker: symbol dm_task_get_info_with_deferred_remove, version Base not defined in file libdevmapper.so.1.02 with link time reference解决方案：You may have to enable the public_ol6_latest repo in order to get this package.sudo yum-config-manager --enable public_ol6_latestAnd then install the package:sudo yum install device-mapper-event-libs Error response from daemon: EOF Docker方案Kitematic Kitematic 是一个具有现代化的界面设计的自由开源软件，它可以让我们在 Docker 中交互式执行任务。Kitematic 设计的非常漂亮、界面美观。使用它，我们可以简单快速地开箱搭建我们的容器而不需要输入命令，可以在图形用户界面中通过简单的点击从而在容器上部署我们的应用。Kitematic 集成了 Docker Hub，允许我们搜索、拉取任何需要的镜像，并在上面部署应用。它同时也能很好地切换到命令行用户接口模式。目前，它包括了自动映射端口、可视化更改环境变量、配置卷、流式日志以及其它功能。 参考教程安装问题 12Error creating machine: exit status 1You will want to check the provider to make sure the machine and associated resources were properly removed. 待解决：github issue 待解决问题如何自动部署Github程序到Docker镜像构建搭载Tomcat环境的镜像部署前端web应用部署后端Java服务]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat服务器运维]]></title>
    <url>%2F2015%2F10%2F03%2FTomcat%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%90%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[记录Tomcat服务器的配置安装脚本 Tomcat安装配置系统环境变量 JAVA_HOME=D:\java\JDK1.6 PATH环境变量加入：%JAVA_HOME%\bin 安装Tomcat 开始-运行-cmd 输入：cd D:\tomcat\bin进入tomcat安装目录 输入：service install tomcat7进行安装（xx为tomcat服务名称，可随意给，也可不设置） 设置为开机启动开始-运行-services.msc进入服务管理器，将刚才安装的tomcat服务设置为自动，并启动，此时tomcat已成功安装并成功注册为自动启动的系统服务。 卸载Tomcat 输入：cd D:\tomcat\bin进入tomcat安装目录 输入：service remove tomcat7进行卸载（xx为已安装tomcat服务的名称，以前没设置就不用写） tomcat内存配置windows服务内存配置编辑tomcat\bin\service.bat找到&quot;%EXECUTABLE%&quot; //US//%SERVICE_NAME% --JvmOptions新增-Xms1024M;-Xmx2048M;-XX:PermSize=512M;-XX:MaxPermSize=1024M;然后卸载掉服务–&gt;安装服务–&gt;启动服务，生效。在localhost:8080/manager/status 查看修改后的可用内存大小 console控制台内存配置编辑catalina.bat，找到下面行修改 12rem ----- Execute The Requested Command ---------------------------------------set JAVA_OPTS=%JAVA_OPTS% -server -Xms6400m -Xmx6400m -XX:MaxNewSize=2048m -XX:PermSize=512M -XX:MaxPermSize=1024m Tomcat与Jetty 单纯比较 Tomcat与Jetty的性能意义不是很大，只能说在某种使用场景下，它表现的各有差异。因为它们面向的使用场景不尽相同。 从架构上来看 Tomcat 在处理少数非常繁忙的连接上更有优势，也就是说连接的生命周期如果短的话，Tomcat 的总体性能更高。而 Jetty 刚好相反，Jetty可以同时处理大量连接而且可以长时间保持这些连接。例如像一些 web 聊天应用非常适合用 Jetty 做服务器，像淘宝的 web 旺旺就是用 Jetty 作为 Servlet 引擎。由于 Jetty 的架构非常简单，作为服务器它可以按需加载组件，这样不需要的组件可以去掉，这样无形可以减少服务器本身的内存开销，处理一次请求也是可以减少产生的临时对象，这样性能也会提高。另外 Jetty 默认使用的是 NIO 技术在处理 I/O 请求上更占优势，Tomcat 默认使用的是 BIO，在处理静态资源时，Tomcat 的性能不如 Jetty。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式学习笔记]]></title>
    <url>%2F2015%2F10%2F03%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[记录常用正则以及相关测试工具 关于正则表达式正则表达式（Regular Expression，在代码中常简写为regex、regexp或RE），计算机科学的一个概念。正则表达式使用单个字符串来描述、匹配一系列符合某个句法规则的字符串。在很多文本编辑器里，正则表达式通常被用来检索、替换那些符合某个模式的文本。许多程序设计语言都支持利用正则表达式进行字符串操作。例如，在Perl中就内建了一个功能强大的正则表达式引擎。正则表达式这个概念最初是由Unix中的工具软件（例如sed和grep）普及开的。正则表达式通常缩写成“regex”，单数有regexp、regex，复数有regexps、regexes、regexen。 常用正则在编写处理字符串的程序或网页时，经常会有查找符合某些复杂规则的字符串的需要。正则表达式就是用于描述这些规则的工具。换句话说，正则表达式就是记录文本规则的代码。 元字符 代码 说明 . 匹配除换行符以外的任意字符 \w 匹配字母或数字或下划线或汉字 \s 匹配任意的空白符 \d 匹配数字 \b 匹配单词的开始或结束 ^ 匹配字符串的开始 $ 匹配字符串的结束 示例 1号10号：^[0-9]+号[0-9]+号?$ TAB+TAB关键词+TAB与TAA分隔符+TAA+TAA关键词，除TAB其余都可为空,如1号-10号：^[0-9０-９零一二三四五六七八九十壹贰叁肆伍陆柒捌玖拾]+[号]?([-])?([0-9０-９零一二三四五六七八九十壹贰叁肆伍陆柒捌玖拾]+)?[号]?$ 转义字符如果你想查找元字符本身的话，比如你查找.或\，就出现了问题：你没办法指定它们，因为它们会被解释成别的意思。这时你就得使用\来取消这些字符的特殊意义。因此，你应该使用\.和\*，要查找\本身，你也得用\. 代码 说明 . \. * \* \ \\ 重复 代码 说明 * 重复零次或更多次 + 重复一次或更多次 ? 重复零次或一次 {n} 重复n次 {n,} 重复n次或更多次 {n,m} 重复n到m次 字符类要想查找数字，字母或数字，空白是很简单的，因为已经有了对应这些字符集合的元字符，但是如果你想匹配没有预定义元字符的字符集合(比如元音字母a,e,i,o,u),应该怎么办？指定一个字符范围，像[0-9]代表的含意与\d就是完全一致的：一位数字；同理[a-z0-9A-Z_]也完全等同于\w（如果只考虑英文的话）。 分支条件正则表达式里的分枝条件指的是有几种规则，如果满足其中任意一种规则都应该当成匹配，具体方法是用 | 把不同的规则分隔开。 分组可以用小括号来指定子表达式(也叫做分组)如IP格式：：(\d{1,3}\.){3}\d{1,3}是一个简单的IP地址匹配表达式：\d{1,3}匹配1到3位的数字，(\d{1,3}.){3}匹配三位数字加上一个英文句号(这个整体也就是这个分组)重复3次，最后再加上一个一到三位的数字(\d{1,3})。 IP地址中每个数字都不能大于255.但是正则表达式中并不提供关于数学的任何功能，所以只能使用冗长的分组，选择，字符类来描述一个正确的IP地址：((2[0-4]\d|25[0-5]|[01]?\d\d?)\.){3}(2[0-4]\d|25[0-5]|[01]?\d\d?) 反义有时需要查找不属于某个能简单定义的字符类的字符。比如想查找除了数字以外，其它任意字符都行的情况，这时需要用到反义,常用的反义代码 代码 说明 \w 匹配任意不是字母，数字，下划线，汉字的字符 \S 匹配任意不是空白符的字符 \D 匹配任意非数字的字符 \B 匹配不是单词开头或结束的位置 [^x] 匹配除了x以外的任意字符 [^aeiou] 匹配除了aeiou这几个字母以外的任意字符 例子：\S+匹配不包含空白符的字符串；&lt;a[^&gt;]+&gt;匹配用尖括号括起来的以a开头的字符串。 后向引用零宽断言负向零宽断言注释贪婪与懒惰处理选项平衡组/递归匹配测试工具regex在线测试 这个专业！oschina regex在线测试 国内的还是不太靠谱！ 相关阅读本文笔记内容参考正则表达式30分钟入门教程]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>RegEx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SublimeText-Markdown书写利器]]></title>
    <url>%2F2015%2F09%2F26%2FSublimeText-Markdown%E4%B9%A6%E5%86%99%E5%88%A9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[曾尝试寻找在线平台写博客 segmentfault：专栏文章模块markdown编写可检测剪贴板图片，且需要审核；但是笔记模块不支持 csdn：改版的markdown编辑器很好，但是不支持粘帖图片，且存在删文风险 几经周折，最终还是选择了自己搭建写作环境：sumblime配置markdown写作环境 + evernote笔记检索+hexo博客框架 + github.io发布；以sublimeText编辑器作为写作环境（markdown语法高亮/预览），以sublime-evernote发布到evernote，hexo搭建博客框架定期发布到github.io，谨记编辑器够用就好，内容应始终放在第一位。 安装Package Control使用Ctrl+`快捷键或者通过View-&gt;Show Console菜单打开命令行，粘贴如下代码： import urllib.request,os; pf = &#39;Package Control.sublime-package&#39;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); open(os.path.join(ipp, pf), &#39;wb&#39;).write(urllib.request.urlopen( &#39;http://sublime.wbond.net/&#39; + pf.replace(&#39; &#39;,&#39;%20&#39;)).read()) SublimeText快捷键 命令面板：Ctrl+Shift+P’ 列选择：Shirft+右键 行选择：Ctrl+L 全屏书写：Shirft + F11 SublimeText 用户配置主题：Material-Theme 12345678910111213141516171819202122232425&#123; "color_scheme": "Packages/Monokai Extended/Monokai Extended Bright.tmTheme", "draw_centered": false, "font_face": "Consolas", "font_size": 9, "gutter": true, "ignored_packages": [ "Markdown", "Vintage" ], "index_exclude_patterns": [ "*.log", "*.gitignore" ], "line_numbers": true, "line_padding_bottom": 1, "line_padding_top": 1, "scroll_past_end": true, "tab_size": 2, "theme": "Material-Theme.sublime-theme", "translate_tabs_to_spaces": false, "word_wrap": true&#125; Markdown插件Markdown语法高亮：Monokai语法高亮：Monokai ExtendedGithub地址 Markdown编辑：MarkdownEditing设置为MarkdownEditing&gt;MultiMarkDown官方文档 选择内容设为链接：Ctrl + Win + V 粘贴板内容设为链接：Ctrl + Win + R 插入链接：Ctrl + Win + K 插入图片：Shift + Win + K 加粗：Ctrl + Shift + B 斜体：Ctrl + Shift + I 标题：Ctrl + 1/2/3/4/5/6 显示Markdown文件标题：Ctrl + Shift + R Markdown预览：OmniMarkupPreviewer Ctrl + Alt + O: 在浏览器中预览(实时无需刷新的哦) Ctrl + Alt + X: 导出HTML Ctrl + Alt + C: HTML标记拷贝至剪贴板 markdown样式next主题修改：/next/source/css/_variables/base.styl文件中的$font-family-chinese、$font-size-base等属性定制 12345678body &#123; position: relative; color: #333; background: #fff; font-size: 14.5px; line-height: 1.8; font-family: Consolas, monaco, "Helvetica Neue", Helvetica, Arial, "Hiragino Sans GB", "Microsoft YaHei", STHeiti, "WenQuanYi Micro Hei", sans-serif;&#125; sublime配置笔记同步到evernote参考配置sublime-evernote Evernote 的快捷键在 Key Bindings——User配置 123456[&#123; "keys": ["super+e"], "command": "show_overlay", "args": &#123;"overlay": "command_palette", "text": "Evernote: "&#125; &#125;,&#123; "keys": ["ctrl+e", "ctrl+s"], "command": "send_to_evernote" &#125;,&#123; "keys": ["ctrl+e", "ctrl+o"], "command": "open_evernote_note" &#125;,&#123; "keys": ["ctrl+e", "ctrl+u"], "command": "save_evernote_note" &#125;,] 如新增笔记：[“ctrl+e”, “ctrl+s”] 就是先按完ctrl + e 后再按 ctrl + s ； sublime-evernote配置 123456789101112 &#123; "code_friendly": true, "code_highlighting_style": "github", "extensions": [ "md" ], "noteStoreUrl": "https://www.evernote.com/shard/s56/notestore", "show_stacks": true, "token": "S=s56:U=63871d:E=15c7d92376e:C=15525e109a8:P=1cd:A=en-devtoken:V=2:H=b30896c360f9be6886b610bbb7dc7df3", "update_on_save": true&#125; 问题记录markdown笔记与evernote保留字冲突问题 问题描述：Evernote complained:The contents of the note are not valid. The inline HTML tag ‘String’ is not allowed in Evernote notes.Retry? 问题解决：List改为List&lt; String&gt;接警]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
        <tag>TextEditor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《数学之美》读书笔记]]></title>
    <url>%2F2015%2F09%2F22%2F%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E%2F</url>
    <content type="text"><![CDATA[谷歌吴军：数学之美 如何化繁为简，如何用数学去解决工程问题，如何跳出固有思维不断去思考创新。 系列二 – 谈谈中文分词2015-09-13 16:31:25一般来讲，根据不同应用，汉语分词的颗粒度大小应该不同。比如，在机器翻译中，颗粒度应该大一些，“北京大学”就不能被分成两个词。而在语音识别中，“北京大学”一般 是被分成两个词。因此，不同的应用，应该有不同的分词系统 系列三 – 隐含马尔可夫模型在语言处理中的应用2015-09-13 16:34:02自然语言是人类交流信息的工具。很多自然语言处理问题都可以等同于通信系统中的解码问题 – 一个人根据接收到的信息，去猜测发话人要表达的意思。这其实就象通信中，我们根据接收端收到的信号去分析、理解、还原发送端传送过来的信息。 2015-09-13 16:35:28在计算机中，如果我们要根据接收到的英语信息，推测说话者的汉语意思，就是机器翻译；如果我们要根据带有拼写错误的语句推测说话者想表达的正确意思，那就是自动纠错。 系列 4 – 怎样度量信息?2015-09-13 21:37:46Google 一直以 “整合全球信息，让人人能获取，使人人能受益” 为使命 2015-09-13 21:40:33一条信息的信息量大小和它的不确定性有直接的关系。 2015-09-13 21:40:44信息量的度量就等于不确定性的多少。 2015-09-13 21:47:32不同语言的冗余度差别很大，而汉语在所有语言中冗余度是相对小的。这和人们普遍的认识”汉语是最简洁的语言”是一致的。 系列五 – 简单之美：布尔代数和搜索引擎的索引2015-09-13 21:48:42建立一个搜索引擎大致需要做这样几件事：自动下载尽可能多的网页；建立快速有效的索引；根据相关性对网页进行公平准确的排序 2015-09-14 22:31:43搜索引擎的索引就变成了一张大表：表的每一行对应一个关键词，而每一个关键词后面跟着一组数字，是包含该关键词 的文献序号。 2015-09-14 22:32:45每当接受一个查询时，这个查询就被分送到许许多多服务器中，这些服务器 同时并行处理用户请求，并把结果送到主服务器进行合并处理，最后将结果返回给用户。 2015-09-14 22:33:05不管索引如何复杂，查找的基本操作仍然是布尔运算。布尔运算把逻辑和数学联系起来了。它的最大好处是容易实现，速度快，这对于海量的信息查找是至关重要的。它的不足是只能给出是与否的判断，而不能给出量化的 度量。因此，所有搜索引擎在内部检索完毕后，都要对符合要求的网页根据相关性排序，然后才返回给用户。 系列六 – 图论和网络爬虫2015-09-14 22:33:57如何自动下载互联网所有的网页呢，它要用到图论中的遍历（Traverse) 算法。 2015-09-14 22:35:57“广度优先算法”（BFS)，因为它先要尽可能广地访问每个节点所直接连接的其他节点。 2015-09-14 22:36:22“深度优先算法”（DFS)，因为它是一条路走到黑。 系列七 – 信息论在信息处理中的应用2015-09-14 22:41:23李开复博士在介绍他发明的 Sphinx 语音识别系统时谈到，如果不用任何语言模型（即零元语言模型）时，复杂度为997，也就是说句子中每个位置有 997 个可能的单词可以填入。如果（二元）语言模型只考虑前后词的搭配不考虑搭配的概率时，复杂度为60。虽然它比不用语言模型好很多，但是和考虑了搭配概率的二元语言模型相比要差很多，因为后者的复杂度只有20。 2015-09-14 22:41:40信息论中仅次于熵的另外两个重要的概念是“互信息”（Mutual Information) 和“相对熵”（Kullback-Leibler Divergence)。 2015-09-14 22:42:00“互信息”是信息熵的引申概念，它是对两个随机事件相关性的度量。 2015-09-14 22:45:15信息论中另外一个重要的概念是“相对熵”，在有些文献中它被称为成“交叉熵”。在英语中是 Kullback-Leibler Divergence， 是以它的两个提出者库尔贝克和莱伯勒的名字命名的。相对熵用来衡量两个正函数是否相似，对于两个完全相同的函数，它们的相对熵等于零。在自然语言处理中可 以用相对熵来衡量两个常用词（在语法上和语义上）是否同义，或者两篇文章的内容是否相近等等。利用相对熵，我们可以到处信息检索中最重要的一个概念：词频率-逆向文档频率（TF/IDF)。 系列八– 贾里尼克的故事和现代语言处理2015-09-14 22:53:52七十年代的 IBM 有点像九十年代的微软和今天的 Google, 给于杰出科学家作任何有兴趣研究的自由。在那种宽松的环境里，贾里尼克等人提出了统计语音识别的框架结构。 在贾里尼克以前，科学家们把语音识别问题当作人工智能问题和模式匹配问题。而贾里尼克把它当成通信问题，并用两个隐含马尔可夫模型（声学模型和语言模型） 把语音识别概括得清清楚楚。这个框架结构对至今的语音和语言处理有着深远的影响，它从根本上使得语音识别有实用的可能。 贾里尼克本人后来也因此当选美国工程院院士。 系列九 – 如何确定网页和查询的相关性2015-09-15 22:48:01如果一个关键词只在很少的网页中出现，我们通过它就容易锁定搜索目标，它的权重也就应该大。反之如果一个词在大量网页中出现，我们看到它仍 然不很清楚要找什么内容，因此它应该小。概括地讲，假定一个关键词w在DW个网页中出现过，那么DW越大，w的权重越小，反之亦然。在信息检索中，使用最多的权重是“逆文本频率指数” 2015-09-15 22:49:07ＴＦ／ＩＤＦ（term frequency/inverse document frequency) 的概念被公认为信息检索中最重要的发明。在搜索、文献分类和其他相关领域有广泛的应用。 2015-09-15 22:50:46信息论的学者们已经发现并指出，其实 IDF 的概念就是一个特定条件下、关键词的概率分布的交叉熵（Kullback-Leibler Divergence)（详见上一系列）。这样，信息检索相关性的度量，又回到了信息论。 系列十 有限状态机和地址识别2015-09-16 22:00:29地址的识别和分析是本地搜索必不可少的技术，尽管有许多识别和分析地址的方法，最有效的是有限状态机。 2015-09-16 22:00:55一个有限状态机是一个特殊的有向图（参见有关图论的系列），它包括一些状态（节点）和连接这些状态的有向弧。 2015-09-16 22:02:38使用有限状态机识别地址，关键要解决两个问题，即通过一些有效的地址建立状态机，以及给定一个有限状态机后，地址字串的匹配算法。 2015-09-16 22:04:22基于有限状态机的地址识别方法在实用中会有一些问题：当用户输入的地址不太标准或者有错别字时，有限状态机会束手无策，因为它只能进行严格匹配。（其实，有限状态机在计算机科学中早期的成功应用是在程序语言编译器的设计中。一个能运行的程序在语法上必须是没有错的，所以不需要模糊匹配。而自然语言则很随意，无法用简单的语法描述。）为了解决这个问题，我们希望有一个能进行模糊匹配、并给出一个字串为正确地址的可能性。为了实现这一目的，科学家们提出了基于概率的有限状态机。这种基于概率的有限状态机和离散的马尔可夫链（详见前面关于马尔可夫模型的系列）基本上等效。 十四 谈谈数学模型的重要性2015-09-17 21:57:04 一个正确的数学模型应当在形式上是简单的。（托勒密的模型显然太复杂。） 一个正确的模型在它开始的时候可能还不如一个精雕细琢过的错误的模型来的准确，但是，如果我们认定大方向是对的，就应该坚持下去。（日心说开始并没有地心说准确。） 大量准确的数据对研发很重要。 正确的模型也可能受噪音干扰，而显得不准确；这时我们不应该用一种凑合的修正方法来弥补它，而是要找到噪音的根源，这也许能通往重大发现。 2015-09-17 21:57:17在网络搜索的研发中，我们在前面提到的单文本词频/逆文本频率指数（TF/IDF) 和网页排名（page rank)都相当于是网络搜索中的”椭圆模型”，它们都很简单易懂。 系列十六（上） 不要把所有的鸡蛋放在一个篮子里2015-09-21 22:08:08最大熵原理指出，当我们需要对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设。（不做主观假设这点很重要。）在这种情况下，概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫”最大熵模型”。我们常说，不要把所有的鸡蛋放在一个篮子里，其实就是最大熵原理的一个朴素的说法，因为当我们遇到不确定性时，就要保留各种可能性。 系列十六 （下）－ 不要把所有的鸡蛋放在一个篮子里2015-09-21 22:14:07最原始的最大熵模型的训练方法是一种称为通用迭代算法 GIS(generalized iterative scaling) 的迭代 算法。GIS 的原理并不复杂，大致可以概括为以下几个步骤： 假定第零次迭代的初始模型为等概率的均匀分布。 用第 N 次迭代的模型来估算每种信息特征在训练数据中的分布，如果超过了实际的，就把相应的模型参数变小；否则，将它们便大。 重复步骤 2 直到收敛。 2015-09-21 22:19:29最大熵模型，可以说是集简与繁于一体，形式简单，实现复杂。 系列十八 － 矩阵运算和文本处理中的分类问题2015-09-21 22:36:15分类的关键是计算相关性。我们首先对两个文本计算出它们的内容词，或者说实词的向量，然后求这两个向量的夹角。当这两个向量夹角为零时，新闻就相关；当它们垂直或者说正交时，新闻则无关。 系列十九 － 马尔可夫链的扩展 贝叶斯网络2015-09-21 22:39:58马尔可夫链 (Markov Chain)，它描述了一种状态序列，其每个状态值取决于前面有限个状态。这种模型，对很多实际问题来讲是一种很粗略的简化。在现实生活中，很多事物相互的关系并不能用一条链来串起来。它们之间的关系可能是交叉的、错综复杂的。 2015-09-21 22:39:35贝叶斯网络。其中每个圆圈表示一个状态。状态之间的连线表示它们的因果关系。比如从心血管疾病出发到吸烟的弧线表示心血管疾病可能和吸烟有关。当然，这些关系可以有一个量化的可信度 (belief)，用一个概率描述。我们可以通过这样一张网络估计出一个人的心血管疾病的可能性。在网络中每个节点概率的计算，可以用贝叶斯公式来进行，贝叶斯网络因此而得名。由于网络的每个弧有一个可信度，贝叶斯网络也被称作信念网络 (belief networks)。 系列二十 －自然语言处理的教父 马库斯2015-09-21 22:43:25PennTree Bank 覆盖多种语言（包括中文）。每一种语言，它有几十万到几百万字的有代表性的句子，每个句子都有的词性标注，语法分析树等等。LDC 语料库如今已成为全世界自然语言处理科学家共用的数据库。如今，在自然语言处理方面发表论文，几乎都要提供基于 LDC 语料库的测试结果。 系列二十二 由电视剧《暗算》所想到的2015-09-21 22:54:10来设计一个密码系统，对这个明码加密。1，找两个很大的素数（质数）P 和 Q，越大越好，比如 100 位长的, 然后计算它们的乘积 N=P×Q，M=（P-1）×（Q-1）。2，找一个和 M 互素的整数 E，也就是说 M 和 E 除了 1 以外没有公约数。3，找一个整数 D，使得 E×D 除以 M 余 1，即 E×D mod M = 1。现在，世界上先进的、最常用的密码系统就设计好了，其中 E 是公钥谁都可以用来加密，D 是私钥用于解密，一定要自己保存好。乘积 N 是公开的，即使敌人知道了也没关系。现在，我们用下面的公式对 X 加密，得到密码 Y。好了，现在没有密钥 D，神仙也无法从 Y 中恢复 X。如果知道 D，根据费尔马小定理，则只要按下面的公式就可以轻而易举地从 Y 中得到 X。 多看笔记 来自多看阅读 for Kindle多看ID: geosmart]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库使用笔记]]></title>
    <url>%2F2015%2F09%2F21%2FMySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[记录一些常用的MysQL运维脚本和常见问题 MySQL函数文档 linux重置mysql root密码123456789# 新建文件touch /mnt/script/mysql-init# 文件内容为SET PASSWORD FOR 'root'@'localhost' = PASSWORD('MyNewPass');# 查看mysqld进程ID并杀掉service mysqld statuskill $&#123;pid&#125;# 重启mysql服务并重置root密码mysqld_safe --init-file=/mnt/script/mysql-init &amp; 重启mysql服务service mysqld restart MySQL开启远程连接问题：Access denied for user ‘root’@’192.168.1.172’ (using password: YES)解决： 1234567mysql -uroot -prootshow grants;use mysqlselect host,user,password from user;update user set host='%' where user="root" and host;GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' WITH GRANT OPTION;FLUSH PRIVILEGES; mysql 新建用户，设置权限DROP USER ‘ugc’@’%’;CREATE USER ‘ugc’@’%’ IDENTIFIED BY ‘ugc’;GRANT ALL PRIVILEGES ON . TO ‘ugc’@’%’ Identified by ‘ugc’;GRANT ALL PRIVILEGES ON . TO ‘ugc’@’localhost’ Identified by ‘ugc’;FLUSH PRIVILEGES; 登陆mysql -uugc -pugc 建库CREATE DATABASE ugc; 数据导入单个导入：source data.sql如：cd /tmp/ugc-data/ &amp;&amp; find . -name &#39;echzutravel.sql&#39; | awk &#39;{ print &quot;source&quot;,$0 }&#39; | mysql --batch -u ugc -p ugc 批量导入：find . -name &#39;*.sql&#39; | awk &#39;{ print &quot;source&quot;,$0 }&#39; | mysql --batch -u root -p db_name如导入/tmp/ugc-data/目录的所有sql文件到ugc库：cd /tmp/ugc-data/ &amp;&amp; find . -name &#39;*.sql&#39; | awk &#39;{ print &quot;source&quot;,$0 }&#39; | mysql --batch -u ugc -p ugc 数据导出 导出整个数据库mysqldump -u 用户名 -p 数据库名 &gt; 导出的文件名mysqldump -u wcnc -p smgp_apps_wcnc &gt; wcnc.sql 导出一个表mysqldump -u 用户名 -p 数据库名表名&gt; 导出的文件名mysqldump -u wcnc -p smgp_apps_wcnc users&gt; wcnc_users.sql 导出一个数据库结构mysqldump -u wcnc -p -d –add-drop-table smgp_apps_wcnc &gt;d:wcnc_db.sql-d 没有数据 –add-drop-table 在每个create语句之前增加一个drop table 示例12345678# 数据库备份`mysqldump geocodingdb -ugeocodingdb -pgeocodingdb --routines --comments &gt; /uadb/geocodingdb.sql`# 压缩备份`mysqldump standarddb -ustandarddb -pstandarddb --routines --comments | gzip -v &gt; /uadb/standarddb.gz`# 压缩已有备份sql`zip -r /uadb/uadb.bakcup.suzhou.0512.zip /uadb/uadb.bakcup.suzhou.0512`# 数据库还原`mysql -ugeocodingdb -pgeocodingdb geocodingdb --comments &lt; /uadb/geocodingdb.sql` 问题记录Too many connections查看最大连接数：show variables like ‘max_connections’;查询一下服务器响应的最大连接数： show global status like ‘Max_used_connections’;show status like ‘%connection%’;显示当前运行的Query： show processlist;设置新的最大连接数为5000：1）shell临时修改： set GLOBAL max_connections=5000;2）在/etc/my.cnf 中修改连接max_connections = 5000 Packet for query is too large 关于max_allowed_packet参数MySQL根据配置文件会限制Server接受的数据包大小。有时候大的插入和更新会受 max_allowed_packet 参数限制，导致写入或者更新失败。 查询：show VARIABLES like ‘%max_allowed_packet%’; 命令行中修改：set global max_allowed_packet = 210241024*10; 在windows（my.ini），Linux（/etc/my.cnf）中 修改：max_allowed_packet = 20M Host is blocked because of many connection errors 错误描述：An error occurred while establishing the connection: message from server: “Host ‘192.168.1.174’ is blocked because of many connection errors; unblock with ‘mysqladmin flush-hosts’” 查询：show VARIABLES like ‘%max_connect_errors%’; 命令行中修改：set global max_connect_errors =1000; 在windows（my.ini），Linux（/etc/my.cnf）中 修改：max_connect_errors= 1000 No buffer space available (maximum connections reached?) 错误描述：大量数据库连接运行sql，抛出异常java.net.SocketException: No buffer space available (maximum connections reached?): JVM_Bind,在运行 Windows Server 2008 R2 或 Windows 7 的多处理器计算机上的内核套接字泄漏 解决：winServer2008多处理器计算机上的内核套接字泄漏bug，下载补丁修复The reason we got this error is a bug in Windows Server 2008 R2 / Windows 7. The kernel leaks loopback sockets due to a race condition on machines with more than one core,patch 2577795 fixes the issue: You can’t specify target table ‘Place’ for update in FROM clause执行错误：DELETE FROM Place where guid in (select guid from Place where address like ‘%市%区’ and address not like ‘%南海区%’) ;修改为：DELETE FROM Place whereguid in (select * from (select guid from Place where address like ‘%市%区’ and address not like ‘%南海区%’) as t); mysql表名区分大小写MySQL在Windows下数据库名、表名、列名、别名都不区分大小写。如果想大小写区分，在my.ini 里面的mysqld部分，加入 lower_case_table_names=0 查询表的字段名称select column_name from information_schema.columns where table_name = ‘ExtractedAddress’ and column_name like ‘地名%’ and column_name &lt;&gt; ‘地名’]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown常用语法]]></title>
    <url>%2F2015%2F09%2F18%2FMarkdown%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[记录一些markdown的语法和使用示例 标题引用列表代码图片 ![geosmart](https://avatars1.githubusercontent.com/u/3156608?v=3&amp;s=64) 链接 [geosmart](geosmart.github.io)geosmart 粗体斜体删除线 ~~这是一条删除线~~这是一条删除线 分割线 表格12345|Header |Column 1 | Column 2 | Column 3 ||:--- |:---- |:----:| ----:||1. Row| is | is | is ||2. Row| left | nicely | right ||3. Row| aligned | centered | aligned | Header Column 1 Column 2 Column 3 1. Row is is is 2. Row left nicely right 3. Row aligned centered aligned]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GATE中文自然语言处理学习笔记]]></title>
    <url>%2F2015%2F09%2F17%2FGATE%E4%B8%AD%E6%96%87%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[GATE GATE(文本工程通用框架)项目开始于 1995 年英国的谢菲尔德大学.经历了近 20 年的不断发展，GATE 已经被应用于广泛的研究和项目开发。 GATE 框架采用了基于组件的软件开发方式和面向对象的灵活编程。 GATE 框架是由纯 Java 语言开发的免费开源软件，遵循 GNU library license。 GATE 使用的编码方式是Unicode，可以支持多种语言编码，并且针对各种斯拉夫语言、日尔曼语言、拉丁系语言和印度语做过系统测试。 GATE 支持的文档类型包括 XML、RTF、Email、HTML、SGML以及纯文本文件。 GATE 作为一个框架，规定其框架内所有的自然语言处理软件系统元素都可以有效的被细分成不同的几种组件，在 GATE 中它们被称为资源。在 GATE 框架下组件的集合被称为 CREOLE。CREOLE 组件是通过 Java Beans 的形式来实现的，CREOLE 在 GATE中分为三种形式：语言组件（LR），处理组件(PR)和可视化组件(VR)。 GATE资源： GATE中文自然语言处理 PipelineCorpusControllerRealtimeCorpusControllerSerialAnalyserControllerGazetteer词典库（Gazetteer）由一组词典列表组成，列表包含实体名称如城市，组织机构，星期等。列表用于发现文本中包含这些名称的文档和命名实体识别。通常词“词典库”可交替地用于实体列表集合，并利用这些列表发现文本中出现这些名称的处理资源。当词典库PR在一个文件上运行时，每个匹配的文本字符串创建一个标注类型Lookup。词典库通常不依靠Token或任何其它标注，而是基于文档的字词内容发现相匹配的字符串。这意味着一个条目可以跨越多个词，也可以把开始和结束作为一个词。如果一个词典库正工作的文本恰好满足词语边界，则词典发现词语边界的方法不同于GATE tokeniser发现词语边界的方法。如果文本中所有词典实体匹配好了，则创建一个Lookup标注。词典库实体怎样匹配文本的细节取决于词典库处理资源及其参数。 Gazetteer生效机制入口:list.defgazetteer： xx.lst格式：gazetteerFileName:marjorType:minorType ANNIE词典库ANNIE词典库是由ANNIE插件提供的。每个独立的词典都是一个普通的文本文件，每行一个条目。 GAZE词典库Gaze是一个编辑词典列表，定义词典和把词典映射到本体的工具。即适用于Plain/Linear 词典（默认）、Hash词典库也适用于Ontology-enabled 词典 (Onto词典)。每当执行保存操作时，重新初始化词典PR关联的视图。注意GAZE不生产规模非常大的词典 (我们假设不浏览超过4000实体，不拷贝超过10000个实体)。 Onto词典Onto词典或者Hierarchical词典，是一个处理资源，能将特定词典列表的实体加入到GATE本体语言资源的类中。Onto词典指定类而不是主要或次要类型，意识到列表和类ID之间的映射关系。Gaze可视资源能显示列表，本体映射和本体类等级，也提供编辑这些内容的方法。 Hash词典Hash词典是OntoText Lab完成的一个词典。它的实现基于几个java.util.HashMap对象简单的查找，并由Atanas Kiryakov的奇妙想法——在HashMaps的搜寻快于在有限状态机的搜寻而得到启发。Hash词典处理资源是ANNIE插件的一部分。词典处理资源用以下方式实现：每个短语，即每一个列表条目被分割成几个部分，各部分取决于位于它们之间的空格，例如，短语：“form is emptiness”有三个部分：“form”， “is”和“emptiness”。也有一个HashMaps列表：mapsList和列表中最长的短语有同样多的元素。所以第一部分短语放在第一个映射。第一部分+空格+第二部分被放置在第二个映射等。完整的词组是放置在适当的映射，并引用一个查找对象是相连。 Flexible词典Flexible词典提供用户灵活的弹性选择他们自己定制的输入和一个外部词典。例如，用户可能想将文本的词语替换成他们的基本形式(这是一种形态学分析仪的输出)或在运行词典之前分割中文文本(用中文Tokeniser)。The Flexible 词典 performs lookup over a document based on the values of an arbitrary feature of an arbitrary annotation type, by using an externally provided 词典.使用外部词典是很重要的，这允许使用任何类型的词典(例如本体词典)。 词典列表collection词典列表collection直接从一组标注的训练文本收集实体事件，构成有实体的词典列表。实体类型和词典列表的结构是由用户定义的。一旦列表collection，一个语义语法用于寻找新文本相同的实体。如果没有列表存在，对于每个标注类型要先创建一个空列表。处理资源运行前，列表集必须加载到GATE。如果已有列表存在，列表只是简单的增加任何新实体。列表collection只收集每个实体的一个事件：在添加新的之前要先检查实体已经不存在。 OntoRoot词典OntoRoot词典是一种动态创建的词典，即结合其他少量的通用Gate资源，对于给定的本体的内容产生基于本体的标注。这个词典是“Gazetteer_Ontology_Based”插件的一部分，已开发成为TAO项目的一部分。 大型KB词典大型KB 词典为ontology-aware自然语言处理提供支持。你可以从RDF装载任何本体，然后使用词典获得lookup标注，标注有实例和类URI。大型KB词典作为插件词典 LKB存在。当前版本的大型KB 词典不使用GATE本体语言资源。相反，它使用自己的原理去加载处理本体。当前版本在不久的未来可能会显著改变。大型KB 词典是从语义搜索平台Ontotext KIM中的一个组件学习起来的。这词典是由KIM发展团队开发的（见http://nmwiki.ontotext.com/lkb_Gazetteer/team-list.html）。在kim名称左边你可以找到源代码、资料/文件管理或源文件。 JAPEJAPE 的全称是 a Java Annotation Patterns Engine，Java 标注模式引擎，JAPE 提供了基于正规表达式的标注有限状态转换。JAPE 是通用模式定义语言 CPSL(Common Pattern Specification Language1)的一个版本。我们通过 JAPE 语言可以编写 GATE 能够识别的规则，通过这些规则来进行较准确的命名实体识别。 grammer-jape入口：base.japeMultiPhase:Phases: 列表jape：:xx.jape示例： 12345678910111213//沿西部干线由南向北行驶至六合区程桥街道荷花社区方桥河桥路段Phase: spacepattern_pathInput: Lookup AtomToken Token LocationOptions: control = appelt debug = trueRule: path1Priority: 4120(&#123;Lookup.majorType == sp_path,Lookup.minorType==prefix&#125;((&#123;AtomToken&#125;)[1,30]):context&#123;Lookup.majorType == sp_path,Lookup.minorType==suffix&#125;):path--&gt;:path.Path = &#123; type = Path, context = :context@string &#125; relationship如何构建？annotation和entity的关系如何构建 JAPE标注类型 Lookup：Annie默认标注结果 Token：Lang_Chinese，最长分词，包含符号用途：数字，附号标注 IKAToken：IKanalyzer分词标注—两两分词，不处理符号 采用了特有的”正向迭代最细粒度切分算法”，具有80万字/秒的高速处理能力 采用了多子处理器分析模式，支持：英文字母（IP地址、Email、URL）、数字（日期，常用中文数量词，罗马数字，科学计数法），中文词汇（姓名、地名处理）等分词处理。 优化的词典存储，更小的内存占用。支持用户词典扩展定义 针对Lucene全文检索优化的查询分析器IKQueryParser；采用歧义分析算法优化查询关键字的搜索排列组合，能极大的提高Lucene检索的命中率。 AtomToken：原子分词，最细颗粒度分词 Tokeniser RulesTokeniser Rules标注解析器。根据数字、标点符号和单词将文本分解成不同的Token。目的是将标记解析效率最大化，并通过语法规则（Grammar rules）减轻负担以此提高标记解析工作的灵活性。JAPE规则包括两部分，LHS和RHS，与LHS匹配上的标注集将会按照RHS的操作执行。 LHS(left hand sode)包含正规表达式操作符（比如*, ?, +）的标注模式 | 或者 零次或者多次发生? 零次或者一次发生 一次或者多次发生 RHS(right hand side)包含了标注集操作描述，格式：{LHS} &gt; {Annotation type};{attribute1}={value1};...;{attributen}={value n} Token TypesWord、Number、Symbol、Punctuation、SpaceToken JAPE优先级控制类型 Brill: Brill 默认的控制风格，作用是在给定的范围内对文档进行匹配并添加类型，当多个匹配规则都匹配到一个文本时默认都会把相应的类型添加上去，这些类型没有重要等级。 All: All 作用类似于Brill，但是All的匹配支持内嵌匹配 First First只匹配第一个匹配到的匹配规则并添加对应类型，所以此时使用’*’,’?’或’+’这类匹配规则是不恰当的 Once: Once 一旦匹配到匹配规则，则整个JAPE解析直接退出 Appelt: Applet 根据优先规则，匹配唯一一个匹配规则 JAPE高亮插件找到一个Linux平台的，gedit的插件：jape-syntax-highlighting，待尝试 当前处理方式 myeclipse设置*.jape默认打开程序为sublime textGeneral&gt;Editors&gt;File Associations 在sublime text代开jape后缀文件，并将Syntax设置为open all with current extention as java sublimet text3安装SublimeAStyleFormatter插件进行代码格式化 ctrl+alt+f: Format current file ctrl+k, ctrl+f: Format current selectionTODO：暂不能自动格式化jape，和语法高亮报错 正则表达式relationshioType的定义原理sentence（有限状态机）包含关系，根据实际情况定义Temporal+SpacePattern(Region+Path)+person GATE在多线程中的配置Using GATE Embedded in a Multithreaded EnvironmentAll the standard ANNIE PRs are safe when independent instances are used in diﬀerent threads concurrently, as are the standard transient document, transient corpus and controller classes. A typical pattern of development for a multithreaded GATE-based application is: * Develop your GATE processing pipeline in GATE Developer. * Save your pipeline as a .gapp ﬁle. * In your application’s initialisation phase, load n copies of the pipeline using `PersistenceManager.loadObjectFromFile()` , or load the pipeline once and then make copies of it using `Factory.duplicate`, and either give one copy to each thread or store them in a pool (e.g. a LinkedList). * When you need to process a text, get one copy of the pipeline from the pool, and return it to the pool when you have ﬁnished processing. Alternatively you can use the Spring Framework as described in the next section to handle the pooling for you. Spring poolingSpring集成GATE池资源管理 问题记录spring集成开发的plugin如何在gate中运行由于插件最终需在gate框架中运行，建议以maven进行项目包/结构管理，fatjar编译打包； 参考Lang_Chinese插件修改gapp(根据creole.xml)，新增PR配置gapp参考配置文件参考 12345678910111213141516171819202122232425262728293031&lt;?xml version="1.0"?&gt;&lt;!-- Geocoding 地址解析 --&gt;&lt;CREOLE-DIRECTORY&gt; &lt;JAR SCAN="true"&gt;gto.geocoding.jar&lt;/JAR&gt; &lt;JAR&gt;lib/util-1.2.jar&lt;/JAR&gt; &lt;JAR&gt;lib/uadb.trext-1.2.jar&lt;/JAR&gt; &lt;CREOLE&gt; &lt;RESOURCE&gt; &lt;NAME&gt;GeocodingProcesser&lt;/NAME&gt; &lt;COMMENT&gt;地址正向解析&lt;/COMMENT&gt; &lt;HELPURL&gt;http://gisc.chzu.edu.cn/mysnspace/geocoding&lt;/HELPURL&gt; &lt;CLASS&gt;gto.geocoding.processor.GeocodingProcessMain&lt;/CLASS&gt; &lt;PARAMETER NAME="inputASName" RUNTIME="true" DEFAULT="GTO" COMMENT="The annotation set to be used as input for the transducer" OPTIONAL="false"&gt;java.lang.String &lt;/PARAMETER&gt; &lt;PARAMETER NAME="outputASName" RUNTIME="true" DEFAULT="GTO" COMMENT="The annotation set to be used as output for the transducer" OPTIONAL="true"&gt;java.lang.String &lt;/PARAMETER&gt; &lt;PARAMETER NAME="geocodingMethod" RUNTIME="true" DEFAULT="LT" COMMENT="The GeocodingMethodEnum to geocoding" OPTIONAL="false"&gt;java.lang.String &lt;/PARAMETER&gt; &lt;PARAMETER NAME="geocodingUrl" RUNTIME="true" DEFAULT="http://192.168.1.52:8080/uadb.app/rest/v100/dz/geocoding" COMMENT="The geocoding url" OPTIONAL="false"&gt;java.lang.String &lt;/PARAMETER&gt; &lt;/RESOURCE&gt; &lt;/CREOLE&gt;&lt;/CREOLE-DIRECTORY&gt; 将插件复制于GATE\plugins根目录内； 在gate中手动配置PR顺序，执行标注无误后，另存为gappurlList新增URLHolder 123&lt;gate.util.persistence.PersistenceManager-URLHolder&gt; &lt;urlString&gt;$relpath$../../../../gto.geocoding/&lt;/urlString&gt;&lt;/gate.util.persistence.PersistenceManager-URLHolder&gt; CollectionPersistence新增PRPersistence 123456789101112131415161718192021222324252627282930313233&lt;gate.util.persistence.PRPersistence&gt; &lt;runtimeParams class="gate.util.persistence.MapPersistence"&gt; &lt;mapType&gt;gate.util.SimpleFeatureMapImpl&lt;/mapType&gt; &lt;localMap&gt; &lt;entry&gt; &lt;string&gt;inputASName&lt;/string&gt; &lt;string&gt;GTO&lt;/string&gt; &lt;/entry&gt; &lt;entry&gt; &lt;string&gt;outputASName&lt;/string&gt; &lt;string&gt;GTO&lt;/string&gt; &lt;/entry&gt; &lt;entry&gt; &lt;string&gt;geocodingMethod&lt;/string&gt; &lt;string&gt;LT&lt;/string&gt; &lt;/entry&gt; &lt;entry&gt; &lt;string&gt;geocodingUrl&lt;/string&gt; &lt;string&gt;http://192.168.1.52:8080/uadb.app/rest/v100/dz/geocoding&lt;/string&gt; &lt;/entry&gt; &lt;/localMap&gt; &lt;/runtimeParams&gt; &lt;resourceType&gt;gto.geocoding.processor.GeocodingProcessMain&lt;/resourceType&gt; &lt;resourceName&gt;GeocodingProcess&lt;/resourceName&gt; &lt;initParams class="gate.util.persistence.MapPersistence"&gt; &lt;mapType&gt;gate.util.SimpleFeatureMapImpl&lt;/mapType&gt; &lt;localMap/&gt; &lt;/initParams&gt; &lt;features class="gate.util.persistence.MapPersistence"&gt; &lt;mapType&gt;gate.util.SimpleFeatureMapImpl&lt;/mapType&gt; &lt;localMap/&gt; &lt;/features&gt; &lt;/gate.util.persistence.PRPersistence&gt; Jface运行GATE环境路径设置问题问题：GATE环境，以Fatjar打包JFace程序，错误日志： 123456789Could not check plugin-mappings.xmljava.net.MalformedURLException: no protocol: plugin-mappings.xmlgate.util.GateException: java.net.MalformedURLException: no protocol: creole.xml gate.creole.ResourceInstantiationException:Couldnot get resource data for gate.corpora.CorpusImpl.You may need first to load the plugin that contains your resource.For example, to create a gate.creole.tokeniser.DefaultTokeniseryou need first to load the ANNIE plugin.Go to the menu File-&gt;Manage CREOLE plugins or use the methodGate.getCreoleRegister().registerDirectories(pluginDirectoryURL) 解决：路径问题，单斜杠改成双斜杠 12345//错误System.setProperty("gate.builtin.ic.dir", new File(gateDir + "/resources/creole").toURL().toURI().toString());//正确String builtinDir = new File(gateDir + "\\resources\\creole").toURI().toURL().toString();System.setProperty("gate.builtin.creole.dir", builtinDir); gate插件加载错误 错误日志 12gate.creole.ResourceInstantiationException: Could not get resource data for gate.creole.gazetteer.DefaultGazetteer.gate.creole.gazetteer.DefaultGazetteer can be found in the ANNIE plugin 问题定位：需要在GATE中注册ANNIE插件，初始化后千万别忘得了！ 解决方案： 123456789101112131415161718192021222324252627private static boolean initialized = false;String gateDir = System.getProperty("user.dir") + "/src/main/resources/gate";@Beforepublic void startup() &#123;System.out.println("---startup");if (!initialized) &#123; // 初始化GATE环境 gateCommonService.setGateDir(gateDir); gateCommonService.initGate(); //注册插件 gateCommonService.registerGatePlugins(new String[] &#123;"ANNIE", "Lang_Chinese"&#125;); initialized = true;&#125;&#125;public void registerGatePlugins(String[] pluginNames) &#123; for (String pluginName : pluginNames) &#123; try &#123; URL pathUrl = getGatePluginPath(pluginName); Gate.getCreoleRegister().registerDirectories(pathUrl); &#125; catch (GateException e) &#123; e.printStackTrace(); &#125; &#125;&#125; Linux中gate初始化错误 错误日志 123Using file:/uadb/uadb.etl.pre/resources/gate/%5Cresources%5Ccreole/ as built-in CREOLE directory URL 2015-11-10 22:08:26,255 [main] [gate.creole.CreoleRegisterImpl] [WARN] - Could not check plugin-mappings.xml java.io.FileNotFoundException: /uadb/uadb.etl.pre/resources/gate/\resources\creole/plugin-mappings.xml (No such file or directory) 问题定位：window下用的\和\符号在linux下和unix下不可用 解决方案：路径符号由\改成// Spring集成GATE，Fatjar打包后的标注乱码问题 问题描述：main和junit测试标注正常，但fatjar打包后的程序业务标注未生成 应标注：[Number, Token, Lookup, SpaceToken, Split, Sentence, IKAToken, AtomToken, Context, Place, Mapcode, Location, Region,Scale, Entity] 实际标注：[Split, Token, SpaceToken, DEFAULT_TOKEN, Sentence, IKAToken,AtomToken] 问题定位chineseNE子管道配置的标注有问题，多出个DEFAULT_TOKEN，Sentence正常标注，但未生成Number和Lookup标注， 解决方案gate环境用spring集成，程序是调用gapp执行标注piopeline，jape和gazetteer等资源都是外部化的，为什么不行？进行各种测试排除，发现输出日志中错误的都是乱码，断定是读文件的编码问题！！！这种问题，解决过程中纠结的要死，解决了感觉自己绝逼傻透了！想起那首记忆深刻的程序员之诗，手持两把锟斤拷，口中直呼烫烫烫，脚踏千朵屯屯屯，笑看万物锘锘锘 1234//错误Document doc = Factory.newDocument(file.toURI().toURL());//正确Document doc = Factory.newDocument(file.toURI().toURL(), "UTF-8"); Gazetteer问题 问题描述：Correct format for gazetteer entry features is: entry* 解决：gazetteer辞典中不能带有冒号，会识别成list.def然后抛错]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>NLP</tag>
        <tag>GATE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven使用笔记]]></title>
    <url>%2F2015%2F09%2F15%2FMaven%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[关于MavenMaven是基于项目对象模型(POM)，可以通过一小段描述信息来管理项目的构建，报告和文档的软件项目管理工具。 本文记录Maven开发过程中常用的脚本和遇到的问题。 Maven Dependency在POM 4中，中还引入了，它主要管理依赖的部署。目前可以使用5个值： * compile，缺省值，适用于所有阶段，会随着项目一起发布。 * provided，类似compile，期望JDK、容器或使用者会提供这个依赖。如servlet.jar。 * runtime，只在运行时使用，如JDBC驱动，适用运行和测试阶段。 * test，只在测试时使用，用于编译和运行测试代码。不会随项目发布。 * system，类似provided，需要显式提供包含依赖的jar，Maven不会在Repository中查找它。 Maven常用指令package：打包war:exploded：编译不生成war包install：安装到本地资源库 eg：mvn install:install-file -Dfile=lt.util-1.0.jar -DgroupId=com.lt -DartifactId=util -Dversion=1.0 -Dpackaging=jarprocess-resources：编译并打包资源新建项目：mvn archetype:generate -DgroupId=com.lt -DartifactId=uadb.etl -DarchetypeArtifactIdmaven-archetype-webapp -DinteractiveMode=false maven dependency exclusionmaven远程库 https://search.maven.org/#search http://maven-repository.com/ http://mvnrepository.com/ http://repository.apache.org/snapshots/ http://maven.outofmemory.cn/ 当然，在国内还是老实参考开源中国社区的教程配置maven1234567891011121314 &lt;mirrors&gt;true&lt;mirror&gt;truetrue&lt;id&gt;nexus-osc&lt;/id&gt;truetrue&lt;mirrorOf&gt;central&lt;/mirrorOf&gt;truetrue&lt;name&gt;Nexus osc&lt;/name&gt;truetrue&lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;true&lt;/mirror&gt;true&lt;mirror&gt;truetrue&lt;id&gt;nexus-osc-thirdparty&lt;/id&gt;truetrue&lt;mirrorOf&gt;thirdparty&lt;/mirrorOf&gt;truetrue&lt;name&gt;Nexus osc thirdparty&lt;/name&gt;truetrue&lt;url&gt;http://maven.oschina.net/content/repositories/thirdparty/&lt;/url&gt;true&lt;/mirror&gt;&lt;/mirrors&gt; maven依赖离线下载-1maven 上传项目到Maven Central RepositoryMaven本地开发 下载apache-maven 设置本地库路径在maven\config\settings.xml中设置本地库路径：&lt;localRepository&gt;D:\Dev\Maven-Respository&lt;/localRepository&gt; 在Myeclipse中设置安装路径在Window&gt;Preferences&gt;Myeclipse&gt;Maven4Myeclipse&gt;Installations中执行Add加入本地maven路径在Window&gt;Preferences&gt;Myeclipse&gt;Maven4Myeclipse&gt;User Settings中Browser选择maven\config\settings.xml，执行Update Settings，Reindex maven引用本地jar包假设将包htmlparser.jar放入了项目下的lib目录中 -&gt; ${project}/lib/htmlparser.jar,则pom.xml中应该配置如下：1234567&lt;dependency&gt; &lt;groupId&gt;com.htmlparser&lt;/groupId&gt; &lt;artifactId&gt;htmlparser&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;project.basedir&#125;/lib/htmlparser.jar&lt;/systemPath&gt; &lt;/dependency&gt; Maven项目聚合为解决多个依赖项目自动打包，可通过聚合maven项目解决pom示例12345678910111213141516171819202122&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;true&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;true&lt;groupId&gt;com.lt.util&lt;/groupId&gt;true&lt;artifactId&gt;util.aggregation&lt;/artifactId&gt;true&lt;version&gt;0.0.1&lt;/version&gt;true&lt;packaging&gt;pom&lt;/packaging&gt;true&lt;name&gt;util.aggregation&lt;/name&gt;true&lt;properties&gt;truetrue&lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;true&lt;/properties&gt;true&lt;modules&gt;truetrue&lt;module&gt;../util.common&lt;/module&gt;truetrue&lt;module&gt;../util.jdbc&lt;/module&gt;truetrue&lt;module&gt;../util.web&lt;/module&gt;truetrue&lt;module&gt;../util.geo&lt;/module&gt;truetrue&lt;module&gt;../util.hibernate&lt;/module&gt;true&lt;/modules&gt;&lt;/project&gt; 通过mvn clean install进行打包 Maven项目依赖继承uadb.parent项目1234567891011121314151617181920212223242526272829303132&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;true&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;true&lt;groupId&gt;com.lt.uadb&lt;/groupId&gt;true&lt;artifactId&gt;uadb.parent&lt;/artifactId&gt;true&lt;version&gt;0.0.1&lt;/version&gt;true&lt;packaging&gt;pom&lt;/packaging&gt;true&lt;name&gt;uadb.parent&lt;/name&gt;true&lt;!-- 项目属性 --&gt;true&lt;properties&gt;truetrue&lt;!-- framework --&gt;truetrue&lt;jdk.version&gt;1.7&lt;/jdk.version&gt;truetrue&lt;!-- test --&gt;truetrue&lt;junit.version&gt;4.8.2&lt;/junit.version&gt;truetrue&lt;!-- encode --&gt;truetrue&lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;true&lt;/properties&gt;true&lt;dependencyManagement&gt;truetrue&lt;dependencies&gt;truetruetrue&lt;!--test --&gt;truetruetrue&lt;dependency&gt;truetruetruetrue&lt;groupId&gt;junit&lt;/groupId&gt;truetruetruetrue&lt;artifactId&gt;junit&lt;/artifactId&gt;truetruetruetrue&lt;version&gt;$&#123;junit.version&#125;&lt;/version&gt;truetruetruetrue&lt;scope&gt;test&lt;/scope&gt;truetruetrue&lt;/dependency&gt;truetrue&lt;/dependencies&gt;true&lt;/dependencyManagement&gt;&lt;/project&gt; child项目引用parent项目12345678910111213141516171819202122232425262728293031&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;true&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;true&lt;artifactId&gt;uadb.etl.pre&lt;/artifactId&gt;true&lt;version&gt;1.0&lt;/version&gt;true&lt;name&gt;uadb.etl.pre&lt;/name&gt;true&lt;packaging&gt;jar&lt;/packaging&gt;true&lt;parent&gt;truetrue&lt;groupId&gt;com.lt.uadb&lt;/groupId&gt;truetrue&lt;artifactId&gt;uadb.parent&lt;/artifactId&gt;truetrue&lt;version&gt;0.0.1&lt;/version&gt;true&lt;/parent&gt;true&lt;!-- 项目属性 --&gt;true&lt;properties&gt;truetrue&lt;!-- framework --&gt;truetrue&lt;jdk.version&gt;1.7&lt;/jdk.version&gt;truetrue&lt;!-- test --&gt;truetrue&lt;junit.version&gt;4.8.2&lt;/junit.version&gt;truetrue&lt;!-- encode --&gt;truetrue&lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;true&lt;/properties&gt;true&lt;dependencies&gt;truetruetrue&lt;!--test --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt;true&lt;/dependencies&gt;&lt;/project&gt; 复制依赖项dependency:copy-dependencies输出到target/deps 在Github搭建个人Maven仓库deploy到本地目录本地目录提交到gtihub上配置github地址为仓库地址Maven问题记录 本地库有改jar包但是依旧无法编译本地下载的Zip已损坏，删除本地库中的jar文件和目录，重新从远处库下载 远程库只能下载索引，不能下载jar！日志：The container ‘Maven Dependencies’ references non existing library ‘D:\soft\Maven\Maven-Respository\org\apache\ant\ant\1.9.3\ant-1.9.3.jar’解决：So I get you are using Eclipse with the M2E plugin. Try to update your Maven configuration : In the Project Explorer, right-click on the project, Maven -&gt; Update project.If the problem still remains, try to clean your project: right-click on your pom.xml, Run as -&gt; Maven build (the second one). Enter “clean package” in the Goals fields. Check the Skip Tests box. Click on the Run button. cannot find maven installation embeddedSimply remove the external maven installation. When you restart eclipse, the embedded maven will reappear. install offline问题The repository system is offline but the artifact org.apache.maven.plugins:maven-install-plugin:pom:2.3.1 is not available in the local repository.取消勾选offline选项,重新执行install]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>J2EE</tag>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQLite数据库使用笔记]]></title>
    <url>%2F2015%2F09%2F15%2FSQLite%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[SQLite特点SQLite只支持库级锁，同时只能允许一个写操作。但SQLite尽量延迟申请X锁，直到数据块真正写盘时才申请X锁，非常巧妙而有效。 SQLite支持3种线程模式:单线程,多线程,串行 可使用WAL（Write-Ahead Logging）模式时，写操作是append到WAL文件，而不直接改动数据库文件，因此数据库文件可以被同时读取。当执行checkpoint操作时，WAL文件的内容会被写回数据库文件。当WAL文件达到SQLITE_DEFAULT_WAL_AUTOCHECKPOINT（默认值是1000）页（默认大小是1KB）时，会自动使用当前COMMIT的线程来执行checkpoint操作。也可以关闭自动checkpoint，改为手动定期checkpoint。jdbc可通过setJournalMode(JournalMode.WAL)/setJounalSizeLimit实现 事务是和数据库连接相关的，每个数据库连接（使用pager来）维护自己的事务，且同时只能有一个事务（但是可以用SAVEPOINT来实现内嵌事务）。 官方文档sqlite在多线程下的应用 WAL模式 -shm文件包含-wal文件的数据索引，-shm文件提升-wal文件的读性能 如果-shm文件被删除，下次数据库连接时会自动新建一个-shm文件 如果执行了checkpoint命令，-war文件可以删除 VACUUM命令VACUUM命令用于重建数据库文件， 执行VACUUM 时，会拷贝整个数据库到Transient databases临时文件中，然后覆盖写回到原来的数据库文件中。写回过程中会创建rollback journal or write-ahead log WAL file以保证transaction atomic。当vacuum执行完毕，临时文件被删除。 重建数据库文件的原因有以下几点 当大量数据被删除后，数据库文件中会有很多空块,空页和碎片，VACUUM rebuild数据库文件，移除这些空块，减少所占的磁盘空间 频繁的inserts, updates, and deletes 导致数据库文件中很多碎片，VACUUM 重建数据库文件使得表，索引连续的存储, 减少空闲页， 减少所占的磁盘空间 当page_size 或用pragma auto_vacuum 命令修改这两个值时， SQLite会自动执行VACUMM VACUUM只对main数据库有效，对ATTACHED数据库无效 如果数据库中还有其他transaction， VACUUM将执行失败 除了使用VACUUM外，还可以使用PRAGMA auto_vacuum控制vacuum的执行12PRAGMA auto_vacuum;PRAGMA auto_vacuum = 0 | NONE | 1 | FULL | 2 | INCREMENTAL; synchronous参数123PRAGMA synchronous = FULL; (2)PRAGMA synchronous = NORMAL; (1)PRAGMA synchronous = OFF; (0) FULL当synchronous设置为FULL (2), SQLite数据库引擎在紧急时刻会暂停以确定数据已经写入磁盘。这使系统崩溃或电源出问题时能确保数据库在重起后不会损坏。FULL synchronous很安全但很慢。 NORMAL当synchronous设置为NORMAL, SQLite数据库引擎在大部分紧急时刻会暂停，但不像FULL模式下那么频繁。 NORMAL模式下有很小的几率(但不是不存在)发生电源故障导致数据库损坏的情况。但实际上，在这种情况 下很可能你的硬盘已经不能使用，或者发生了其他的不可恢复的硬件错误。 OFF设置为synchronous OFF (0)时，SQLite在传递数据给系统以后直接继续而不暂停。若运行SQLite的应用程序崩溃， 数据不会损伤，但在系统崩溃或写入数据时意外断电的情况下数据库可能会损坏。另一方面，在synchronous OFF时 一些操作可能会快50倍甚至更多。在SQLite 2中，缺省值为NORMAL.而在3中修改为FULL。 www.2cto.com 建议：如果有定期备份的机制，而且少量数据丢失可接受，用OFF。 问题记录提交-wal修改到数据库main文件执行VACUUM命令即可生成最新的数据库-db文件 如何删除使用中的SQLite数据库参考 1234// 添加System.gc()和Thread.sleep进行强制删除 System.gc();Thread.sleep(1000);FileDeleteStrategy.FORCE.delete(workFile); SQLite开启WAL读写模式1234SQLiteConfig config = new SQLiteConfig();config.setOpenMode(SQLiteOpenMode.READWRITE);config.setJournalMode(JournalMode.WAL); dataSource.setConfig(config); SQLite批量更新123456789101112131415161718192021 /** * 批量更新 * * @param updateSqlList */ public void batchUpdate(List&lt;String&gt; updateSqlList) &#123;if (updateSqlList.size() &gt; 0) &#123; try &#123; trueConnection conn = dataSource.getConnection();trueStatement statement = conn.createStatement();truefor (String sql : updateSqlList) &#123;true statement.addBatch(sql);true&#125;trueint[] count = statement.executeBatch();truelog.info("SQLite-JDBC批量更新&#123;&#125;条", count.length); &#125; catch (SQLException e) &#123;truee.printStackTrace(); &#125;&#125; &#125; sqlite除法运算保留小数问题12345select distinct 1/100 from 兴趣点# 结果：0select distinct cast(1 as real)/100 from 兴趣点# 结果：0.01 sqlite存储number型时小于0的值会以0存储sqlite3.8-shell连接数据库cd /usr/local/sqlite &amp;&amp; sqlite3 /uadb/data/geocodingdb.db Cannot change read-only flag after establishing a connection日志：[org.hibernate.engine.jdbc.spi.SqlExceptionHelper] [ERROR] - Cannot change read-only flag after establishing a connection. Use SQLiteConfig#setReadOnly and SQLiteConfig.createConnection().解决：]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>J2EE</tag>
        <tag>SQLite</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP学习笔记]]></title>
    <url>%2F2015%2F09%2F11%2FNLP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[记录一些NLP学习过程中的专业名词 专业名词 NLP(Natural Language Processing)：自然语言处理，主要涉及计算机处理人类语言的数据结构和算法的计算科学。 Ontology本体：本体是一种描述术语（包含哪些词汇）及术语间关系（描述苹果、香蕉、水果之间的关系）的概念模型。符号到本体的某种映射本体是表达概念之间关系的有效手段，它是共享概念模型的明确的形式化规范说明，它在共享范围内描述了领域中的概念及概念之间的关系，使其具有明确的、形式化的定义,从而实现人机之间以及机器之间的信息交互、知识共享与重用。 FSM(Finite-state machine)有限状态机：称状态机，是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。 GATE(General Architecture for Text Engineering)文本工程通用框架 IR(Information Resolve)信息检索 IE(Information Extract)信息抽取 HMM(Hidden Markov Models)隐马尔科夫模型：一个隐马尔科夫模型它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。 由一个向量和两个矩阵(pi,A,B)描述的隐马尔科夫模型对于实际系统有着巨大的价值，虽然经常只是一种近似，但它们却是经得起分析的。隐马尔科夫模型通常解决的问题包括： 对于一个观察序列匹配最可能的系统——评估，使用前向算法（forward algorithm）解决； 对于已生成的一个观察序列，确定最可能的隐藏状态序列——解码，使用Viterbi 算法（Viterbi algorithm）解决； 对于已生成的观察序列，决定最可能的模型参数——学习，使用前向-后向算法（forward-backward algorithm）解决。HMM扩展阅读 GATE专业名词 LR(Language Resource)：语言资源，与数据相关的资源，比如词典、文档和本体(Ontology)等。其中一些语言组件需要和软件搭配使用（比如，WordNet 使用了 C 和 Prolog语言的 API 的用户查询接口），虽然其中涉及了软件，但是由于 API 也是为语言资源服务的，所以我们仍然把这些资源定义为语言组件。 PR(Processing Resource)：处理资源，表示主要算法实体，如，解析算法，生成算法或n-元模型（ngram）建模算法。 VR（Pisual Resource）: 可视化资源，指构成 GATE 的可视化界面 GUI 的相关资源。 CREOLE(a Collection of Reusable Objects for Language Engineering)可重用语言引擎对象集合，提供了文本解析、文本抽取、结果测算等众多插件 ANNIE Corpus 语料库，文档的集合 行业名词 GTO：geography text ontology iCERS ：Integrated Crime Emergency Response System Integrated Crime Emergency Response System (iCERS) is a large-scale spatio-temporal system which integrates all sorts of crime emergency service resources and majors its features as common codes used for public emergency events reporting. CE2M ：Crime Emergency Event Model The ontology for Crime Emergency Event Model (CE2M) is recommended as an effective means to implement semantic level integration. CE2M is stratified into three levels: Event, Process and Action. CE2M constructs the vocabulary and the common model for exchange of iCERS information, thus it becomes the common comprehension of each business subsystems. 相关阅读 《数学之美》科普阅读 《统计自然语处理基础》阅读中… 《概率论》]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Solr的Hbase二级索引]]></title>
    <url>%2F2015%2F09%2F01%2F%E5%9F%BA%E4%BA%8ESolr%E7%9A%84Hbase%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[关于Hbase二级索引HBase 是一个列存数据库，每行数据只有一个主键RowKey，无法依据指定列的数据进行检索。查询时需要通过RowKey进行检索，然后查看指定列的数据是什么，效率低下。在实际应用中，我们经常需要根据指定列进行检索，或者几个列进行组合检索，这就提出了建立 HBase 二级索引的需求。 二级索引构建方式：表索引、列索引、全文索引 表索引是将索引数据单独存储为一张表，通过 HBase Coprocessor 生成并访问索引数据。 列索引是将索引数据与源数据存储在相同的 Region 里，索引数据定义为一个单独的列族，也是利用 Coprocessor 来生成并访问索引数据。对于表索引，源数据表与索引表的数据一致性很难保证，访问两张不同的表也会增加 IO 开销和远程调用的次数。对于列索引，单表的数据容量会急剧增加，对同一 Region 里的多个列族进行 Split 或 Merge 等操作时可能会造成数据丢失或不一致。 全文索引：以CDH5中的Lily HBase Indexer服务实现，其使用SolrCloud存储HBase的索引数据，Indexer索引和搜索不会影响HBase运行的稳定性和HBase数据写入的吞吐量，因为索引和搜索过程是完全分开并且异步的。Lily HBase Indexer在CDH5中运行必须依赖HBase、SolrCloud和Zookeeper服务。 关于Key-Value Indexer组件CDH官方文档hbase-indexer官方WIKI参考博客：Email Indexing Using Cloudera Search and HBase参考博客： Cloudera Search Solr初探参考博客：一种基于UDH Search的HBase二级索引构建方案 CDH5.4中的Key-Value Indexer使用的是Lily HBase NRT Indexer服务，Lily HBase Indexer是一款灵活的、可扩展的、高容错的、事务性的，并且近实时的处理HBase列索引数据的分布式服务软件。它是NGDATA公司开发的Lily系统的一部分，已开放源代码。Lily HBase Indexer使用SolrCloud来存储HBase的索引数据，当HBase执行写入、更新或删除操作时，Indexer通过HBase的replication功能来把这些操作抽象成一系列的Event事件，并用来保证写入Solr中的HBase索引数据的一致性。并且Indexer支持用户自定义的抽取，转换规则来索引HBase列数据。Solr搜索结果会包含用户自定义的columnfamily:qualifier字段结果，这样应用程序就可以直接访问HBase的列数据。而且Indexer索引和搜索不会影响HBase运行的稳定性和HBase数据写入的吞吐量，因为索引和搜索过程是完全分开并且异步的。Lily HBase Indexer在CDH5中运行必须依赖HBase、SolrCloud和Zookeeper服务。 使用 Lily HBase Batch Indexer 进行索引借助 Cloudera Search，您可以利用 MapReduce 作业对 HBase 表进行批量索引。批量索引不需要以下操作： HBase 复制 Lily HBase Indexer 服务 通过 Lily HBase Indexer 服务注册 Lily HBase Indexer 配置该索引器支持灵活的、自定义的、特定于应用程序的规则来将 HBase 数据提取、转换和加载到 Solr。Solr 搜索结果可以包含到存储在 HBase 中的数据的 columnFamily:qualifier 链接。这样，应用程序可以使用搜索结果集直接访问匹配的原始 HBase 单元格。 创建HBase集群的表中列索引的步骤：Tutorial教程 填充 HBase 表。 创建相应的 SolrCloud 集合 创建 Lily HBase Indexer 配置 创建 Morphline 配置文件 注册 Lily HBase Indexer Configuration 和 Lily HBase Indexer Service 填充 HBase 表在配置和启动系统后，创建 HBase 表并向其添加行。例如：对于每个新表，在需要通过发出格式命令进行索引的每个列系列上设置 REPLICATION_SCOPE： 123456789101112131415161718192021222324$ hbase shell #测试数据：列簇设置REPLICATION_SCOPEdisable 'User'drop 'User'create 'User', &#123;NAME =&gt; 'data', REPLICATION_SCOPE =&gt; 1&#125; disable 'User'alter 'User', &#123;NAME =&gt; 'detail', REPLICATION_SCOPE =&gt; 1&#125; enable 'User'#新增CFdisable 'User'alter 'User', &#123;NAME =&gt; 'detail', REPLICATION_SCOPE =&gt; 1&#125; enable 'User'#修改现有disable 'User'alter 'User', &#123;NAME =&gt; 'data', REPLICATION_SCOPE =&gt; 1&#125; enable 'User'# 插入测试数据 put 'User','row1','data:name','u1'put 'User','row1','data:psd','123' 创建相应的 SolrCloud 集合用于 HBase 索引的 SolrCloud 集合必须具有可容纳 HBase 列系列的类型和要进行索引处理的限定符的 Solr 架构。若要开始，请考虑将包括一切 data 的字段添加到默认schema。一旦您决定采用一种schema，使用以下表单命令创建 SolrCloud 集合： user示例配置12# 生成实体配置文件：solrctl instancedir --generate $HOME/hbase-indexer/User 编辑schema，需包含以下内容vim $HOME/hbase-indexer/User/conf/schema.xml 1234567&lt;!-- 绑定rowkey--&gt;&lt;field name="id" type="string" indexed="true" stored="true" required="true" multiValued="false" /&gt; &lt;field name="name" type="string" indexed="true" stored="true"/&gt;&lt;field name="psd" type="string" indexed="true" stored="true"/&gt; &lt;field name="address" type="string" indexed="true" stored="true"/&gt; &lt;field name="photo" type="string" indexed="true" stored="true"/&gt; 1234# 创建 collection实例并将配置文件上传到 zookeeper：solrctl instancedir --create User $HOME/hbase-indexer/User# 上传到 zookeeper 之后，其他节点就可以从zookeeper下载配置文件。接下来创建 collection:solrctl collection --create User 注意在schema.xml中uniqueKey必须为rowkey,而rowkey默认使用’id’字段表示，中必须要有uniqueKey对应的id字段。 创建 Lily HBase Indexer 配置Indexer-configuration官方参考在HBase-Solr的安装目录/usr/lib/hbase-solr/下，创建morphline-hbase-mapper.xml文件，文件内容如下： $ vim $HOME/hbase-indexer/morphline-hbase-mapper.xml 123456789101112&lt;?xml version="1.0"?&gt; &lt;!-- table：需要索引的HBase表名称--&gt; &lt;!-- mapper：用来实现和读取指定的Morphline配置文件类，固定为MorphlineResultToSolrMapper--&gt;&lt;indexer table="User" mapper="com.ngdata.hbaseindexer.morphline.MorphlineResultToSolrMapper"&gt; &lt;!--param中的name参数用来指定当前配置为morphlineFile文件 --&gt; &lt;!--value用来指定morphlines.conf文件的路径，绝对或者相对路径用来指定本地路径，如果是使用Cloudera Manager来管理morphlines.conf就直接写入值morphlines.conf"--&gt; &lt;param name="morphlineFile" value="morphlines.conf"/&gt; &lt;!-- The optional morphlineId identifies a morphline if there are multiple morphlines in morphlines.conf --&gt; &lt;param name="morphlineId" value="userMap"/&gt;&lt;/indexer&gt; 注意：当使用绝对或者相对路径来指定路径时，集群中的其它机器也要在配置路径上有该文件，如果是通过Cloudera Manager管理的话只需要在CM中修改后即可，CM会自动分发给集群。当然该配置文件还有很多其它参数可以配置，扩展阅读。 创建 Morphline 配置文件Morphlines是一款开源的，用来减少构建hadoop ETL数据流程时间的应用程序。它可以替代传统的通过MapReduce来抽取、转换、加载数据的过程，提供了一系列的命令工具，具体可以参见：http://kitesdk.org/docs/0.13.0/kite-morphlines/morphlinesReferenceGuide.html。 对于HBase的其提供了extractHBaseCells命令来读取HBase的列数据。我们采用Cloudera Manager来管理morphlines.conf文件，使用CM来管理morphlines. conf文件除了上面提到的好处之外，还有一个好处就是当我们需要增加索引列的时候，如果采用本地路径方式将需要重新注册Lily HBase Indexer的配置文件，而采用CM管理的话只需要修改morphlines.conf文件后重启Key-Value HBase Indexer服务即可。具体操作为：进入Key-Value Store Indexer面板-&gt;配置-&gt;服务范围-&gt;Morphlines-&gt;Morphlines文件。在该选项加入如下配置： 注意：每个Collection对应一个morphline-hbase-mapper.xml $ vim /$HOME/morphlines.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950SOLR_LOCATOR : &#123; # Name of solr collection collection : collection # ZooKeeper ensemble zkHost : "$ZK_HOST"&#125;morphlines : [&#123;id : morphlineimportCommands : ["org.kitesdk.**", "com.ngdata.**"]commands : [ &#123; extractHBaseCells &#123; mappings : [ &#123; inputColumn : "data:name" outputField : "data_name" type : string source : value &#125;, &#123; inputColumn : "data:psd" outputField : "data_psd" type : string source : value &#125;, &#123; inputColumn : "data:address" outputField : "data_address" type : string source : value &#125;, &#123; inputColumn : "data:photo" outputField : "data_photo" type : string source : value &#125; ] &#125; &#125; &#123; logDebug &#123; format : "output record: &#123;&#125;", args : ["@&#123;&#125;"] &#125; &#125;]&#125;] 注册 Lily HBase Indexer Configuration 和 Lily HBase Indexer Service当 Lily HBase Indexer 配置 XML文件的内容令人满意，将它注册到 Lily HBase Indexer Service。上传 Lily HBase Indexer 配置 XML文件至 ZooKeeper，由给定的 SolrCloud 集合完成此操作。例如： 123456hbase-indexer add-indexer \--name userIndexer \--indexer-conf $HOME/hbase-indexer/User/conf/morphline-hbase-mapper.xml \--connection-param solr.zk=server1:2181/solr \--connection-param solr.collection=User \--zookeeper server1:2181 验证索引器是否已成功创建执行$ hbase-indexer list-indexers验证索引器是否已成功创建 更多帮助，请使用以下命令： 1234hbase-indexer add-indexer --helphbase-indexer list-indexers --helphbase-indexer update-indexer --helphbase-indexer delete-indexer --help 测试是solr是否已新建索引写入数据时，在solr-webui控制台查看日志是否更新 12345678910111213141516171819put 'User','row1','data','u1'put 'User','row1','data:name','u2'put 'User','row2','data:name','u2'put 'User','row2','data:psd','123' put 'User','row2','data:address','address2' put 'User','row2','data:photo','photo2' put 'User','row2','data:name','u2'put 'User','row2','data:psd','123' put 'User','row2','detail:address','address2' put 'User','row2','detail:photo','photo2' put 'User','row3','data:name','u2'put 'User','row3','data:psd','123' put 'User','row3','detail:address','江苏省南京市' put 'User','row3','detail:photo','phto3' 折腾几天弄好，下一步是如何以构建好的索引Hbase实现多列条件的组合查询。 扩展命令1234567891011121314151617181920212223# solrctlsolrctl instancedir --list solrctl collection --list # 更新coolection配置solrctl instancedir --update User $HOME/hbase-indexer/Usersolrctl collection --reload User#删除instancedirsolrctl instancedir --delete User#删除collectionsolrctl collection --delete User#删除collection所有docsolrctl collection --deletedocs User#删除User配置目录rm -rf $HOME/hbase-indexer/User# hbase-indexer# 若修改了morphline-hbase-mapper.xml，需更新索引hbase-indexer update-indexer -n userIndexer # 删除索引hbase-indexer delete-indexer -n userIndexer 所遇问题QALily HBase Indexer Service注册错误详细日志123456[WARN ][08:56:49,677][.com:2181)] org.apache.zookeeper.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnectjava.net.ConnectException: Connection refused at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) 解决：将帮助文档原文中的-zookeeper hbase-cluster-zookeeper:2181中hbase-cluster-zookeeper换成zoomkeeper的主机名 schema.xm和morphline.conf配置问题12ERROR org.apache.solr.common.SolrException: ERROR: [doc=row3] unknown field 'data'trueat org.apache.solr.update.DocumentBuilder.toDocument(DocumentBuilder.java:185) 解决方式：Thanks for the response. In the meantime I got a solution which is fine for me using: But type=”ignored” is a good hint once I want to get rid of the fields I do not need, thanks. – schema新增配置： 12&lt;dynamicField name="*" type="string" indexed="true" stored="true" /&gt;&lt;field name="data" type="string" indexed="true" stored="true" multiValued="true"/&gt; 修改schema.xml后，执行以下命令更新配置：solrctl instancedir –update hbase-collection-user $HOME/hbase-collection-usersolrctl collection –reload hbase-collection-user 修改Collection当我们创建Collection完成后，如果需要修改schema.xml文件重新配置需要索引的字段可以按如下操作： 如果是修改原有schema.xml中字段值，而在solr中已经插入了索引数据，那么我们需要清空索引数据集，清空数据集可以通过solr API来完成。 如果是在原有schema.xml中加入新的索引字段，那么可以跳过1，直接执行： 12solrctl instancedir --update solrtest $HOME/solrtest solrctl collection --reload solrtest 多个HbaseTable配置schema.xml和morphline.conf解决方式：email-schema示例 Q：morphline.conf和morphline-hbase-mapper.xml文件是否每个HbaseTable都要对应配置一个?A：每一个Hbase Table对应生成一个Solr的Collection索引，每个索引对应一个Lily HBase Indexer 配置文件morphlines.conf和morphline配置文件morphline-hbase-mapper.xml，其中morphlines.conf可由CDH的Key-Value Store Indexer控制台管理，以id区分 官方说明：12345Creating a Lily HBase Indexer configuration Individual Lily HBase Indexers are configured using the hbase-indexer command line utility. Typically, there is one Lily HBase Indexer configuration for each HBase table, but there can be as many Lily HBase Indexer configurations as there are tables and column families and corresponding collections in the SolrCloud. Each Lily HBase Indexer configuration is defined in an XML file such as morphline-hbase-mapper.xml. 对HBaseTable已有数据新建索引需要用到Lily HBase Indexer的批处理索引功能了 123456789101112131415sudo hadoop --config /etc/hadoop/conf \jar /usr/lib/hbase-solr/tools/hbase-indexer-mr-1.5-cdh5.4.4-job.jar \--conf /etc/hbase/conf/hbase-site.xml \-D 'mapred.child.java.opts=-Xmx500m' \--hbase-indexer-zk master:2181 \--collection hbase-collection-user \--hbase-indexer-name userIndexer \--hbase-indexer-file $HOME/hbase-collection-user/conf/morphline-hbase-mapper.xml \--go-live \``` 错误日志 ``` javaCaused by: java.io.IOException Can not find resource solrconfig.xml in classpath or /root/file:/tmp/hadoop-root/mapred/local/1441858645500/6a1a458e-35e2-4f66-82df-02795ba44e2c.solr.zip/collection1/conf]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>分布式</tag>
        <tag>CDH</tag>
        <tag>Hbase</tag>
        <tag>Solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一路见识的SQL/NOSQL数据库ORM]]></title>
    <url>%2F2015%2F08%2F16%2F%E4%B8%80%E8%B7%AF%E8%A7%81%E8%AF%86%E7%9A%84SQL-NOSQL%E6%95%B0%E6%8D%AE%E5%BA%93ORM%2F</url>
    <content type="text"><![CDATA[一路走来，关系数据库到非关系数据库，不觉已接触了不少的ORM框架也在这些ORM框架的基础上积累了一些通用的DAO，下面从 学习笔记（原理、优势、劣势） 和 个人总结（踩过的坑，注意事项） 两方面展开描述： LINQLINQ全称Language Integrated Query大学期间接触DotNet/C#一般会对LINQ有所了解，印象中是封装了DAO层的数据库连接，可以通过一些如select/where/group by等关键词以熟悉的代码形式进行数据CRUD操作下面是官方的LINQ的官方描述 12345LINQ to SQL 是 .NET Framework 3.5 版的一个组件，提供了用于将关系数据作为对象管理的运行时基础结构。在 LINQ to SQL 中，关系数据库的数据模型映射到用开发人员所用的编程语言表示的对象模型。当应用程序运行时，LINQ to SQL 会将对象模型中的语言集成查询转换为 SQL，然后将它们发送到数据库进行执行。当数据库返回结果时，LINQ to SQL 会将它们转换回您可以用您自己的编程语言处理的对象。使用 Visual Studio 的开发人员通常使用对象关系设计器，它提供了用于实现许多 LINQ to SQL 功能的用户界面。 后来DotNET没深入学习了，工作的技术选型都是J2EE和一敏捷脚本类（Python）的方案，不做太多总结 Spring-JDBC官方文档Spring Framework JDBC会封装处理JDBC底层的细节，让JDBC更友好，具体如下 Spring-JDBC自动处理的 Open connection. Prepare and execute the statement. Set up the loop to iterate through the results (if any). Process any exception. Handle transactions. Close the connection, statement and resultset.用户需设置的 Define connection parameters. Specify the SQL statement. Declare parameters and provide parameter values Do the work for each iteration. Spring-JDBC的使用概要 JdbcTemplate - 这是经典的也是最常用的Spring对于JDBC访问的方案。这也是最低级别的封装, 其他的工作模式事实上在底层使用了JdbcTemplate作为其底层的实现基础。JdbcTemplate在JDK 1.4以上的环境上工作得很好。 NamedParameterJdbcTemplate - 对JdbcTemplate做了封装，提供了更加便捷的基于命名参数的使用方式而不是传统的JDBC所使用的“?”作为参数的占位符。这种方式在你需要为某个SQL指定许多个参数时，显得更加直观而易用。该特性必须工作在JDK 1.4以上。 SimpleJdbcTemplate - 这个类结合了JdbcTemplate和NamedParameterJdbcTemplate的最常用的功能，同时它也利用了一些Java5的特性所带来的优势，例如泛型、varargs和autoboxing等，从而提供了更加简便的API访问方式。需要工作在Java 5以上的环境中。 SimpleJdbcInsert 和 SimpleJdbcCall - 这两个类可以充分利用数据库元数据的特性来简化配置。通过使用这两个类进行编程，你可以仅仅提供数据库表名或者存储过程的名称以及一个Map作为参数。其中Map的key需要与数据库表中的字段保持一致。这两个类通常和SimpleJdbcTemplate配合使用。这两个类需要工作在JDK 5以上，同时数据库需要提供足够的元数据信息。 RDBMS 对象包括MappingSqlQuery, SqlUpdate and StoredProcedure - 这种方式允许你在初始化你的数据访问层时创建可重用并且线程安全的对象。该对象在你定义了你的查询语句，声明查询参数并编译相应的Query之后被模型化。一旦模型化完成，任何执行函数就可以传入不同的参数对之进行多次调用。这种方式需要工作在JDK 1.4以上。 优势：强大、优雅、轻量、持续更新维护，与JDBC相对，减少了大量的冗余DAO层代码 setMaxRows和setFetchSizeThey do different things.The setMaxRows = number of rows that can be returned overall.setFetchSize = number that will be returned in each database roundtrip i.e.setFetchSize Gives the JDBC driver a hint as to the number of rows that should be fetched from the database when more rows are needed for ResultSet objects genrated by this Statement.setMaxRows Sets the limit for the maximum number of rows that any ResultSet object generated by this Statement object can contain to the given number. Spring JDBC +Spring JPA缓存可以用第三方，例如ehcached,或者mc 拼接SQL拼接SQL注意需特别注意字段值中存在转义字符（如”\”,”‘“）的情况，应使用类PreparedStatement中?方式替换变量执行CRUD操作 HibernateMyBatisMyBatis的前身叫iBatis，本是apache的一个开源项目, 2010年这个项目由apache software foundation 迁移到了google code，并且改名为MyBatis。MyBatis 是支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以对配置和原生Map使用简单的 XML 或注解，将接口和 Java 的 POJOs(Plain Old Java Objects,普通的 Java对象)映射成数据库中的记录。Mybatis的功能架构分为三层： API接口层提供给外部使用的接口API，开发人员通过这些本地API来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。 数据处理层负责具体的SQL查找、SQL解析、SQL执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。 基础支撑层负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑。 Mybatis与Hibernate比较Mybatis：小巧、方便、高效、简单、直接、半自动、移植性低Hibernate：强大、方便、高效、复杂、绕弯子、全自动、移植性高 Mybatis应用场景一直在用Hibnernate，抽取一个完善的DAO抽象类后会少很多工作，更受益与其更换数据库时超强的移植性，DAO层基本不作修改，更换数据库方言即可。但在以下场景时，Mybatis自有其可取之处： 当无法对数据库结构做到控制和修改，Mybatis的灵活性将比hibernate更适合； 当系统数据处理量巨大，性能要求极为苛刻，这往往意味着我们必须通过经过高度优化的sql语句（或存储过程）才能达到系统性能设计指标，在这种情况下Mybatis会有更好的可控性和表现，可以进行细粒度的优化。 NOSQLMorphiaJongoSpring Data for HadoopKunderaJackson-ObjectMapper]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>ORM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop生态圈]]></title>
    <url>%2F2015%2F08%2F11%2FHadoop%E7%94%9F%E6%80%81%E5%9C%88%2F</url>
    <content type="text"><![CDATA[开始正式接触Hadoop，从CDH自动化分布式环境部署延伸的Hadoop生态圈的术语，列举如下：查看所有术语 Apache Accumulo基于谷歌 BigTable 设计的分类、分布键值存储。Accumulo 为在 HDFS 上运行的 NoSQL DBMS，支持高效存储和结构化数据检索，包括查询范围。Accumulo 表可用作 MapReduce 作业的输入和输出。Accumulo 功能包括自动负载平衡和分区、数据压缩和细粒度安全标签。 Apache Avro在网上存储和传输数据的序列化系统。Avro 为 Avro 数据序列（通常称为“Avro 数据文件”）提供丰富的数据结构、紧凑的二级制编码和容器文件的支持。Avro 独立于语言，可使用多个语言绑定，包括 Java、C、C++、Python 和 Ruby。生成或使用文件的 CDH 中的所有组件支持 Avro 数据文件作为文件格式。Avro 提供与系统（如 Apache Thrift）和协议缓冲类似的功能。 Apache Bigtop开发封装和 Apache Hadoop 生态系统项目的互操作性测试的项目。 Apache Crunch用于编写、测试和运行 MapReduce 管道的 Java 库。请参见 Apache Crunch。 Apache Flume一个分布式、可靠可用的系统，用于高效收集、聚合和移动大量文本或从多个不同源至集中式数据存储的流数据。 Apache Giraph一个在 Apache Hadoop 上运行的大型、容错的图像处理框架。 Apache Hadoop一个免费的开源软件框架，支持数据密集型分布应用程序。Apache Hadoop的核心组件为 HDFS 和 MapReduce 和 YARN 处理框架。该术语也用于与 Hadoop 相关的生态系统项目，位于分布式计算和大规模数据处理的基础架构之下。 MapReduceMapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，得到最终结果。简单地说，MapReduce就是”任务的分解与结果的汇总”。在Hadoop中，用于执行MapReduce任务的机器角色有两个：一个是JobTracker；另一个是TaskTracker，JobTracker是用于调度工作的，TaskTracker是用于执行工作的。一个Hadoop集群中只有一台JobTracker。在分布式计算中，MapReduce框架负责处理了并行编程中分布式存储、工作调度、负载均衡、容错均衡、容错处理以及网络通信等复杂问题，把处理过程高度抽象为两个函数：map和reduce，map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。需要注意的是，用MapReduce来处理的数据集（或任务）必须具备这样的特点：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都可以完全并行地进行处理。 MapReduce v1 (MRv1) MapReduce 作业执行的运行时间框架。它定义两个守护程序： JobTracker - 协调运行 MapReduce作业，并提供资源管理和作业生命周期管理。在 YARN 中，这些功能由两个单独组件执行。 TaskTracker - 运行 MapReduce 作业已拆分的任务。 WordCount流程参考中文教程 HUE新建输入文件：txtHUE地址：http://server1.lt.com:8888/filebrowser/；或者在xshell中su hdfs切换用户新建文件：file01.txt，file02.txt 查看输入文件 ：hadoop fs -ls /user/uadb/exchange/input/wordCount/ 查看输入文件内容：hadoop fs -cat /user/uadb/exchange/input/wordCount/file*.txt 上传apreduce程序：jar 运行MapReduce程序：hadoop jar /uadb/wordCount.jar me.demo.hadoop.mapreduce.WordCount /user/uadb/exchange/input/wordCount/ /user/uadb/exchange/output/wordCountFatJar打包：hadoop jar /uadb/wordCount.jar com.simontuffs.onejar.Boot /user/uadb/exchange/input/wordCount/ /user/uadb/exchange/output/wordCount 查看执行结果： hadoop fs -cat /user/uadb/exchange/output/wordCount/*-0000* YARNApache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一种新的 Hadoop 资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。YARN最初是为了修复MapReduce实现里的明显不足，并对可伸缩性（支持一万个节点和二十万个内核的集群）、可靠性和集群利用率进行了提升。YARN实现这些需求的方式是，把Job Tracker的两个主要功能（资源管理和作业调度/监控）分成了两个独立的服务程序——全局的资源管理（RM）和针对每个应用的应用 Master（AM），这里说的应用要么是传统意义上的MapReduce任务，要么是任务的有向无环图（DAG）。 运行分布式应用程序的通用体系结构。YARN 指定以下组件： ResourceManager - 管理计算资源的全局分配至应用程序。ApplicationMaster - 管理应用程序的生命周期NodeManager - 启动和监视群集中机器上的计算容器JobHistory Server - 跟踪已完成的应用程序主应用程序为群集资源与资源管理器协商 - 按照多个容器的描述，每个应用程序拥有特定的内存限制 - 然后在这些容器中运行应用程序特定的进程。在群集节点上运行的节点管理器监督容器，确保应用程序未使用超出其已分配资源的资源。 MapReduce v2 (MRv2) 作为 YARN 应用程序实施。 Apache HBase面向列的可伸缩分布式数据存储。它提供实时读/写随机访问 HDFS 托管的大规模数据集的权限。Hbase Web UI：http://{master}:60010/ Hbase表结构设计在HBase中，数据是按Column Family来分割的，同一个Column Family下的所有列的数据放在一个文件（为简化下面的描述在此使用文件这个词，在HBase内部使用的是Store）中。 HBase本身的设计目标是 支持稀疏表，而 稀疏表通常会有很多列，但是每一行有值的列又比较少。如果不使用Column Family的概念，那么有两种设计方案：1.把所有列的数据放在一个文件中（也就是传统的按行存储）。那么当我们想要访问少数几个列的数据时，需要遍历每一行，读取整个表的数据，这样子是很低效的。2.把每个列的数据单独分开存在一个文件中（按列存储）。那么当我们想要访问少数几个列的数据时，只需要读取对应的文件，不用读取整个表的数据，读取效率很高。然而，由于稀疏表通常会有很多列，这会导致文件数量特别多，这本身会影响文件系统的效率。 而Column Family的提出就是为了在上面两种方案中做一个折中。HBase中 将一个Column Family中的列存在一起，而不同Column Family的数据则分开。由于在HBase中Column Family的数量通常很小，同时HBase建议把经常一起访问的比较类似的列放在同一个Column Family中，这样就可以在访问少数几个列时，只读取尽量少的数据。 Hbase性能优化配置HBase Master 的 Java 堆栈大小（字节）-Master Default Group：377M（默认）&gt;HBase RegionServer 的 Java 堆栈大小（字节）-RegionServer Default Group :588M（默认）&gt;1024MHBase 客户端写入缓冲-hbase.client.write.buffer：2M（默认）&gt; HBaseMasterHMaster 负责给HRegionServer分配区域,并且负责对集群环境中的HReginServer进行负载均衡，HMaster还负责监控集群环境中的HReginServer的运行状况，如果某一台HReginServer down机，HBaseMaster将会把不可用的HReginServer来提供服务的HLog和表进行重新分配转交给其他HReginServer来提供，HBaseMaster还负责对数据和表进行管理，处理表结构和表中数据的变更，因为在 META 系统表中存储了所有的相关表信息。并且HMaster实现了ZooKeeper的Watcher接口可以和zookeeper集群交互。 HRegionServerHReginServer负责处理用户的读和写的操作。HReginServer通过与HBaseMaster通信获取自己需要服务的数据表，并向HMaster反馈自己的运行状况。当一个写的请求到来的时候，它首先会写到一个叫做HLog的write-ahead log中。HLog被缓存在内存中，称为Memcache，每一个HStore只能有一个Memcache。当Memcache到达配置的大小以后，将会创建一个MapFile，将其写到磁盘中去。这将减少HReginServer的内存压力。当一起读取的请求到来的时候，HReginServer会先在Memcache中寻找该数据，当找不到的时候，才会去在MapFiles 中寻找。 HBase ClientHBase Client负责寻找提供需求数据的HReginServer。在这个过程中，HBase Client将首先与HMaster通信，找到ROOT区域。这个操作是Client和Master之间仅有的通信操作。一旦ROOT区域被找到以后，Client就可以通过扫描ROOT区域找到相应的META区域去定位实际提供数据的HReginServer。当定位到提供数据的HReginServer以后，Client就可以通过这个HReginServer找到需要的数据了。这些信息将会被Client缓存起来，当下次请求的时候，就不需要走上面的这个流程了。 HBase ServiceHBase Thrift Server和HBase REST Server是通过非Java程序对HBase进行访问的一种途径。 Lily HBase IndexerLily HBase Indexer provides the ability to quickly and easily search for any content stored in HBase.It allows you to quickly and easily index HBase rows into Solr, without writing a line of code. It doesn’t require Lily, but originates from years of experience indexing HBase as part of Lily - the Customer Intelligence Data Management Platform from NGDATA.Lily HBase Indexer drives HBase indexing support in Cloudera Search, the SEP trigger notification mechanism is used inside Lily as well. Lily HBase NRT Indexer服务，Lily HBase Indexer是一款灵活的、可扩展的、高容错的、事务性的，并且近实时的处理HBase列索引数据的分布式服务软件。它是NGDATA公司开发的Lily系统的一部分，已开放源代码。Lily HBase Indexer使用SolrCloud来存储HBase的索引数据，当HBase执行写入、更新或删除操作时，Indexer通过HBase的replication功能来把这些操作抽象成一系列的Event事件，并用来保证写入Solr中的HBase索引数据的一致性。并且Indexer支持用户自定义的抽取，转换规则来索引HBase列数据。Solr搜索结果会包含用户自定义的columnfamily:qualifier字段结果，这样应用程序就可以直接访问HBase的列数据。而且Indexer索引和搜索不会影响HBase运行的稳定性和HBase数据写入的吞吐量，因为索引和搜索过程是完全分开并且异步的。Lily HBase Indexer在CDH5中运行必须依赖HBase、SolrCloud和Zookeeper服务。 MorphlinesMorphlines是一款开源的，用来减少构建hadoop ETL数据流程时间的应用程序。它可以替代传统的通过MapReduce来抽取、转换、加载数据的过程，提供了一系列的命令工具，具体可以参见：http://kitesdk.org/docs/0.13.0/kite-morphlines/morphlinesReferenceGuide.html。对于HBase的其提供了extractHBaseCells命令来读取HBase的列数据。我们采用Cloudera Manager来管理morphlines.conf文件，使用CM来管理morphlines.conf文件除了上面提到的好处之外，还有一个好处就是当我们需要增加索引列的时候，如果采用本地路径方式将需要重新注册Lily HBase Indexer的配置文件，而采用CM管理的话只需要修改morphlines.conf文件后重启Key-Value HBase Indexer服务即可,CM会自动分发给集群。 HDFSHadoop分布式文件系统（Distributed File System） NamenodeNamenode 管理者文件系统的Namespace。它维护着文件系统树(filesystem tree)以及文件树中所有的文件和文件夹的元数据(metadata)。管理这些信息的文件有两个，分别是Namespace 镜像文件(Namespace image)和操作日志文件(edit log)，这些信息被Cache在RAM中，当然，这两个文件也会被持久化存储在本地硬盘。Namenode记录着每个文件中各个块所在的数据节点的位置信息，但是他并不持久化存储这些信息，因为这些信息会在系统启动时从数据节点重建。Namenode结构图课抽象为如图：NameNode Web UI：http://master.lt.com:50070/ 客户端(client)代表用户与namenode和datanode交互来访问整个文件系统。客户端提供了一些列的文件系统接口，因此我们在编程时，几乎无须知道datanode和namenode，即可完成我们所需要的功能。 DatanodeDatanode是文件系统的工作节点，他们根据客户端或者是namenode的调度存储和检索数据，并且定期向namenode发送他们所存储的块(block)的列表。 Apache SolrSolr是Apache Lucene项目的开源企业搜索平台。其主要功能包括全文检索、命中标示、分面搜索、动态聚类、数据库集成，以及富文本（如Word、PDF）的处理。Solr是高度可扩展的，并提供了分布式搜索和索引复制。Solr 4还增加了NoSQL支持，以及基于Zookeeper的分布式扩展功能SolrCloud。 Solr可用于Hbase的二级索引 Apache HiveHadoop 的数据仓库系统，使用诸如 SQL 的语言（称为 HiveQL），有助于实现汇总和 HDFS 中存储的大数据集的分析。 HiveServer支持通过 Apache Thrift 连接将客户端连接至 Hive 的服务器进程。 HiveServer2支持通过网络连接将客户端连接至 Hive 的服务器进程。这些客户端可以是本机命令行编辑器或使用 ODBC 或 JDBC 驱动程序的应用程序和工具。 HiveQLHadoop 的一种查询语言，使用类似于标准 SQL 的语法，以在 HDFS 上执行 MapReduce 作业。HiveQL 不支持所有的 SQL 功能。不支持事务和物化视图，对索引和查询的支持有限。它支持不属于标准 SQL 的功能，如多表格，包括多表格插入和创建选择的表格。 在内部，编译器将 HiveQL 语句转换为 MapReduce 作业的有向无环图，提交至 Hadoop，以执行。Beeswax 包含在 Hue内，为 HiveQL 查询提供图形化前端。 Apache MahoutHadoop 的机器学习库。它是您能够构建克扩展至大型数据集的机器学习库，从而简化了构建智能应用程序的任务。Mahout 支持的主要使用情形包括： 建议挖掘 -基于过去偏好标识用户喜好，如在线购物建议。 群集 - 类似项目的组；如解决类似主题的文档 分类 - 学习现有类别中成员的共同之处，然后使用该信息分类新项目。 频繁的项目集挖掘 - 采用一组项目组（如查询会话或购物车中的条目），识别通常一起出现的项目。 Apache Oozie协调数据接收、存储、转换和分析操作的工作流程和协调服务。 Apache Pig数据流语言和在 MapReduce 顶部构建的并行执行框架。在内部，编译器将 Pig 语句转换为 MapReduce 作业的有向无环图，提交至 Hadoop，以执行。 Apache Sentry企业级大数据安全的下一步骤，为存储在 Apache Hadoop 中的数据提供细粒度授权。独立的安全模块，集成开源 SQL 查询引擎 Apache Hive 和 Cloudera Impala，Sentry 提供高级身份验证控制以为企业数据集启用多用户应用程序和跨职能流程。 Apache Spark分布式计算的一般框架，为迭代和交互式处理提供出色性能。Spark 为 Java、Scala 和 Python 展示用户友好的 API。 Apache Sqoop在 Hadoop 和外部结构化数据存储（如关系数据库）之间高效批量传输数据的工具。Sqoop 导入表格内容至 HDFS、Apache Hive 和 Apache HBase，并生成允许用户解译表架构的 Java 类。Sqoop 也可以从 Hadoop 存储中提取数据，并将记录从 HDFS 导出至外部结构化数据存储，如关系数据库和企业数据仓库。 Sqoop 有两个版本：Sqoop 和 Sqoop 2。Sqoop 要求客户端安装和配置。Sqoop 2 是基于网络的服务，具有客户命令行界面。拥有 Sqoop 2 连接器，在服务器上配置数据库驱动。 Apache ZooKeeper维护配置信息、命名并提供分布式同步和组服务的集中式服务。 CDHCloudera Apache Hadoop 分布包含核心 Hadoop（HDFS、MapReduce、YARN）以及以下相关项目：Apache Avro、Apache Flume、Fuse-DFS、Apache HBase、Apache Hive、Hue、Cloudera Impala、Apache Mahout、Apache Oozie、Apache Pig、Cloudera Search、Apache Sentry、Apache Spark、Apache Sqoop、Apache Whirr、Apache ZooKeeper、DataFu 和 Kite。 CDH 为免费 100% 开源，并在 Apache 2.0 许可证下许可。CDH 支持多个 Linux 分配。 Cloudera ManagerCloudera Manager定义CDH、Cloudera Impala 和 Cloudera Search 的端到瑞管理应用程序。Cloudera Manager 允许管理员轻松有效地协调、监控和管理 Hadoop 群集和 CDH 安装。 Cloudera Manager版本Cloudera Manager 有两个可用版本：Cloudera Express 和 Cloudera Enterprise。 Cloudera Express免费下载，包含 CDH，涵盖企业级 Apache Hadoop、Apache HBase、Cloudera Impala、Cloudera Search、Apache Spark 和 Cloudera Manager 分布，提供强大的集群管理功能，如自动部署、集中管理、监控和诊断工具。Cloudera Express 使数据驱动的企业可评估 Apache Hadoop。 Cloudera Management ServiceCloudera Management Service 可作为一组角色实施各种管理功能： Activity Monitor - 收集有关 MapReduce 服务运行的活动的信息。默认情况下未添加此角色。 Host Monitor - 收集有关主机的运行状况和指标信息 Service Monitor - 收集有关服务的运行状况和指标信息以及 YARN 和 Impala 服务中的活动信息 Event Server - 聚合 relevant Hadoop 事件并将其用于警报和搜索 Alert Publisher - 为特定类型的事件生成和提供警报 Reports Manager - 生成报告，它提供用户、用户组和目录的磁盘使用率的历史视图，用户和 YARN 池的处理活动，以及 HBase 表和命名空间。此角色未在 Cloudera Express 中添加。Cloudera Manager 将单独管理每个角色，而不是作为 Cloudera Manager Server 的一部分进行管理，可实现可扩展性（例如，在大型部署中，它可用于将监控器角色置于自身的主机上）和隔离。 此外，对于特定版本的 Cloudera Enterprise 许可证，Cloudera Management Service 还为 Cloudera Navigator 提供 Navigator Audit Server 和 Navigator Metadata Server 角色。 Cloudera Impala可实时查询存储在 HDFS 或 Apache HBase 中数据的服务。它支持相同的元数据和 ODBC 和 JDBC 驱动程序作为 Apache Hive 和基于 Hive 标准查询语言 (HiveQL) 的查询语言。要避免延迟，Impala 规避 MapReduce 通过特殊分布式查询引擎（与商业并行 RDBMS 中的引擎相似）直接访问数据。 Hue为 CDH 服务构建自定义 GUI 应用程序的服务和包含以下内置应用程序的工具：Apache Pig、Apache HBase 和 Sqoop 2 shell，Apache Pig 编辑器、Beeswax Hive UI，Cloudera Impala 查询编辑器，Solr 搜索应用程序Hive 元存储管理器，Oozie 应用程序编辑器、调度程序和提交者，Apache HBase 浏览器，Sqoop 2 应用程序，HDFS 文件管理器和 MapReduce 和 YARN 作业浏览器。 主页：http://master.lt.com:8888/MapReduce任务查看：http://192.168.1.100:8888/jobbrowser/Hbase主页：http://192.168.1.100:8888/hbaseHue Hbase文档 Hive主页：http://192.168.1.100:8888/beeswax/ ##问题记录 Hue-Hbase表中文字段插入和编辑问题]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>分布式</tag>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群部署]]></title>
    <url>%2F2015%2F08%2F08%2FHadoop%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Hadoop版本区别目前不收费的Hadoop版本主要有三个，分别是： Apache（最原始的版本，所有发行版均基于这个版本进行改进） Cloudera版本（Cloudera’s Distribution Including Apache Hadoop，简称CDH） Hortonworks版本(Hortonworks Data Platform，简称“HDP”），对于国内而言，绝大多数选择CDH版本， CDH和Apache版本主要区别： CDH对Hadoop版本的划分非常清晰，目前维护的有两个系列的版本，分别是cdh4和cdh5 由于Cloudera做Hadoop开发的，其他厂商仅是做Hadoop集成或CDH集成，和Hadoop trunk能最快的同步，保证业务的前向兼容性； 安全 CDH支持Kerberos安全认证，apache hadoop则使用简陋的用户名匹配认证 CDH文档清晰（包括中文文档），很多采用Apache版本的用户都会阅读CDH提供的文档，包括安装文档、升级文档等。 CDH支持Yum/Apt包，Tar包，RPM包，CM安装，Cloudera Manager三种方式安装,Apache hadoop只支持Tar包安装。 关于CDH官方介绍Cloudera 提供一个可扩展、灵活、集成的平台，可用来方便地管理您的企业中快速增长的多种多样的数据。业界领先的 Cloudera 产品和解决方案使您能够部署并管理 Apache Hadoop 和相关项目、操作和分析您的数据以及保护数据的安全。 Cloudera 提供下列产品和工具： CDH — Cloudera 分发的 Apache Hadoop 和其他相关开放源代码项目，包括 Impala 和 Cloudera Search。CDH 还提供安全保护以及与许多硬件和软件解决方案的集成。 Cloudera Manager — 一个复杂的应用程序，用于部署、管理、监控您的 CDH 部署并诊断问题。Cloudera Manager 提供 Admin Console，这是一种基于 Web 的用户界面，使您的企业数据管理简单而直接。它还包括 Cloudera Manager API，可用来获取群集运行状况信息和度量以及配置 Cloudera Manager。 Cloudera Navigator — CDH 平台的端到端数据管理工具。Cloudera Navigator 使管理员、数据经理和分析师能够了解 Hadoop 中的大量数据。Cloudera Navigator 中强大的审核、数据管理、沿袭管理和生命周期管理使企业能够遵守严格的法规遵从性和法规要求。 Cloudera Impala — 一种大规模并行处理 SQL 引擎，用于交互式分析和商业智能。其高度优化的体系结构使它非常适合用于具有联接、聚合和子查询的传统 BI 样式的查询。它可以查询来自各种源的 Hadoop 数据文件，包括由 MapReduce 作业生成的数据文件或加载到 Hive 表中的数据文件。YARN 和 Llama 资源管理组件让 Impala 能够共存于使用 Impala SQL 查询并发运行批处理工作负载的群集上。您可以通过 Cloudera Manager 用户界面管理 Impala 及其他 Hadoop 组件，并通过 Sentry 授权框架保护其数据。 Cloudera QuickStart VM下载地址为方便开始使用CDH、Cloudera Manager、Cloudera Impala 和 Cloudera Search，这些虚拟机器包含您所需的一切。 CDH安装步骤CDH安装参考资料 cloudera简介 cdh5官方文档 CDH5安装环境要求 CDH5的3种安装方式 在线Yum/Wget安装教程 在线Yum/Wget安装教程-中文 在线安装优势 联网安装、升级，非常方便 自动下载依赖软件包 Hadoop生态系统包自动匹配，不需要你寻找与当前Hadoop匹配的Hbase，Flume，Hive等软件，Yum/Apt会根据当前安装Hadoop版本自动寻找匹配版本的软件包，并保证兼容性。 自动创建相关目录并软链到合适的地方（如conf和logs等目录）；自动创建hdfs, mapred用户，hdfs用户是HDFS的最高权限用户，mapred用户则负责mapreduce执行过程中相关目录的权限。 在线安装步骤静态IP配置cd /mnt/scripts/batch-update-ip &amp;&amp; chmod +x static-ip.sh &amp;&amp; ./static-ip.sh localhost eth1 192.168.1 81 1示例：cd /mnt/scripts/batch-update-ip &amp;&amp; chmod +x static-ip.sh &amp;&amp; ./static-ip.sh slave3.lt.com eth1 192.168.1 103 1 脚本地址 HostName配置/FQDN配置FQDN是Fully Qualified Domain Name的缩写, 含义是完整的域名. 例如, 一台机器主机名(hostname)是www, 域后缀(domain)是example.com, 那么该主机的FQDN应该是www.example.com.注意：hosts配置不当，后面server和agent间通讯会存在问题，参考host配置如下： server/agent配置(master) 123456789101112131415161718192021222324252627# vim /etc/hosts# solr/hue/hive server127.0.0.1 localhost.localdomain localhost192.168.1.90 cdhagent.lt.com cdhagent192.168.1.91 server1.lt.com server1192.168.1.92 server2.lt.com server2192.168.1.93 server3.lt.com server3192.168.1.94 server4.lt.com server4192.168.1.95 server5.lt.com server5# HDFS NameNode192.168.1.100 master.lt.com master# HDFS DataNode192.168.1.101 slave1.lt.com slave1192.168.1.102 slave2.lt.com slave2192.168.1.103 slave3.lt.com slave3192.168.1.104 slave4.lt.com slave4192.168.1.105 slave5.lt.com slave5192.168.1.106 slave6.lt.com slave6192.168.1.107 slave7.lt.com slave7192.168.1.108 slave8.lt.com slave8192.168.1.109 slave9.lt.com slave9192.168.1.110 slave10.lt.com slave10 network配置 1234# vim /etc/sysconfig/networkNETWORKING=yesHOSTNAME=master.lt.comNTPSERVERARGS=iburst TODO：可局域网搭建DNS服务器，Slave机器即可不配配置hosts SSH无密码登陆配置TODO：补充github脚本地址删除现有ssh配置（可选）：rm /root/.ssh/authorized_keys &amp;&amp; rm /root/.ssh/known_hostscd /mnt/scripts/batch-ssh &amp;&amp; chmod +x keygen_master.sh &amp;&amp; ./keygen_master.sh iptables防火墙配置123456#查看防火墙状态service iptables status#关闭防火墙service iptables stop#永久关闭防火墙chkconfig iptables off selinux强制访问控制安全配置123456789101112#查看SELinux状态/usr/sbin/sestatus -v#或命令getenforce#关闭SELinux#1. 临时关闭（不用重启机器）：setenforce 0 ##设置SELinux 成为permissive模式 ##setenforce 1 设置SELinux 成为enforcing模式#2. 修改配置文件vim /etc/selinux/config#将SELINUX=enforcing改为SELINUX=disabled#3. reboot swap配置Cloudera 建议将 /proc/sys/vm/swappiness 设置为 0。默认设置为 60。查看当前swap分区设置cat /proc/sys/vm/swappiness临时修改值：sudo sysctl vm.swappiness=0永久修改值：vim /etc/sysctl.conf，在最后加一行vm.swappiness = 0 123swappiness的值的大小对如何使用swap分区是有着很大的联系swappiness=0的时候表示最大限度使用物理内存，然后才是 swap空间swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面 安装NTP服务NTP配置教程集群中所有主机必须保持时间同步，如果时间相差较大会引起各种问题。 具体思路如下：master节点作为ntp服务器与外界对时中心同步时间，随后对所有datanode节点提供时间同步服务。所有datanode节点以master节点为基础同步时间。所有节点安装相关组件： yum install ntp 。配置开机启动： chkconfig ntpd on ,检查是否设置成功： chkconfig --list ntpd 其中2-5为on状态就代表成功。 主节点配置在配置之前，先使用ntpdate手动同步一下时间，免得本机与对时中心时间差距太大，使得ntpd不能正常同步。 ntpdate -u 202.112.29.82编辑/etc/ntp.conf配置文件，配置完成后启动服务service ntpd start 123456789101112131415161718192021222324driftfile /var/lib/ntp/driftrestrict default kod nomodify notrap nopeer noqueryrestrict -6 default kod nomodify notrap nopeer noqueryrestrict 127.0.0.1restrict -6 ::1#允许内网其他机器同步时间restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# 中国这边最活跃的时间服务器 : http://www.pool.ntp.org/zone/cnserver 3.cn.pool.ntp.org perfer server 1.asia.pool.ntp.org server 3.asia.pool.ntp.org# 外部时间服务器不可用时，以本地时间作为时间服务server 127.127.1.0 # local clockfudge 127.127.1.0 stratum 10includefile /etc/ntp/crypto/pwkeys /etc/ntp/keysserver 127.127.1.0 iburst 查看配置结果： netstat -tlunp | grep ntp 查看服务连接和监听 ntpq -p查看网络中的NTP服务器，同时显示客户端和每个服务器的关系 ntpstat 查看时间同步状态 NTP客户端配置编辑/etc/ntp.conf配置文件，配置完成后启动服务service ntpd restart 123456789101112131415driftfile /var/lib/ntp/driftrestrict 127.0.0.1restrict -6 ::1# 配置时间服务器为本地的时间服务器server 192.168.1.100restrict 192.168.1.100 nomodify notrap noqueryserver 127.127.1.0 # local clockfudge 127.127.1.0 stratum 10includefile /etc/ntp/crypto/pwkeys /etc/ntp/keys cloudera-manager-installer.bin安装123wget http：//archive.cloudera.com/cm5/installer/latest/cloudera-manager-installer.binchmod u+x cloudera-manager-installer.binsudo ./cloudera-manager-installer.bin 在线配置打开http：//{host}：7180/cmf/login默认用户名/密码：admin/admincloudera配置文件 CDH5运维相关系统服务服务查询：service --status-all| grep cloudera服务端 cloudera-scm-server cloudera-scm-server-db客户端 cloudera-scm-agent 嵌入式 PostgreSQL 数据库嵌入式 PostgreSQL 数据库修改postgresql配置允许远程访问 scm数据库密码 123456# cat /etc/cloudera-scm-server/db.propertiescom.cloudera.cmf.db.type=postgresqlcom.cloudera.cmf.db.host=localhost:7432com.cloudera.cmf.db.name=scmcom.cloudera.cmf.db.user=scmcom.cloudera.cmf.db.password=30MCe9Mxuk root密码： /var/lib/cloudera-scm-server-db/data/generated_password.txt 卸载Cloudera卸载 Cloudera Manager 和托管软件参考博客 Cloudera Manager API官方文档Cloudera Manager API 提供了配置和服务生命周期管理、服务运行状况信息和指标，并允许您配置 Cloudera Manager 本身。此 API 与 Cloudera Manager Admin Console 在同一主机和端口上接受服务，不需要任何额外过程或额外配置。此 API 支持 HTTP 基本验证，接受的用户和凭据与 Cloudera Manager Admin Console 相同。 Cloudera Management ServiceCloudera Management Service 可作为一组角色实施各种管理功能：Activity Monitor - 收集有关 MapReduce 服务运行的活动的信息。默认情况下未添加此角色。Host Monitor - 收集有关主机的运行状况和指标信息Service Monitor - 收集有关服务的运行状况和指标信息以及 YARN 和 Impala 服务中的活动信息Event Server - 聚合 relevant Hadoop 事件并将其用于警报和搜索Alert Publisher - 为特定类型的事件生成和提供警报Reports Manager - 生成报告，它提供用户、用户组和目录的磁盘使用率的历史视图，用户和 YARN 池的处理活动，以及 HBase 表和命名空间。此角色未在 Cloudera Express 中添加。Cloudera Manager 将单独管理每个角色，而不是作为 Cloudera Manager Server 的一部分进行管理，可实现可扩展性（例如，在大型部署中，它可用于将监控器角色置于自身的主机上）和隔离。 此外，对于特定版本的 Cloudera Enterprise 许可证，Cloudera Management Service 还为 Cloudera Navigator 提供 Navigator Audit Server 和 Navigator Metadata Server 角色。 安装问题记录 无法发出查询：未能连接到 Host Monitor 解决方式：新增cloudera management services ,注意新增的目标机器因为scm-server安装的同一台机器，机器内存最好&gt;=8G urlopen error [Errno 111] Connection refused 问题场景：CM中主机CDH5安装Parcel完成后无法分配和激活Agent 详细日志 downloader INFO Finished download [ url： http：//master.lt.com：7180/cmf/parcel/download/CDH-5.4.4-1.cdh5.4.4.p0.4-el6.parcel, state： exception, total_bytes： 0, downloaded_bytes： 0, start_time： 2015-08-11 22：18：32, download_end_time： , end_time： 2015-08-11 22：18：33, code： 600, exception： , path： None ] 错误分析：在slave机器执行nc -v master.lt.com 7182 返回refused，判断是host配置有误导致dns解析不了 解决方式：在slave1的hosts中新增master的域名解析 正在获取安装锁问题 问题描述：意外中止安装后重新安装提示”正在获取安装锁…“ ； 问题解决：在Manager节点运行rm /tmp/.scm_prepare_node.lock删除Manager的lock文件后重新安装即可； 对于此 Cloudera Manager 版本 (5.4.7) 太新的 CDH 版本不会显示 问题描述：Versions of CDH that are too new for this version of Cloudera Manager (5.4.7) will not be shown. 问题定位：PARCELS表fileName=CDH-5.4.7-1.cdh5.4.7.p0.3-el6.parcel的hash值为null，判断为parcel文件问题或scm数据库生成干扰问题 解决思路：判断为/opt/cloudera/parcel-repo/目录内的本地parcel应该在scm数据库初始化结束后再放入 问题解决：停止server&gt;重置scm数据库&gt;启动server&gt;复制parcel到/opt/cloudera/parcel-repo/目录 参考 [Errno 2] No such file or directory: u’/opt/cloudera/parcels/CDH-5.4.7-1.cdh5.4.7.p0.3/meta/parcel.json’ - master.lt.com需要存在目录：/opt/cloudera/parcel-cache 启用透明大页面问题 问题描述：已启用“透明大页面”，它可能会导致重大的性能问题。版本为“CentOS release 6.5 (Final)”且发行版为“2.6.32-431.el6.x86_64”的 Kernel 已将 enabled 设置为“[always] madvise never”，并将 defrag 设置为“[always] madvise never”。 问题解决：运行“echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag”以禁用此设置，然后将同一命令添加到一个 init 脚本中，如 /etc/rc.local， 修改Cloudera Manager 管理机器的IP解决方案]]></content>
      <categories>
        <category>大数据</category>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>分布式</tag>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python安装升级教程]]></title>
    <url>%2F2015%2F08%2F05%2FPython%E5%AE%89%E8%A3%85%E5%8D%87%E7%BA%A7%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[升级前安装依赖yum groupinstall &quot;Development tools&quot; Python 2.7.9 下载&gt;解压&gt;编译&gt;安装123456wget https://www.python.org/ftp/python/2.7.9/Python-2.7.9.tgz tar jxvf Python-2.7.9.tgz mv Python-2.7.9 pythoncd python./configure --prefix=/usr/local/pythonmake &amp;&amp; make altinstall 关于altinstall12 Warningmake install can overwrite or masquerade the python binary. make altinstall is therefore recommended instead of make install since it only installs exec_prefix/bin/pythonversion. 修改系统默认Python 查看已有python版本python -V 查看Python版本/usr/local/python/bin/python2.7 -V 将系统默认Python改为新安装的Pythonln -fs /usr/local/python/bin/python2.7 /usr/bin/python yum无法运行问题进行更改后，yum会无法运行了。修改/usr/bin/yum文件，将第一行的#!/usr/bin/python中的python改为系统原有的python版本，如下：#!/usr/bin/python2.6]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IIS与Tomcat共享80端口]]></title>
    <url>%2F2015%2F08%2F03%2FIIS%E4%B8%8ETomcat%E5%85%B1%E4%BA%AB80%E7%AB%AF%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[微信公众号有80端口要求，但是80端口已运行了一个项目，如何将后端J2EE实现的服务集成到80端口，即Tomcat集成进IIS。调研了三种方案，Apache Tomcat Connector（isapi_redirect实现）、ARR（IIS中的反向代理）和 IIS2Tomcat(BonCode AJP实现)，按照最轻量最简洁的部署要求，最终采用AJP实现。 isapi_redirect官方参考实现思路： The IIS-Tomcat redirector is an IIS plugin (filter + extension), IIS load the redirector plugin and calls its filter function for each in-coming request. The filter then tests the request URL against a list of URI-paths held inside uriworkermap.properties, If the current request matches one of the entries in the list of URI-paths, the filter transfers the request to the extension. The extension collects the request parameters and forwards them to the appropriate worker using the defined protocol like ajp13. The extension collects the response from the worker and returns it to the browser配置注册表，DLL等步骤繁琐，易出错，如官方所说你很难一次性配置成功-_- ARRARR：Application Request Routing，类似Nginx的反向代理配置教程ARR is a good starting point if you want to connect Apache Tomcat to IIS 7, however, there are some issues especially under load that make this less than ideal solution. There are still differences in the way headers are handled between ARR and Tomcat and not all are transferred. ARR will be heavier on the network as it requires that http data is transferred in full byte length without being able to take advantage of binary compression and byte encoding the AJP protocol offers. Under load ARR will not be aware of Tomcat thread handling resulting in unnecessarily dropped connections. Secure (https) connections cannot be easily handled if tomcat needs to be aware of certificates used. AJPAJP：Apache JServ Protocol version项目地址下载地址（需翻墙）参考博客详细配置视频教程 默认网站二级应用程序配置要点 在IIS下新增网站examples，或者在默认网站中新增应用程序examples,物理路径指向｛catalina_home｝/webapps/examples； 执行Connector_Setup.exe安装，默认参数即可，选择指定的IIS网站，如examples； 应用程序池的托管管道模式设置为集成； 在examples根目录新增配置BIN目录（包含BonCodeAJP13.dll、BonCodeIIS.dll），并在根目录新增web.config，内容如下： 完整配置 123456789101112131415161718&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration&gt; &lt;system.webServer&gt; &lt;validation validateIntegratedModeConfiguration="false"/&gt; &lt;handlers&gt; &lt;add name="BonCodeForAll" path="*" verb="*" type="BonCodeIIS.BonCodeCallHandler" resourceType="Unspecified" preCondition="integratedMode" /&gt; &lt;add name="BonCode-Tomcat-JSP-Handler" path="*.jsp" verb="*" type="BonCodeIIS.BonCodeCallHandler" preCondition="integratedMode" /&gt; &lt;add name="BonCode-Tomcat-CFC-Handler" path="*.cfc" verb="*" type="BonCodeIIS.BonCodeCallHandler" preCondition="integratedMode" /&gt; &lt;add name="BonCode-Tomcat-CFM-Handler" path="*.cfm" verb="*" type="BonCodeIIS.BonCodeCallHandler" preCondition="integratedMode" /&gt; &lt;/handlers&gt; &lt;defaultDocument&gt; &lt;files&gt; &lt;add value="index.jsp" /&gt; &lt;add value="index.cfm" /&gt; &lt;/files&gt; &lt;/defaultDocument&gt; &lt;/system.webServer&gt;&lt;/configuration&gt; 精简配置 123456&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration&gt; &lt;system.webServer&gt; &lt;validation validateIntegratedModeConfiguration="false"/&gt; &lt;/system.webServer&gt;&lt;/configuration&gt; 若完整配置报错，采用精简配置即可。 问题记录 在唯一密钥属性“name”设置为“BonCode-Tomcat-JSP-Handler”时，无法添加类型为“add”的重复集合项解决：改用精简配置]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>J2EE</tag>
        <tag>Web服务器</tag>
        <tag>IIS</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[backbone.marionette学习要点]]></title>
    <url>%2F2015%2F07%2F28%2Fbackbone-marionette%E5%AD%A6%E4%B9%A0%E8%A6%81%E7%82%B9%2F</url>
    <content type="text"><![CDATA[Marionette（Backbone的牵线木偶）提供rendering、template管理、UI对象 Modelrest数据，event事件 Collection：ListViewItemView Backbone.View的扩展，用于渲染Backbone.Model 拥有modelEvents属性，可绑定View方法 CollectionView 用于渲染Backbone.Collection 可渲染List 拥有collectionEvents/childEvents属性， add/remove/reset/etc后自动更新视图 支持Filtering（filter方法）、Sorting（comparator属性） 支持Pagination CompositeView renders a Collection within a template ItemView（model）和CollectionView（collection）的组合 用于detail/table/nested lists/treeview类型的场景 View ContainersRegionView容器，渲染至DOM，DOM和Backbone的桥梁 LayoutView 用于复杂嵌套布局，multi-view组成的widget Region容器 render all in one call Applicationinitialization初始化和 bootstrapping引导程序 Module类似Application，多个Modules组成Application可控制start/stop模块（及其子模块） Controller组件间的交互，用于复杂业务逻辑的业务场景 layouts/regions：动态create/display/dispose页面dom event aggregrator可bind/trigger事件module/application级别的事件通道，其他所有模块都可监听，可用于如用户登陆成功的业务场景 Commands多处触发，一处处理 Request/Response用于提供Application Level Data，如购物车当前状态提供全局状态，无需所有模块都做处理，但易被滥用，当全局可操作时难预测其影响 appRouter：]]></content>
      <categories>
        <category>前端技术</category>
      </categories>
      <tags>
        <tag>Backbone</tag>
        <tag>Marionette</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《谜踪之国》读书笔记]]></title>
    <url>%2F2015%2F07%2F21%2F%E3%80%8A%E8%B0%9C%E8%B8%AA%E4%B9%8B%E5%9B%BD%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[谜踪之国-天下霸唱探险类的，没有藏地密码写的精彩 第三话 海森堡不确定原理2015-07-14 13:43:29世事茫茫如大海，人生何处无风波。 2015-07-14 19:12:50当可执其端而理其绪，举一隅而知三隅；随机生变，鬼神莫测；分寸即定，任意纵横；通篇玩熟，定教四海扬名 2015-07-17 13:47:09好花不常开，好景不常在 2015-07-17 14:20:27见不尽者，是天下之事；悟不尽者，是天下之理” 2015-07-22 13:01:01万物的命运，皆是由无数个点所组成的一条曲线，没有人知道线的中间会发生什么，或是会遇到什么，只是所有的线，最终都会前往同一个终点，这个终点就是死亡，绝不存在例外，曲线中出现的任何一个点，也都不可能对终点产生影响。 2015-07-25 22:07:56近代科学观念支持大爆炸形成宇宙的理论，“宇”和“宙”就是时间与空间的坐标，这和中国传统观念里“盘古开天地”之类的传说有些相似，据说以前只有一片混沌，清浊不分，从盘古产生时间的那一刻被称为“零秒”，而在“零秒坐标”出现之前，还没有时间存在。 2015-07-25 22:09:21从前有句古话是“蝶梦庄周未可知”，是说庄周以为自己在梦中变为了蝴蝶，其实也有可能庄周自己才是蝴蝶做的一场梦，这句话可以用来比喻真实的不确定性，那些看得见摸得到的东西，却未必真实可信。 2015-07-26 17:44:31说开天地怕、道破鬼神惊 2015-07-26 17:48:18并不是出现了“原因”才会产生“结果”，而是结果造就了复杂的原因。没有结果的原因不能称为原因，正是由于结果的存在，才会使前边发生的事件成为原因。因果之间的关系就像一株参天大树，注定成为事实的结果是根，原因则是枝杈纵横交错的茂密树冠，事先掌握了结果的人，就能洞悉命运的规律。 2015-08-07 19:36:06混沌定律，基本上分为三个部分。事物发展运行的轨迹好像是多元化的，存在着无数种可能性，不管你预先布置得如何周密，事到临头也总会出现意料之外的情况，所谓计划赶不上变化，是对第一定律的最好概括。第二定律说白了就是‘怕什么来什么’，你越是不想让它发生的事，它发生的概率就越大。比方说我有块面包，正面抹满了黄油，又不小心失手把它掉在了名贵的地毯上，面包正反两面朝下的概率看起来似乎差不多，其实不管面包掉落多少次，抹了黄油的那一面都会永远朝下，因为事情总是会往咱们最不想看到的方向发展，这既是摩非原理——宿命的重力。另外还有第三定律……” 2015-08-26 09:13:51挫折只是成功者的勋章，疾风劲草，方显英雄本色，洪波汹涌，愈见生命不息。 多看笔记 来自多看阅读 for Kindle]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>小说</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《三体》读书笔记]]></title>
    <url>%2F2015%2F07%2F21%2F%E3%80%8A%E4%B8%89%E4%BD%93%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[三体三部曲-刘慈欣 给岁月以文明，给时光以生命。 宇宙社会学：（1）生存是文明的第一需求；（2）文明不断增长和扩张，但宇宙中的物质总量不变。 【时间开始后约170亿年，我们的星星】 宇宙就是一座黑暗森林，每个文明都是带枪的猎人，像幽灵般潜行与林间，轻轻拨开挡路的树枝，竭力不让脚步发出一点儿声音，连呼吸都必须小心翼翼：他必须小心，因为林中到处都有与他一样潜行的猎人，如果他发现了别的生命，能做的只有一件事：开枪消灭之。在这片森林中，他人就是地狱，就是永恒的威胁，任何暴露自己存在的生命都将很快被消灭，这就是宇宙文明的图景，这就是对费米悖论的解释 2015-01-06 09:16:05“城市就是森林，每一个男人都是猎手，每一个女人都是陷阱。” 2015-02-10 12:11:38自由女神像基座上的埃玛·拉扎的诗原文为：把你们疲惫的人，你们贫穷的人，你们渴望呼吸自由空气的挤在一堆的人都给我/把那些无家可归、饱经风浪的人都送来/在这金色的大门旁，我要为他们把灯举起。 2015-02-10 13:57:09给时光以生命，而不是给生命以时光。 2015-02-10 13:57:28给岁月以文明，给时光以生命。 2015-02-11 21:58:25精骛八极，心游万仞 2015-03-19 23:01:31从科学角度讲，毁灭一词并不准确，没有真正毁掉什么，更没有灭掉什么，物质总量一点不少都还在，角动量也还在，只是物质的组合方式变了变，像一副扑克牌，仅仅重洗而已……可生命是一手同花顺，一洗什么都没了。” 2015-03-19 23:05:56处于幼年的人类文明曾经打开家门向外看了一眼，外面无边的暗夜吓住了他，他面对黑暗中的广袤和深邃打了个寒战，紧紧地关上了门。 2015-03-31 20:09:40这是一种放弃，她终于看清了，使自己这粒沙尘四处飘飞的，是怎样的天风；把自己这片小叶送向远方的，是怎样的大河。她彻底放弃了，让风吹透躯体，让阳光穿过灵魂。多看笔记 来自多看阅读 for Kindle]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>小说</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《把时间当作朋友》读书笔记]]></title>
    <url>%2F2015%2F07%2F21%2F%E3%80%8A%E6%8A%8A%E6%97%B6%E9%97%B4%E5%BD%93%E4%BD%9C%E6%9C%8B%E5%8F%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[把时间当作朋友李笑来-李笑来 We do not plan to fail, we fail to plan 2015-06-12 18:00:14柳比歇夫的日志，是“事件-时间日志”（event-time log）。他的方法要比李敖的方法更为高级。李敖的事件记录，往往只能记录事件的名称，是一种基于结果的记录；而柳比歇夫的“事件-时间日志”却是一种基于过程的记录。这里的细微差别是，基于过程的记录要比基于结果的记录只能更为详尽。 2015-06-12 18:01:22“管理时间”是不可能的，那么解决方法就只能是，想尽一切办法真正了解自己，真正了解时间、精确地感知时间；而后再想尽一切办法使自己以及自己的行为与时间“合拍”，就是我的说法——“与时间做朋友” 2015-06-15 00:46:27“We do not plan to fail, we fail to plan.” 2015-06-15 00:47:54字典里说，所谓的成功就是达成预期目标 2015-06-15 00:48:14“失败只有一种，就是半途而废。”注: 希望为也能懂 2015-06-15 00:50:05对像我这样的普通人来讲，证明我的目标现实可行的方法比较简单：1) 已经有人做到了；2) 我与那人没有太大的差距。 2015-06-15 08:04:30时间的浪费，往往是因为1) 目标不现实或者目前暂时尚不可行；2) 为了达到目标而制定的实施策略有误。 2015-06-15 08:09:38计划是必须的，目标当然应该是确定的。一般来讲，越是短期的目标，越容易清晰。越是清晰的目标越容易实现 2015-06-15 23:41:20很多的时候，没必要做计划的原因有两个：除了前面提到过的“大多数计划其实非常简单”之外，另外一个是“初始状态下，我们往往实际上并没有能力去制定合理有效的计划”。因为做任何事情，我们都可能要经历相同的过程：逐步熟悉，小心摸索，失败失败再失败，认真反思，卷土重来，直至成功。而在最初甚至连基本的认知都没有的时候，制定出来的计划十有八九只不过是空谈注: 走想停看 2015-06-15 23:47:00除了“试错”、“观察”、“阅读”之外，“思考”，准确地说，“正确地思考”，才是获取真正意义上的知识的主要手段。 2015-06-18 08:01:37写作能力在自学能力中占据着重要的地位。 2015-06-18 08:01:51写出简捷、有效、朴素、准确、具体的说明性说理性文章的能力。 2015-06-18 08:09:11无论如何，都不要也不应该用别人的错误惩罚自己，那么做不仅不对，并且愚蠢。 2015-06-18 21:29:47热爱考试的理由很简单，它是通行证，它意味着机会，其他没有此通行证的人无法获得的机会。虽然有些人也许会用其他方式获得那个什么机会，但，既然你没有其他方式，就不要抱怨——反正抱怨没用。抱怨最浪费时间，即便抱怨得正确。举个极端的例子。如果这个社会确实不公平，你要是抱怨一下当然没什么不对的。可是，抱怨不仅要花费时间，还会引发负面情绪，使你丧失斗志。 2015-06-18 21:30:20热爱考试的你，肯定应该有动力为了它做很多准备。首先要弄清楚这个考试对你的分量。对你很重要，那就要下苦功。 2015-06-18 21:33:03如果，你是个完美主义者，总是想更上一层，那还有另外一个终极技巧——把你学会的东西教给别人。教是最好的方法。清楚明了地表述那些你自以为了解的东西并不像想象的那么容易。有的时候，你没能给别人讲清楚，可能是你自己没想明白。更多的时候，被教者的提问，往往会令你发现你的想法还有很多不全面的地方。不要吝啬你的时间，不要吝啬你的精力，更不要目光短浅，记住，教别人等于自己学，只有学好的人才可能教会别人。另外，随着时间的推移，你在知识上不吝共享的经历，最终会让你明白这是最好的助人为乐的方法，并且获得的永远只能是尊重。 2015-06-18 21:33:31总结一下： 要热爱考试，因为你喜欢通行证。 分辨考试的重要性。 提前很久开始准备重要的考试。 做题是最好的准备方法。 通过做题了解考试的重点、难点。 全面补习难点重点，并经常重新审视。 教是最好的学习方法。 2015-06-19 09:23:33笔记都起码有这样几个好处：可以使自己保持参与状态。提供一个完整的捕捉灵感、疑惑的机制。可以用来与其他参与者沟通、讨论正确的信息。 第五章: 小心所谓“成功学”2015-06-22 22:24:56最近《新周刊》[1] 有一篇不错的文章，标题是《有一种毒药叫成功》。对于“成功学”对“成功”庸俗而又过分简单化的定义，《新周刊》如此讽刺：……在成功学的逻辑中，如果你没有赚到“豪宅、名车、年入百万”，如果你没有成为他人艳羡的成功人士，就证明你不行，你犯了“不成功罪”！助你“实现人生价值”、“开发个人潜能”、“三个月赚到一百万”、“有车有房”、“三十五岁以前退休”……成功学泛滥于职场和网络，上进人群迷失在多款提升课程和短期培训班里，成功学大师满天飞，成功学培训蔚为大观成产业。……当全民成功变成狂热风潮，成功上升为绝对真理般的、人人趋之若鹜的主流价值观，成功学就是一粒毒药，而信奉成功学的人就沦为牺牲品。 2015-06-22 22:30:32我做完这件事之后所获得的欢乐和幸福是不是一定要建立在比较的基础上才可以获得的？然后标记出并优先实施那些无需比较就可以获得欢乐和幸福的行动方案。时间会一如既往地分分钟钟、岁岁年年地流逝，但，你会惊讶于你生活的变化。每一分钟，每一秒，每一天，每一年，时间的质量竟然会如此不同。 2015-06-22 22:30:46马丁 塞利格曼——Positive Psychology的鼻祖。最近哈佛大学里窜红的那个“快乐学”教授，就可以算作是塞利格曼的传人。 [↩] 2015-06-24 21:32:19资源不仅稀缺，并且分布很不均匀 2015-06-24 21:39:38人类拥有的普遍的认知偏差之一就是：把成功揽到自己身上，把失败归咎于别人或者坏运气。（心理学上有个专门的名词：self-serving bias 。 2015-06-25 08:05:23努力从失败者身上汲取经验。不要说模仿成功者，就算观察成功者很难。成功者很多，但是，你身边的真正的成功者却很少；成功背后的东西很难看清楚，所谓成功的真实性也很难判断，而成功者们又会有意无意的美化包装他们的经验，而这一切，都在干扰你的判断。但观察失败者却要相对容易得多，因为他们的失败往往是显然而确定的，而失败的原因往往很容易确定，尽管失败者会找各种各样的借口。并且，你身边的失败者数量，显然要多于成功者的数量，于是，你就有了更多的观察机会。 2015-06-25 08:07:42从理性角度出发，所谓我们能体会的运气，只不过因小概率事件发生而产生的感受而已 2015-06-25 08:10:59Shallow men believe in luck. Strong men believe in cause and effect. 2015-06-25 08:12:16骤然临之而不惊， 无故加之而不怒 2015-06-25 21:19:30尽管不应该盲目乐观，但一定不能悲观地生活。神奇的是，努力往往真的会改变一个人的运气。将近两千五百年前，塞涅卡（罗马哲学家、悲剧作家、政治家）就把这件事儿说得非常清楚“所谓的幸运就是当你准备好了的时候机会来了。”（Luck is what happens when preparation meets opportunity.） 2015-06-25 21:38:47专心做可以提升自己的事情；学习并拥有更多更好的技能；成为一个值得交往的人；• 学会独善其身，以不给他人制造麻烦为美德；用你的独立赢得尊重；• 除非有特殊原因，应该尽量回避那些连在物质生活上都不能独善其身的人；那些精神生活上都不能独善其身的，就更应该回避了——尽管甄别起来比较困难；• 真正关心一个朋友的意思是说，你情愿在他身上花费甚至浪费更多的时间；• 记住，一个人的幸福程度，往往取决于他多大程度上可以脱离对外部世界的依附。 2015-06-26 18:25:22每个人各不相同，有些人在工作学习上可以获得更多的乐趣，有的人在生活琐事中可以获得更得的幸福。 2015-06-29 08:07:10凡是值得做的事情，都值得慢慢做——做很久很久。 2015-06-29 08:11:39. 养成规律生活的习惯 2015-06-29 08:11:56每天检查自己的时间表至少三次 2015-06-29 08:12:31完成任何一项任务都要实际上花费比原计划更多的时间 2015-06-29 08:12:38假定你永远都会遇到交通堵塞。 2015-06-29 08:13:11意外总是发生的原因绝对不是因为你的运气格外差，而往往只不过是因为你考虑得不够周全。 2015-06-30 08:03:05假定其他人都会迟到 2015-06-30 08:03:11尽量不要因为别人迟到而责怪他们 2015-06-30 08:05:55姑娘当初生我们的时候我们没有说愿意！ 2014-01-01 11:11:39在几乎所有的社会里，对每一个个体的心理健康最不利的，来自于整个社会的，也许就是对“自卑”的定义了。古今中外，几乎所有的社会里，“自卑”被都定义为是负面的，“自信”才是健康的，而“自负”也是负面的。其实，假想一下，在一个“感觉”自然就是准确的社会里，所有的“自卑”、“自信”、“自负”都是没有必要存在的概念了。然而现实却是我们生活在一个因“感觉”的本性而自然扭曲的世界。为了自己的心理健康，很多的时候我们确实有必要漠视甚至忽略整个社会灌输给我们的观念——因为很多的时候那不过是“整个社会的扭曲的感觉”而已。所以，我在课堂上无数次提到自卑不是缺点。该自卑的时候就要自卑，这才是正常的。 2014-01-01 11:15:54停止嘲弄他人。 2014-01-01 11:16:56忘记自己的优点。 2014-01-01 11:20:24。拿出一张纸，一支笔，罗列一下。左面罗列你的优点，右面罗列你的缺点——花上一天时间也不过分，因为你需要分辨“这个真的是我的优点么？”以及“这个真的是我的缺点么？”而后，再尝试着猜想一下别人是如何看待你的优点或者缺点的。甚至你可以旁敲侧击地去了解一下—— 2014-01-01 11:21:06我们描述一个事物的方式往往会限制我们对那个事物的了解。所以，有的时候，精巧恰当的比喻往往会仅仅因为不同的表述方法就会颠覆我们对同一事物的看法。 2014-01-01 11:23:52马太福音里说，“他有的，就再给他，让他多余；他没有的，就连同他所有的，一并夺走。 2014-01-01 11:24:54，时间不一定就是金钱。毕竟，不是每个人都有能力把时间转换成金钱，也不是所有人都可以把时间转换成同等高额的金钱。拿出纸笔、列一列，然后问问自己，“我的时间究竟可以标价多少？”——这就是一个人决心不再浪费时间的最有效的起点和动力。只有爱惜才可能产生节约的动力。 2014-01-03 03:06:38急功近利是一个贬义词，多少是有些肤浅的。其实，急功近利是所有人的本性，只不过，只有少数人最终通过心智的力量彻底想清楚，事实上急功近利往往是一个风险高于回报的行为模式。 2014-01-03 03:08:40这一刻之后的任何时间都充满了变数——保守的选择通常是避免风险的最佳行为模式 第七章 真正的解决方案2014-01-03 16:29:50文章本天成，妙手偶得之” 2014-01-03 16:40:19，贫穷，从整体上来看是“永存之困境”（persistent problem）。无论这世界发展成什么样子，都不可能彻底根除贫困，因为最终，所谓的贫困是相对的。 2014-01-04 03:12:58耐心究竟从何而来呢？首先，所有的耐心都来自于了解。 2014-01-04 03:15:19爱因斯坦确实曾用这样的一个比喻解释相对论：“一位先生和一位漂亮女孩在一起呆上一小时，他会感觉像一分钟；但如果让他在火炉子上呆上一分钟，他会感觉比一小时还长。这就是相对论。”但 2015-07-08 21:29:50We can’t solve problems by using the same kind of thinking we used when we created them. — Albert Einstein多看笔记 来自多看阅读 for Kindle]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>鸡汤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《MacTalk·人生元编程》读书笔记]]></title>
    <url>%2F2015%2F07%2F21%2F%E3%80%8AMacTalk%C2%B7%E4%BA%BA%E7%94%9F%E5%85%83%E7%BC%96%E7%A8%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[MacTalk·人生元编程 生命中遇见的每一本书，都不是偶然 生命中遇见的每一本书，都不是偶然2015-07-09 08:10:15如果你问我是否取得了最后的成功，答案是‘当然没有！’如果是的话，生活将会变得令人厌烦。 2015-07-09 08:10:42我们是幸运的，因为我们的心灵是如此不可预知；正因为如此，生活才充满了情趣。尽管如此，我们仍在进行努力来科学地了解我们自身……” 怀念20072015-07-09 10:58:44一个产品从无到有是困难的，从有到精是艰难的，而当你站上一个巅峰之后，哪怕是做最微小的改进和提升，都需要花费大量的人力物力，同时还要承受失败的风险。我们都知道，从平庸到优秀是容易的，从优秀到卓越是痛苦的，可能很多人、公司穷尽一生都无法达到卓越的境地。 趣谈个人建站2015-07-13 11:39:40我们真的不能一心多用吗？或者说并发带给我们的到底是效率的提升还是状态的下降 2015-07-13 11:43:30任务的并行，上下文的切换，注意力的分散，都会让你的效率大打折扣，所以设计模式中的职责单一原则不是盖的，一个类尽可能只做一件事情，无论是效率还是后期维护都会好很多，人脑其实也是一样。 2015-07-13 11:45:10 简单任务的并发是大脑天生的nature，每个人都在不自觉的应用。 在宽松的环境中让简单机械的任务和复杂有机的任务并行完成是非常不错的做法，提高效率节省时间。 在高危环境中（驾驶、高空作业等等）我们应该专心致志的只做当前的工作。 对于复杂任务，我们最好一件一件完成，即使有些人能够同时处理多重任务，那也需要长期的艰苦训练，比如郭靖君，你能否做到，就得看有没有周伯通那样的大哥！ 2015-07-13 11:47:17大牛领会了返璞归真和万物生长的道理，知行合一，遇事抖抖衣袖，不溅起一片涟漪” 2015-07-13 12:25:02坊间流传，普通程序员用bash，文艺程序员用zsh，XX程序员直接用原生的sh，我建议大家文艺一点，用zsh好一些，功能也最强大。目前各个版本的Linux默认的shell都是bash，如果你想用zsh，需要安装一下，如下：sudo apt-get install zsh具体的配置我就不介绍了，感兴趣的读者，可以参考http://leeiio.me/bash-to-zsh-for-mac/ 2015-07-13 12:55:42第一个十年我才华横溢，“贼光闪现”，令周边黯然失色；第二个十年，我终于“宝光现形”，不再去抢风头，反而与身边的美丽相得益彰；进入第三个十年，繁华落尽见真醇，我进入了“醇光初现”的阶段，真正体味到了境界之美。——台湾作家林清玄 锤子和钉子2015-07-20 08:15:26Hater，这种人对自己不了解或没有勇气尝试的事物永远持否定态度，如果你发现一个人大部分时间在否定着什么，那么他们的意见不听也罢，甚至于那些鼓励的建议也仅仅是建议而已，仅供参考，因为最终不是那些提建议的人去做事和承担后果 2015-07-21 08:10:42所有人都是在试错中成长，那些不犯错的人充满了各种幻觉，其实是因为他不再成长了。 2015-07-21 08:11:00他们一定要给你泼冷水的。泼冷水的愿望之强烈，你无法想象。那种强烈借助了太多的力量：怀疑、嫉妒、恐惧、愤怒。而在表现的过程中却又包装上另外一层表皮：关怀、爱护、友爱、帮助。 2015-07-21 08:13:44不管是谈恋爱、做项目、创业、投资都会面临这沉没成本的问题，基本的句式就是『我们已经投入了这么多……』『我们已经走了这么远……』『我们已经牺牲了这么多……』。纠结于过去的沉没成本，会让人感到痛苦和犹豫不决，该放手时要放手， 2015-07-21 08:15:36你一直坚持着最后失败了，就是固执不懂变通。中途转向失败了，就是没有再坚持最后一公里。相反，如果你一直坚持着最后成事了，那就是无所畏惧一往无前；转向成功了，那就是灵活柔性随需应变。可见选择是一件多么艰难而奇妙的事。是沉没的成本还是沉默的坚持，都是你自己的判断和选择，有时候是命数，有时候是形式，只要是自己选择，接受就好了，只能这样。 2015-07-21 22:48:25曾经有位古人说过，如果你手里有一把锤子，所有东西看上去都像钉子。还有一位今人说过，如果你有一个钉子，就会满大街找锤子！ 多看笔记 来自多看阅读 for Kindle]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>鸡汤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xmanager实现CentOS远程控制]]></title>
    <url>%2F2015%2F07%2F15%2Fxmanager%E5%AE%9E%E7%8E%B0CentOS%E8%BF%9C%E7%A8%8B%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本文主要介绍通过Xmanager连接CentOS远程桌面时，在CentOS远程机器和Windows客户端需要做的一些配置。 Xmanager简介Xmanager 全称Netsarang Xmanager，是国外一套非常优秀的远程监控软件。在UNIX/Linux和Windows网络环境中，Xmanager是较好的连通解决方案。通过Xmanager连接Linux远程桌面进行图形化管理其实就是利用了Xmanager套装里面的Xbrowser程序。Linux远程图形化管理除了Xbrowser，还有同样优秀的VNC。 特点包括：可通过Xcongfig工具设置多个Xmanager设置；支持多用户的Windows终端环境；支持多个IP地址；支持本地资源数据库；通过热键转换键盘映射；支持多窗口下的Windows打印功能等 CentOS远程机器配置 安装gdmyum -y install gdm配置系统为图形模式，打开/etc/inittab，修改为id:5:initdefault: (若已为5则不需修改) 启用图形化界面vim /etc/inittab，3改为5 1id:5:initdefault: 配置gdm参数vim /etc/gdm/custom.conf，在[security]和[xdmcp]字段下分别添加如下内容： 123456789[daemon][security]AllowRemoteRoot=true[xdmcp]Port=177Enable=1[greeter][chooser][debug] 防火墙设置 关闭防火墙临时关闭：service iptables stop永久关闭：checkcfg iptables off 在防火墙上打开udp协议177 端口iptables -A INPUT -p tcp --dport 177 -j ACCEPTservice iptables save 重启机器reboot Windows客户端配置 XManager远程桌面连接在Windows上打开XBrowser通过IP即可远程连接CentOSXbrowser&gt;工具&gt;选项&gt;添加主机&gt;连接 Xshell远程桌面连接，Xshell隧道连接&gt;执行gnome-panel命令备注：只能访问用户文件夹 效果图]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>Xmanager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开放虚拟化格式之OVF-OVA]]></title>
    <url>%2F2015%2F07%2F12%2F%E5%BC%80%E6%94%BE%E8%99%9A%E6%8B%9F%E5%8C%96%E6%A0%BC%E5%BC%8F%E4%B9%8BOVF-OVA%2F</url>
    <content type="text"><![CDATA[遇到Sphere6.0 (esxi6 )导出的OVF虚拟机模板在Vmware WorkStation 9和VmWare WorkStation11中导入报错的问题，暂改成OVA格式进行数据交换。 #问题描述 解决方式以OVA格式进行数据交换VMware导出时将后缀改为OVA后到处；ESXI导出时选OVA #基础知识 OVFOVF（Open Virtualization Format：开放虚拟化格式 ）OVF是一种开源的文件规范，它描述了一个开源、安全、有效、可拓展的便携式虚拟打包以及软件分布格式，它一般有几个部分组成，分别是ovf文件、mf文件、cert文件、vmdk文件和iso文件。 OVAOVA（Open Virtualization Appliance：开放虚拟化设备）OVA包是一个单一的文件，所有必要的信息都封装在里面。OVA文件则采用.tar文件扩展名,包含了一个OVF 包中所有文件类型。这样OVA单一的文件格式使得它非常便携。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>EXSI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[磁盘阵列配置]]></title>
    <url>%2F2015%2F07%2F12%2F%E7%A3%81%E7%9B%98%E9%98%B5%E5%88%97%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[新购进一台戴尔塔式服务器，机器3*300G的硬盘默认已做RAID5配置，新增硬盘需配置磁盘阵列 RAID容量计算器配置前，可根据硬盘数和RAID级别，可计算配置后的硬盘实际可用容量参考计算工具 RAID容量计算器 raid-calculator #RAID基础知识 RAID 0 特点：高性能 低稳定性 中等成本 RAID0又称为Stripe（条带化）或Striping，它代表了所有RAID级别中最高的存储性能。 RAID0的缺点是不提供数据冗余，因此一旦用户数据损坏，损坏的数据将无法得到恢复。 RAID 0具有的特点，使其特别适用于对性能要求较高，而对数据安全不太在乎的领域，如图形工作站等。 对于个人用户，RAID 0也是提高硬盘存储性能的绝佳选择。 RAID 1 特点：高稳定性 普通性能 中等成本 RAID 1又称为Mirror或Mirroring（镜像），它的宗旨是最大限度的保证用户数据的可用性和可修复性。 RAID 1的操作方式是把用户写入硬盘的数据百分之百地自动复制到另外一个硬盘上。 Mirror虽不能提高存储性能，但由于其具有的高数据安全性，使其尤其适用于存放重要数据，如服务器和数据库存储等领域。 RAID 5 特点：高性能 中等稳定性 中等成本 RAID 5 是一种存储性能、数据安全和存储成本兼顾的存储解决方案。 RAID 5可以理解为是RAID 0和RAID 1的折衷方案。 RAID 5可以为系统提供数据安全保障，但保障程度要比Mirror低而磁盘空间利用率要比Mirror高。 RAID 5具有和RAID 0相近似的数据读取速度，只是多了一个奇偶校验信息，写入数据的速度比对单个磁盘进行写入操作稍慢。同时由于多个数据对应一个奇偶校验信息，RAID 5的磁盘空间利用率要比RAID 1高，存储成本相对较低。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>RAID</tag>
        <tag>硬件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx负载均衡参数配置]]></title>
    <url>%2F2015%2F07%2F12%2Fnginx%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[问题描述Nginx服务器返回大量502Bad Gateway和504 Time-Out，代理服务器Jetty端存在大量CLOSE_WAIT和TIME_WAIT状态的连接错误信息查看口令：netstat -n | awk &#39;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}&#39; 解决方案 Linux中TCP/IP内核参数 优化编辑参数：vi /etc/sysctl.conf12345678910111213net.core.somaxconn = 4096net.ipv4.tcp_max_syn_backlog = 8192net.ipv4.tcp_syn_retries= 5net.ipv4.tcp_synack_retries = 5net.ipv4.tcp_abort_on_overflow=0net.ipv4.tcp_tw_reuse=1net.ipv4.tcp_tw_recycle=1 net.ipv4.tcp_timestamps=1net.ipv4.tcp_syncookies=1net.ipv4.tcp_max_tw_buckets=90000net.ipv4.tcp_fin_timeout=30net.ipv4.ip_local_port_range=10000 65000net.ipv4.tcp_keepalive_time=1200 让参数生效：/sbin/sysctl -p Nginx配置参数主要配置三个proxy_超时控制参数12345678910111213141516171819upstream uadb_server&#123; server 192.168.1.81:8080 weight=1 max_fails=2 fail_timeout=0; server 192.168.1.82:8080 weight=1 max_fails=2 fail_timeout=0; &#125; server &#123; listen 9090; server_name uadb_server; access_log /var/log/nginx/uadb_server-access-ssl.log; error_log /var/log/nginx/uadb_server-error-ssl.log; location /&#123; proxy_pass http://uadb_server; # time out settings proxy_connect_timeout 60; proxy_read_timeout 3600; proxy_send_timeout 3600; proxy_temp_file_write_size 64k; proxy_redirect off; &#125; &#125; Nginx upstream负载均衡/反向代理 upstream算法分析 轮询每个请求按时间顺序分配到不同的后端服务器了，后端服务器down掉，自动切除； weight：设定服务器权值：如weight=2，服务器性能不均时候使用。weight：默认为1，weight越大，负载的权重越大； ip_hash ：每个请求按访问ip的hash结果分配，每个访客有固定的后端服务器，可以解决session问题； fair（第三方）：按后端服务器的响应时间来分配，响应时间短的优先分配 url_hash (第三方)： 按访问的url的hash结果分配，使每个url定向到同一个后端服务器，后端为缓存服务器比较有效。 upstream参数介绍 down：当前的IP server暂时不参与负载，不进行反向代理； max_fails：允许请求失败的次数默认为1，当超过最大次数时，返回proxy_next_upstream模块定义的错误； fail_timeout：max_fails次失败后，暂停的时间； backup：其它所有非backup机器down或者忙时候，请求backup机器，这台机器压力最轻。 netstat参数状态查看口令：netstat -an参数说明：1234567891011LISTEN：侦听来自远方的TCP端口的连接请求；SYN-SENT：在发送连接请求后等待匹配的连接请求；SYN-RECEIVED：在收到和发送一个连接请求后等待对方对连接请求的确认；ESTABLISHED：代表一个打开的连接，我们常用此作为并发连接数；FIN-WAIT-1：等待远程TCP连接中断请求，或先前的连接中断请求的确认；FIN-WAIT-2：从远程TCP等待连接中断请求；CLOSE-WAIT：等待从本地用户发来的连接中断请求；CLOSING：等待远程TCP对连接中断的确认；LAST-ACK：等待原来发向远程TCP的连接中断的确认；TIME-WAIT：等待足够的时间以确保远程TCP连接收到中断请求的确认；CLOSED：没有任何连接状态； 服务器TCP连接状态查看口令：netstat -an|awk &#39;/^tcp/{++S[$NF]}END{for (a in S)print a,S[a]}&#39; CLOSED：没有连接活动或正在进行的； LISTEN：服务器正在等待的进入呼叫； SYN_RECV：一个连接请求已经到达，等待确认； SYN_SENT：应用已经开始，打开一个连接； ESTABLISHED：正常数据传输状态，也可以近似的理解为当前服务器的并发数； FIN_WAIT1：应用已经完成； FIN_WAIT2：另一边同意释放； ITMED_WAIT：等待所有分组死掉； CLOSING：两边同时尝试关闭； TIME_WAIT：另一边已初始化一个释放； LAST_ACK：等待所有分组死掉；]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>Web服务器</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS常用脚本]]></title>
    <url>%2F2015%2F07%2F06%2Fcentos%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[CentOS日常使用过程中积累的Shell脚本或Linux命令，持续维护更新 网络设置HostName主机名123456#查看hostnamehostname #设置指定的域名解析地址/etc/hosts # 设置主机名和网络配置/etc/sysconfig/network 静态IP设置下载 static-ip-centos6.sh参数: static-ip.sh &lt;hostname&gt; &lt;interface&gt; &lt;baseip&gt; &lt;ipaddress&gt; &lt;gateway/dns&gt;示例1chmod +x ./static-ip.sh &amp;&amp; ./static-ip.sh localhost eth0 192.168.1 81 1 下载 static-ip-centos7.sh参数: 在文件中修改ip相关参数 手动设置IP 123456#临时设置IPifconfig eth0 192.168.1.160# 设置DNS /etc/resolv.conf 临时设置IPifconfig eth0 192.168.1.160 1234567891011121314#针对特定的网卡进行手动设置vim /etc/sysconfig/network-scripts/ifcfg-eth0DEVICE="eth0"BOOTPROTO="static"HWADDR="00:0C:29:86:3D:16"IPV6INIT="yes"NM_CONTROLLED="yes"ONBOOT="yes"TYPE="Ethernet"UUID="dcf18d86-45ea-4b5c-9627-e75b19b3b6e7"IPADDR=192.168.1.82NETMAST=255.255.255.0DNS1=192.168.1.1 集群环境下ssh免密码登陆认证脚本todo-gitrepository 确保各节点安装ssh,expect 把所有文件拷贝到主节点上 配置hosts.conf和slaves.conf 执行keygen_master 12chmod +x keygen_master.sh./keygen_master.sh PS查看进程通常用ps查看进程PID，kill终止进程 grep 是搜索例如：ps -ef | grep java表示查看所有进程里 CMD 是 java 的进程信息 -aux 显示所有状态ps -aux | grep java kill 命令用于终止进程例如： kill -9 [PID]-9 表示强迫进程立即停止 端口查看如查看80端口：lsof -i tcp:80列出所有端口：netstat -ntlp nohup后台运行后台运行test.jarnohup java -jar test.jar不输出nohup日志：&gt;/dev/null 2&gt;&amp;1 &amp;nohup java -jar test.jar &gt;/dev/null 2&gt;&amp;1 &amp; 重定向 0 表示标准输入 1 标准输出,在一般使用时，默认的是标准输出 2 标准错误信息输出可以用来指定需要重定向的标准输入或输出。例如，将某个程序的错误信息输出到log文件 中：./program 2&gt;log。这样标准输出还是在屏幕上，但是错误信息会输出到log文件中。另外，也可 以实现0，1，2之间的重定向。2&gt;&amp;1：将错误信息重定向到标准输出。 /dev/null它就像一个无底洞，所有重定向到它的信息都会消失得无影无踪。当我们不需要回显程序的所有信息时，就可以将输出重定到/dev/null。 读取文件头/尾/实时内容 head filename读取头部，使用命令head。默认显示文件 filename 的前十行内容head -n 20 filename：显示文件的前20行内容head -n -20 filename ：若n后面的整数为负数时，如则表示列出除尾部的20行外的所有行 tail filename 读取尾部，使用命令tail，使用方法同head相似tail -n 20 filename：显示文件的最后20行内容tail -n +20 filename：显示文件自第20行开始后的所有行（包括第20行） tail -f filename：动态显示最新的文件内容具体用法man head或man tail获取 chkconfig问题chkconfig –add myservice问题：service myservice does not support chkconfig我们一般在脚本开头加入下面两句就好了vim /etc/init.d/myservice添加下面两句到 #!/bin/bash 之后 123#!/bin/bash# chkconfig: 2345 10 90# description: myservice .... 图形化界面切换CentOS6vim /etc/inittab 12345678910# Default runlevel. The runlevels used by RHS are:# 0 - halt (Do NOT set initdefault to this) --停机# 1 - Single user mode --单用户模式# 2 - Multiuser, without NFS (The same as 3, if you do not havenetworking) --多用户模式，不支持NFS# 3 - Full multiuser mode --多用户模式 # 4 - unused --没有使用# 5 - X11 --图形界面方式# 6 - reboot (Do NOT set initdefault to this) --重新启动# --默认运行等级是5id:5:initdefault: 图形化界面切换CentOS7使用systemd创建符号链接指向默认运行级别。 首先删除已经存在的符号链接rm /etc/systemd/system/default.target 默认级别转换为3(文本模式)ln -sf /lib/systemd/system/multi-user.target /etc/systemd/system/default.target或者默认级别转换为5(图形模式)ln -sf /lib/systemd/system/graphical.target /etc/systemd/system/default.target 重启reboot 文件操作mkdir 新建目录mkdir -p 无目录新建目录 touch 新建文件eg：touch test.log 追加文本到文件eg：echo &quot;/opt/cm/etc/init.d/cloudera-scm-server start&quot; &gt;&gt; /etc/rc.local rm 文件删除参数：123-r 就是向下递归，不管有多少级目录，一并删除-f 就是直接强行删除，不作任何提示的意思-rf 递归强制删除 需要提醒的是：使用这个rm -rf的时候一定要格外小心，linux没有回收站的 tar文件解压tar在linux上是常用的打包、压缩、加压缩工具，他的参数很多，折里仅仅列举常用的压缩与解压缩参数 123456789-c ：create 建立压缩档案的参数；-x ： 解压缩压缩档案的参数；-z ： 是否需要用gzip压缩；-v： 压缩的过程中显示档案；-f： 置顶文档名，在f后面立即接文件名，不能再加参数1 将tgz文件解压到指定目录tar zxvf test.tgz -C 指定目录比如将/source/kernel.tgz解压到 /source/linux-2.6.29 目录tar zxvf /source/kernel.tgz -C /source/ linux-2.6.29 VIM操作http://www.eepw.com.cn/article/48018.htm vi删除*.swp临时文件删除i插入esc命令行:底行（x或:wq保存） 查找操作findfind是最常见和最强大的查找命令，你可以用它找到任何你想找的文件。find的使用格式如下：1234 $ find &lt;指定目录&gt; &lt;指定条件&gt; &lt;指定动作&gt; - &lt;指定目录&gt;： 所要搜索的目录及其所有子目录。默认为当前目录。 - &lt;指定条件&gt;： 所要搜索的文件的特征。 - &lt;指定动作&gt;： 对搜索结果进行特定的处理。 如果什么参数也不加，find默认搜索当前目录及其子目录，并且不过滤任何结果（也就是返回所有文件），将它们全都显示在屏幕上。find的使用实例：123456 $ find . -name &quot;my*&quot;搜索当前目录（含子目录，以下同）中，所有文件名以my开头的文件。 $ find . -name &quot;my*&quot; -ls搜索当前目录中，所有文件名以my开头的文件，并显示它们的详细信息。 $ find . -type f -mmin -10搜索当前目录中，所有过去10分钟中更新过的普通文件。如果不加-type f参数，则搜索普通文件+特殊文件+目录。 locatelocate命令其实是“find -name”的另一种写法，但是要比后者快得多，原因在于它不搜索具体目录，而是搜索一个数据库（/var/lib/locatedb），这个数据库中含有本地所有文件信息。Linux系统自动创建这个数据库，并且每天自动更新一次，所以使用locate命令查不到最新变动过的文件。为了避免这种情况，可以在使用locate之前，先使用updatedb命令，手动更新数据库。locate命令的使用实例： 123456 $ locate /etc/sh搜索etc目录下所有以sh开头的文件。 $ locate ~/m搜索用户主目录下，所有以m开头的文件。 $ locate -i ~/m搜索用户主目录下，所有以m开头的文件，并且忽略大小写。 whereiswhereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。whereis命令的使用实例：$ whereis grep whichwhich命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。which命令的使用实例：$ which grep typetype命令其实不能算查找命令，它是用来区分某个命令到底是由shell自带的，还是由shell外部的独立二进制文件提供的。如果一个命令是外部命令，那么使用-p参数，会显示该命令的路径，相当于which命令。type命令的使用实例： 123456 $ type cd系统会提示，cd是shell的自带命令（build-in）。 $ type grep系统会提示，grep是一个外部命令，并显示该命令的路径。 $ type -p grep加上-p参数后，就相当于which命令。 系统时间 date 查看系统时间 date -s 修改时间如：date -s 03/04/2013（将系统日期设定为2013年03月04日） date -s 110:38（将系统时间设定为上午 10:38）修改完后执行：clock -w ,强制将时间写入COMS！ chmod命令详解 使用权限：所有使用者使用方式：chmod [-cfvR] [–help] [–version] mode file…说明：Linux/Unix 的档案存取权限分为三级 : 档案拥有者、群组、其他。利用 chmod 可以藉以控制档案如何被他人所存取。mode ：权限设定字串，格式如下 ：[ugoa…][[+-=][rwxX]…][,…]，其中u 表示该档案的拥有者，g 表示与该档案的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是。123456789101112131415161718192021222324252627282930+ 表示增加权限、- 表示取消权限、= 表示唯一设定权限。r 表示可读取，w 表示可写入，x 表示可执行，X 表示只有当该档案是个子目录或者该档案已经被设定过为可执行。-c : 若该档案权限确实已经更改，才显示其更改动作-f : 若该档案权限无法被更改也不要显示错误讯息-v : 显示权限变更的详细资料-R : 对目前目录下的所有档案与子目录进行相同的权限变更(即以递回的方式逐个变更)--help : 显示辅助说明--version : 显示版本 范例： 将档案 file1.txt 设为所有人皆可读取chmod ugo+r file1.txt 将档案 file1.txt 设为所有人皆可读取chmod a+r file1.txt 将档案 file1.txt 与 file2.txt 设为该档案拥有者，与其所属同一个群体者可写入，但其他以外的人则不可写入chmod ug+w,o-w file1.txt file2.txt 将 ex1.py 设定为只有该档案拥有者可以执行chmod u+x ex1.py 将目前目录下的所有档案与子目录皆设为任何人可读取chmod -R a+r * 此外chmod也可以用数字来表示权限如 chmod 777 file 语法为：chmod abc file 其中a,b,c各为一个数字，分别表示User、Group、及Other的权限。 r=4，w=2，x=1 若要rwx属性则4+2+1=7； 若要rw-属性则4+2=6； 若要r-x属性则4+1=7。 范例： chmod a=rwx file 和 chmod 777 file 效果相同 chmod ug=rwx,o=x file 和 chmod 771 file 效果相同 若用chmod 4755 filename可使此程式具有root的权限 chown命令详解 使用权限：root使用方式：chown [-cfhvR] [–help] [–version] user[:group] file…说明：Linux/Unix 是多人多工作业系统，所有的档案皆有拥有者。利用chown 可以将档案的拥有者加以改变。一般来说，这个指令只有是由系统管理者(root)所使用，一般使用者没有权限可以改变别人的档案拥有者，也没有权限可以自己的档案拥有者改设为别人。只有系统管理者(root)才有这样的权限。123456789101112131415161718192021222324252627user : 新的档案拥有者的使用者IDgroup : 新的档案拥有者的使用者群体(group)-c : 若该档案拥有者确实已经更改，才显示其更改动作-f : 若该档案拥有者无法被更改也不要显示错误讯息-h : 只对于连结(link)进行变更，而非该 link 真正指向的档案-v : 显示拥有者变更的详细资料-R : 对目前目录下的所有档案与子目录进行相同的拥有者变更(即以递回的方式逐个变更)--help : 显示辅助说明--version : 显示版本 范例： 将档案 file1.txt 的拥有者设为 users 群体的使用者 jessiechown jessie:users file1.txt 将目前目录下的所有档案与子目录的拥有者皆设为 users 群体的使用者 lamportchown -R lamport:users *-rw------- (600) -- 只有属主有读写权限。-rw-r--r-- (644) -- 只有属主有读写权限；而属组用户和其他用户只有读权限。-rwx------ (700) -- 只有属主有读、写、执行权限。-rwxr-xr-x (755) -- 属主有读、写、执行权限；而属组用户和其他用户只有读、执行权限。-rwx--x--x (711) -- 属主有读、写、执行权限；而属组用户和其他用户只有执行权限。-rw-rw-rw- (666) -- 所有用户都有文件读、写权限。这种做法不可取。-rwxrwxrwx (777) -- 所有用户都有读、写、执行权限。更不可取的做法。以下是对目录的两个普通设定：drwx------ (700) - 只有属主可在目录中读、写。drwxr-xr-x (755) - 所有用户可读该目录，但只有属主才能改变目录中的内容suid的代表数字是4，比如4755的结果是-rwsr-xr-xsgid的代表数字是2，比如6755的结果是-rwsr-sr-xsticky位代表数字是1，比如7755的结果是-rwsr-sr-t scp命令详解 关于scp scp是secure copy的缩写，scp是linux系统下基于ssh登陆进行安全的远程文件拷贝命令。linux的scp命令可以在linux服务器之间复制文件和目录。 scp命令的用途scp在网络上不同的主机之间复制文件，它使用ssh安全协议传输数据，具有和ssh一样的验证机制，从而安全的远程拷贝文件。scp命令基本格式：scp [-1246BCpqrv] [-c cipher] [-F ssh_config] [-i identity_file] [-l limit] [-o ssh_option] [-P port] [-S program] [[user@]host1:]file1 [...] [[user@]host2:]file2 scp命令的参数说明1234567891011121314151617-1 强制scp命令使用协议ssh1-2 强制scp命令使用协议ssh2-4 强制scp命令只使用IPv4寻址-6 强制scp命令只使用IPv6寻址-B 使用批处理模式（传输过程中不询问传输口令或短语）-C 允许压缩。（将-C标志传递给ssh，从而打开压缩功能）-p 保留原文件的修改时间，访问时间和访问权限。-q 不显示传输进度条。-r 递归复制整个目录。-v 详细方式显示输出。scp和ssh(1)会显示出整个过程的调试信息。这些信息用于调试连接，验证和配置问题。-c cipher 以cipher将数据传输进行加密，这个选项将直接传递给ssh。-F ssh_config 指定一个替代的ssh配置文件，此参数直接传递给ssh。-i identity_file 从指定文件中读取传输时使用的密钥文件，此参数直接传递给ssh。-l limit 限定用户所能使用的带宽，以Kbit/s为单位。-o ssh_option 如果习惯于使用ssh_config(5)中的参数传递方式，-P port 注意是大写的P, port是指定数据传输用到的端口号-S program 指定加密传输时所使用的程序。此程序必须能够理解ssh(1)的选项。 从本地服务器复制到远程服务器复制文件命令格式：1234scp local_file remote_username@remote_ip:remote_folderscp local_file remote_username@remote_ip:remote_filescp local_file remote_ip:remote_folderscp local_file remote_ip:remote_file 实例1234scp /home/linux/soft/scp.zip root@www.mydomain.com:/home/linux/others/softscp /home/linux/soft/scp.zip root@www.mydomain.com:/home/linux/others/soft/scp2.zipscp /home/linux/soft/scp.zip www.mydomain.com:/home/linux/others/softscp /home/linux/soft/scp.zip www.mydomain.com:/home/linux/others/soft/scp2.zip 复制目录命令格式：12scp -r local_folder remote_username@remote_ip:remote_folderscp -r local_folder remote_ip:remote_folder 实例:将 本地 soft 目录 复制 到 远程 others 目录下，即复制后远程服务器上会有/home/linux/others/soft/ 目录12scp -r /home/linux/soft/ root@www.mydomain.com:/home/linux/others/scp -r /home/linux/soft/ www.mydomain.com:/home/linux/others/ 从远程服务器复制到本地服务器从远程复制到本地的scp命令与上面的命令雷同，只要将从本地复制到远程的命令后面2个参数互换顺序就行了。例如12scp root@www.mydomain.com:/home/linux/soft/scp.zip /home/linux/others/scp.zipscp -r www.mydomain.com:/home/linux/soft/ /home/linux/others/ rpm命令命令格式 rpm {-q|–query} [select-options] [query-options] yum命令yum切换阿里云源1234mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backupwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repoyum clean allyum makecache 查询yum search {name} 安装yum install {name}]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb一键安装脚本]]></title>
    <url>%2F2015%2F07%2F01%2Fmongodb%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[准备内容 mongodb安装包（官网版本：mongodb-linux-x86_64-2.6.3.tgz）下载后并重命名为mongodb.tar.gz mongodb安装脚本 安装步骤将文件复制到CentOS后进行安装 CentOS路径：/tmp 执行 123cd /tmpchmod +x install-mongodb.sh sudo ./install-mongodb.sh 一路回车即可]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>MongoDB</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日志分析工具]]></title>
    <url>%2F2015%2F06%2F30%2F%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[最近需要对多台Web服务器/Java客户端程序的日志进行分析，比较了一些开源的日志分析产工具，目前在用的有OtrosLogViewer（olv）和LogExpert AWStats 基于Perl，开源、简洁、强大的网站日志分析工具Free real-time logfile analyzer to get advanced statistics (GNU GPL).AWStats is a free powerful and featureful tool that generates advanced web, streaming, ftp or mail server statistics, graphically. This log analyzer works as a CGI or from command line and shows you all possible information your log contains, in few graphical web pages. It uses a partial information file to be able to process large log files, often and quickly. It can analyze log files from all major server tools like Apache log files (NCSA combined/XLF/ELF log format or common/CLF log format), WebStar, IIS (W3C log format) and a lot of other web, proxy, wap, streaming servers, mail servers and some ftp servers.Take a look at this comparison table for an idea on features and differences between most famous statistics tools (AWStats, Analog, Webalizer,…).AWStats is a free software distributed under the GNU General Public License. You can have a look at this license chart to know what you can/can’t do.As AWStats works from the command line but also as a CGI, it can work with all web hosting providers which allow Perl, CGI and log access. OtrosLogViewer 基于Java开发的，开源、强大、可自定义日志语法规则进行解析、UI友好，可拖拽http日志进行分析，好评 问题1：在windows下切换log会非常卡顿，linux下正常，暂未找到原因 问题2： illegal character in schema name at index xxx 不能直接拖拽文件（如http://192.168.1.81:8080/logs/uadb/uadb.log），需拖拽浏览器页面（如http://192.168.1.81:8080/logs/uadb）中的日志链接 问题3 按%d{yyyy-MM-dd HH\:mm\:ss,SSS} [%t] [%c] [%p] - %m%n规则拆分行存在问题，存在多行并在一起 Useful software for analysing applications logs and traces. LogExpert 小而美的单机日志分析工具，好评 You are a developer needing a nice tail application for MS Windows? You are in the need for a powerful logfile analysis tool? You love logfiles? You live in your logfiles? Or at least: you have to work with them? Download LogExpert if you answered “yes” to any of the questions above! Log Parser 基于.NET平台 Log Parser is a powerful, versatile tool that provides universal query access to text-based data such as log files, XML files and CSV files, as well as key data sources on the Windows operating system such as the Event Log, the Registry, the file system, and Active Directory. You tell Log Parser what information you need and how you want it processed. The results of your query can be custom-formatted in text based output, or they can be persisted to more specialty targets like SQL, SYSLOG, or a chart. Most software is designed to accomplish a limited number of specific tasks. Log Parser is different… the number of ways it can be used is limited only by the needs and imagination of the user. The world is your database with Log Parser. Chainsaw 虽是apache官方的专业Log4j分析工具，2004年就没更新了,差评！ Chainsaw v2 is a companion application to Log4j written by members of the Log4j development community. Like a number of Open Source projects, this new version was built upon inspirations, ideas and creations of others. Chainsaw v2 has it’s roots from the original Chainsaw utility written by Oliver Burn, and with inspiration from the Log Factor 5 utility contributed by ThoughtWorks Inc.]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>日志分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用git命令]]></title>
    <url>%2F2015%2F06%2F30%2F%E5%B8%B8%E7%94%A8git%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[用github来搭建个人技术笔记，少不了记录一些常用的git命令 GitGit基础用爱一起画Git参考教程 克隆库1git clone https://github.com/geosmart/geosmart.io 新建 ‘gh-pages’分支，会自动生成github pages1git checkout --orphan gh-pages 本地提交1git commit -a -m &quot;commit message&quot; 初始版本提交123git push --set-upstream origin master或git push origin master 推送到服务器123git push origin ``` # git同步配置 git syncgit config –global credential.helper storegit config –global push.default matching123456789# git bash记住密码## 设置环境变量在windows中添加一个HOME环境变量，变量名:HOME,变量值：%USERPROFILE%## 创建git用户名和密码存储文件进入%HOME%目录，新建一个名为 &quot;_netrc&quot; 的文件，文件中内容格式如下：```yamlmachine &#123;git account name&#125;.github.comlogin your-usernmaepassword your-password 重新打开git bash即可，无需再输入用户名和密码 查看git配置1git config --list 问题记录CAfile: C:/Program Files/Git/mingw64/ssl/certs/ca-bundle.crtYou have to fix the path to bin/curl-ca-bundle.crt. I had to specify the absolute path, using back-slashes:git config --system http.sslcainfo &quot;C:\Program Files (x86)\git\bin\curl-ca-bundle.crt&quot;`` or — not really recommended — you may choose to switch off SSL checks completely by executing:git config –system http.sslverify false`]]></content>
      <categories>
        <category>版本控制</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LNMP安装]]></title>
    <url>%2F2015%2F06%2F30%2Flnmp%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[具体参考LNMP一键安装官网 NMP环境配置LNMP:Linux+Nginx+Mysql+PHP LNMP相关软件安装目录Nginx 目录: /usr/local/nginx/MySQL 目录 : /usr/local/mysql/MySQL数据库所在目录：/usr/local/mysql/var/MariaDB 目录 : /usr/local/mariadb/MariaDB数据库所在目录：/usr/local/mariadb/var/PHP目录 : /usr/local/php/PHPMyAdmin目录 : 0.9版为/home/wwwroot/phpmyadmin/ 1.0版为 /home/wwwroot/default/phpmyadmin/ 强烈建议将此目录重命名为其不容易猜到的名字。phpmyadmin可自己从官网下载新版替换。默认网站目录 : 0.9版为 /home/wwwroot/ 1.0版为 /home/wwwroot/default/Nginx日志目录：/home/wwwlogs//root/vhost.sh添加的虚拟主机配置文件所在目录：/usr/local/nginx/conf/vhost/PureFtpd 目录：/usr/local/pureftpd/PureFtpd web管理目录： 0.9版为/home/wwwroot/default/ftp/ 1.0版为 /home/wwwroot/default/ftp/Proftpd 目录：/usr/local/proftpd/Redis 目录：/usr/local/redis/ ##一键安装下载wget --no-check-certificate https://api.sinas3.com/v1/SAE_lnmp/soft/lnmp1.2-full.tar.gz一键下载安装wget -c http://soft.vpser.net/lnmp/lnmp1.2-full.tar.gz &amp;&amp; tar zxf lnmp1.2-full.tar.gz &amp;&amp; cd lnmp1.2-full &amp;&amp; ./install.sh lnmp 离线安装cd /tmp/lnmp &amp;&amp; tar zxf lnmp1.2-full.tar.gz &amp;&amp; cd lnmp1.2-full &amp;&amp; ./install.sh lnmp网络情况10M带宽耗时：45分钟]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>Web服务器</tag>
        <tag>LNMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo安装]]></title>
    <url>%2F2015%2F06%2F30%2Fhexo%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[hexo环境搭建123456789npm install -g hexo-clihexo initnpm installnpm install hexo-deployer-git --savenpm install hexo-server --savenpm install hexo-generator-sitemap --savenpm install hexo-generator-feed --savenpm install hexo-toc --savenpm install hexo-html-minifier --save 可选插件12npm un hexo-renderer-marked --savenpm i hexo-renderer-markdown-it --save 常用命令123456789101112hexo g (generate)hexo s (server)hexo clean (clear public)hexo n [layout] &lt;title &gt;(new post)eg:hexo n draft titlehexo publish [layout] &lt;title&gt;eg:hexo publish draft titlehexo d (deploy)hexo d -g (generate and deploy)hexo ghexo s 配置评论插件多说挂了，换gitment,参考Gitment：使用 GitHub Issues 搭建评论系统 常见问题hexo部署错误错误日志：Error: spawn git ENOENT解决方案：方案1）添加环境变量C:\Program Files (x86)\Git\bin;C:\Program Files (x86)\Git\libexec\git-core方案2）安装github windows&gt;在项目中Open in Gitshell&gt;执行hexo d -g]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix-server安装脚本]]></title>
    <url>%2F2015%2F06%2F30%2Fzabbix-server%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[关于zabbix zabbix是一个基于WEB界面的提供分布式系统监视以及网络监视功能的企业级的开源解决方案，能监视各种网络参数，保证服务器系统的安全运营；并提供灵活的通知机制以让系统工程师快速定位/解决存在的各种问题。zabbix由2部分构成，zabbix server与可选组件zabbix agent。 zabbix server可以通过SNMP，zabbix agent，ping，端口监视等方法提供对远程服务器/网络状态的监视，数据收集等功能。 zabbix agent需要安装在被监视的目标服务器上，它主要完成对硬件信息或与操作系统有关的内存，CPU等信息的收集。 准备内容 LNMP/LNAP环境安装 zabbix安装包（官网版本：zabbix-2.2.9.tar.gz） zabbix安装脚本 zabbix清空历史监控数据脚本 安装步骤 PHP参数配置为安装zabbix监控WebUI，需要预先配置phpvim /usr/local/php/etc/php.ini查找配置下列参数：123456memory_limit = 128Mpost_max_size = 50Mupload_max_filesize =50Mmax_execution_time = 600max_input_time = 600date.timezone = Asia/Shanghai 修改后执行service php-fpm restart 修改zabbix_server程序的磁盘路径修改zabbix_server主程序路径12# vim /usr/local/zabbix/misc/init.d/tru64/zabbix_serverDAEMON=/usr/local/zabbix/sbin/zabbix_server 添加下面两句到#!/bin/bash之后，解决service myservicedoes not support chkconfig问题12# chkconfig: 2345 10 90 # description:zabbix.... 编辑zabbix_server配置文件vim /usr/local/zabbix/etc/zabbix_server.conf12345DBHost=localhostDBName = zabbix DBPassword =zabbix DBUser = zabbix LogFile=/tmp/zabbix_server.log 添加zabbix服务Service端口（不能重复操作） 123456cat &gt;&gt;/etc/services&lt;&lt;EOFzabbix-agent 10050/tcp Zabbix Agentzabbix-agent 10050/udp Zabbix Agentzabbix-trapper 10051/tcp Zabbix Trapperzabbix-trapper 10051/udp Zabbix TrapperEOF Mysql中新建Zabbix数据库 123456789101112131415161718192021222324252627282930313233mysql -uroot -prootcreate database zabbix;grant all privileges on zabbix.* to zabbix@localhost identified by &apos;zabbix&apos;;quit``` 6. 执行安装脚本``` bash cd /usr/local/zabbixchmod +x configurecd /usr/local/zabbix/scriptchmod +x install-zabbix_server.sh sudo ./install-zabbix_server.sh``` # 相关操作1. zabbix网站中的启用中文后乱码问题* 在zabbix网站目录下的include/locales.inc.php文件中启用中文（&apos;display&apos;=true）* 在windows下控制面板-&gt;字体-&gt;选择一种中文字库例如“楷体”，把它拷贝到zabbix的web端的fonts目录下例如：/var/www/html/zabbix/fonts，并且把TTF后缀改为ttf* 修改zabbix的web端/include/defines.inc.php，如下``` php//define(&apos;ZBX_GRAPH_FONT_NAME&apos;, &apos;DejaVuSans&apos;); // origin namedefine(&apos;ZBX_GRAPH_FONT_NAME&apos;, &apos;simkai&apos;); // custom font name``` 2. 若zabbix的host无法访问，考虑防火墙是否需要关闭/加入信任端口``` bash#查看防火墙状态service iptables status #关闭防火墙 service iptables stop #永久关闭防火墙 chkconfig iptables off 编译问题‘aclocal-1.14’ is missing on your system.You should only need it if you modified ‘acinclude.m4’ or ‘configure.ac’ or m4 files included by ‘configure.ac’.解决方法 12touch configure.ac aclocal.m4 configure Makefile.am Makefile.inmake 查看zabbix服务是否已启动 123456netstat -utlnp | grep zabbix ``` 5. 配置文件更新后，需重启客户端服务``` bashservice zabbix_server restart zabbix web配置简略，贴几张效果图]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>监控</tag>
        <tag>Zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix-agent安装脚本]]></title>
    <url>%2F2015%2F06%2F29%2Fzabbix-agent%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[准备内容 zabbix安装包（官网版本：zabbix-2.2.9.tar.gz） yum groupinstall “Development tools” zabbix安装脚本 cd /tmp &amp;&amp; tar -zxf zabbix.gz 安装步骤 修改zabbix_server程序的磁盘路径修改zabbix_server主程序路径12# vim /usr/local/zabbix/misc/init.d/tru64/zabbix_serverDAEMON=/usr/local/zabbix/sbin/zabbix_server 添加下面两句到#!/bin/bash之后，解决service myservicedoes not support chkconfig问题12# chkconfig: 2345 10 90 # description:zabbix.... 编辑zabbix_agentd配置文件vim /usr/local/zabbix/etc/zabbix_agentd.conf 12345678LogFile=/tmp/zabbix_agentd.log #服务端IP Server=192.168.1.80#服务端IP ServerActive= 192.168.1.80#客户端IP与zabbix-web配置上的hostName一致 Hostname=localhostUnsafeUserParameters=1 执行安装脚本 123cd /usr/local/zabbix/script/install-zabbix_agentd.shchmod +x install-zabbix_agentd.sh sudo ./install-zabbix_agentd.sh 相关操作 若zabbix的host无法访问，考虑防火墙是否需要关闭/加入信任端口 #查看防火墙状态 service iptables status #关闭防火墙 service iptables stop #永久关闭防火墙 chkconfig iptables off 查看zabbix服务是否已启动 netstat -utlnp | grep zabbix 配置文件更新后，需重启客户端服务 service zabbix_agentd restart]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>监控</tag>
        <tag>Zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb手动安装脚本]]></title>
    <url>%2F2015%2F06%2F28%2Fmongodb%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[准备事项下载安装包文件（二进制编译版）1234mkdir -p /usr/local/mongodbcd /usr/local/mongodbwget http://fastdl.mongodb.org/linux/mongodb-linux-x86_64-2.6.3.tgztar -zvxf mongodb-linux-x86_64-2.6.3.tgz 重命名&gt;新建数据/日志目录123mv mongodb-linux-x86_64-2.6.3 mongodbmkdir datamkdir log 配置环境变量CentOs中配置path环境变量,确保mongodb的bin目录包含在path环境变量中。 配置PATH1234vim /etc/profile #set for mongodb export MONGODB_HOME=/usr/local/mongodb export PATH=$MONGODB_HOME/bin:$PATH 查看当前PATH1echo $PATH 让环境变量生效1234567source /etc/profile//验证环境变量是否生效mongod -versionecho $PATH``` ## 添加CentOS开机启动项 vim /etc/rc.d/rc.local//将mongodb启动命令手动追加到本文件中：/usr/local/mongodb/bin/mongod –dbpath /usr/local/mongodb/data –logpath /usr/local/mongodb/log/mongodb.log –maxConns=2000 –fork –smallfiles12345678910111213141516171819# 启动MongoDB## 配置文件形式``` bashvim /usr/local/mongodb/mongodb.confdbpath=/usr/local/mongodb/datalogpath=/usr/local/mongodb/log/mongodb.loglogappend=trueport=27017fork=truenoauth=truejournal=truesmallfiles=true``` ## 命令行形式``` Bash/usr/local/mongodb/bin/mongod --dbpath /usr/local/mongodb/data --logpath /usr/local/mongodb/log/mongodb.log --fork --smallfiles//可选：--auth 增加用户1useradd mongodb -M -s /sbin/nologin 启动服务/测试服务状态12345678service mongod startservice mongod statusshutdown -r nowservice mongod statusmongo adminshow dbs；db.test.find();exit 部署问题记录MongoVUE不能连接将27017端口加入信任列表；局域网测试直接关闭防火墙1234567891011//关闭防火墙service iptables stop//开启chkconfig iptables on//关闭chkconfig iptables off//查询TCP连接情况 netstat -n | awk '/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;'//查询端口占用情况： netstat -anp | grep portno//（例如：netstat –apn | grep 80）]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jdk安装脚本]]></title>
    <url>%2F2015%2F06%2F28%2Fjdk%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[准备内容 jdk7 X64安装包（官网版本：jdk-7u80-linux-x64.tar.gz）下载后并重命名为jetty.tar.gz jdk安装脚本 安装步骤将文件复制到CentOS后进行安装 CentOS路径：/tmp/jdk 执行 1234cd /tmp/jdkchmod +x install-jdk.shsudo ./install-jdk.sh 一路回车即可 #附件 install-jdk.sh1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#!/bin/shBASEDIR=$(cd `dirname $0`; pwd)#jdk-7u80-linux-x64read -p "Please select java tar package full path path[/tmp/jdk.tar.gz] " INSTALL_FILEif [ ! -f "$INSTALL_FILE" ]; thentrueINSTALL_FILE="/tmp/jdk.tar.gz"fi# Set install pathread -p "Please select java install path path[/usr/local/java]: " INSTALL_PATHif [ "$INSTALL_PATH" = "" ]; thentrueINSTALL_PATH="/usr/local/java"fiif [ ! -d $INSTALL_PATH ]; then echo "mkdir $INSTALL_PATH" mkdir -p $INSTALL_PATHfiecho "uncompress $INSTALL_FILE to $INSTALL_PATH"if [ -w $INSTALL_PATH ]; then tar -zxvf $INSTALL_FILE -C $INSTALL_PATH --strip-components=1else sudo tar -zxvf $INSTALL_FILE -C $INSTALL_PATH --strip-components=1fiecho "Setting java environment..."echo "export JAVA_HOME=$INSTALL_PATH" | sudo tee -a /etc/profileJAVA_HOME=$INSTALL_PATH#FIXMEecho "export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar" | sudo tee -a /etc/profileecho 'export PATH=$JAVA_HOME/bin:$PATH' | sudo tee -a /etc/profileecho "refresh java environment..."#TODO what does "." do? the same as "source" command?. /etc/profilesource /etc/profilejava -versionif [ "$?" = "0" ]; thenecho -e "\033[32m Installed, please source /etc/profile or relogin. \033[0m"elseecho -e "\033[31m Install failed. \033[0m"fiunset BASEDIRunset INSTALL_PATHunset INSTALL_FILEexit 0 卸载JDK 1234567#查看系统已安装的jdkrpm -qa|grep jdkjava-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64#卸载指定版本的jdkrpm -e --nodeps java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64# 删除JAVA_HOME，CLASSPATH等相关环境变量vim /etc/profile]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>J2EE</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jetty手动安装脚本]]></title>
    <url>%2F2015%2F06%2F28%2Fjetty%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[##安装步骤将文件复制到CentOS后进行安装 下载解压Jetty1234cd /tmpwget http://eclipse.org/downloads/download.php?file=/jetty/stable-9/dist/jetty-distribution-9.3.0.v20150612.tar.gz&amp;r=1tar -xzvf jetty-distribution-9.3.0.v20150612.tar.gzmv jetty-distribution-9.1.1.v20140108 /usr/local/jetty 新建用户&gt;配置jetty所属权限12useradd -m jettychown -R jetty:jetty /usr/local/jetty/ 安装到系统服务123ln -s /usr/local/jetty/bin/jetty.sh /etc/init.d/jettychkconfig --add jettychkconfig --level 345 jetty on 编辑启动脚本12345vim /etc/default/jettyJETTY_HOME=/usr/local/jettyJETTY_USER=jettyJETTY_PORT=8080JETTY_LOGS=/usr/local/jetty/logs/ 启动服务&gt;测试service jetty start curl localhost:8080 常用操作指令jetty [-d] {start|stop|run|restart|check|supervise} [ CONFIGS ... ]]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>Web服务器</tag>
        <tag>Jetty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jetty一键安装脚本]]></title>
    <url>%2F2015%2F06%2F28%2Fjetty%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[准备内容 jetty安装包（官网版本：jetty-distribution-9.2.11.v20150529.tar）下载后并重命名为jetty.tar.gz jetty安装脚本 已安装JDK1.7 安装步骤将文件复制到CentOS后进行安装 CentOS路径：/tmp/jetty 执行 123cd /tmp/jettychmod +x install-jetty.shsudo ./install-jetty.sh 一路回车即可]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>Web服务器</tag>
        <tag>Jetty</tag>
      </tags>
  </entry>
</search>
